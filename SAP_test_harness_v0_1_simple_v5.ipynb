{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLCnPEYvZb3CZZvigR5OUu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArindamBanerji/supply-chain-llm/blob/master/SAP_test_harness_v0_1_simple_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAP Test Harness Colab Scaffolding Documentation\n",
        "\n",
        "## Overview\n",
        "This document describes the Google Colab scaffolding used for running the SAP test harness MVP 0.1. The scaffolding provides a structured environment for executing various test suites and analyzing results.\n",
        "\n",
        "## Setup Cells (1-4)\n",
        "1. Drive Mount Cell\n",
        "   - Mounts Google Drive if not already mounted\n",
        "   - Essential for file access\n",
        "\n",
        "2. Variables Setup Cell\n",
        "   - Sets base directory and repository paths\n",
        "   - Configures project-specific variables\n",
        "\n",
        "3. Directory Structure Cell\n",
        "   - Sets up test harness directory paths\n",
        "   - Configures paths for mock SAP and tests\n",
        "\n",
        "4. Helper Functions Cell\n",
        "   - Directory creation utilities\n",
        "   - Path verification functions\n",
        "\n",
        "## Environment Configuration (5-8)\n",
        "5. Directory Management Cell\n",
        "   - Handles directory cleanup\n",
        "   - Downloads/updates source code\n",
        "\n",
        "6. File Verification Cell\n",
        "   - File existence checking\n",
        "   - Path validation utilities\n",
        "\n",
        "7. Requirements Installation Cell\n",
        "   - Installs required Python packages\n",
        "   - Sets up dependencies\n",
        "\n",
        "8. Python Path Configuration Cell\n",
        "   - Configures Python path\n",
        "   - Ensures proper module imports\n",
        "\n",
        "## Test Scripts Categories\n",
        "\n",
        "### Direct Test Scripts (without test runner)\n",
        "- Test Script 1: Basic material management test\n",
        "- Test Script 4: Local P2P tests\n",
        "These scripts directly test the harness functionality without using the test runner.\n",
        "\n",
        "### Test Runner Scripts\n",
        "Scripts that use the test_runner.py framework:\n",
        "1. Material Operations Tests (Scripts 2-3)\n",
        "   - V1 and V2 implementations\n",
        "   - Focus on material management\n",
        "\n",
        "2. Process Flow Tests (Scripts 5-7)\n",
        "   - State management tests\n",
        "   - P2P flow testing\n",
        "   - Transaction verification\n",
        "\n",
        "3. Special Case Tests (Scripts 8-9)\n",
        "   - Error case testing\n",
        "   - Multi-document testing\n",
        "\n",
        "4. Consolidated Testing (Script 10)\n",
        "   - Comprehensive MVP testing\n",
        "   - Aggregates multiple test categories\n",
        "\n",
        "## Test Execution Pattern\n",
        "1. Setup Environment\n",
        "   - Mount drive\n",
        "   - Configure paths\n",
        "   - Install dependencies\n",
        "\n",
        "2. Initialize Test Harness\n",
        "   - Import required modules\n",
        "   - Configure test parameters\n",
        "\n",
        "3. Execute Tests\n",
        "   - Run individual test suites\n",
        "   - Collect and analyze results\n",
        "\n",
        "4. Report Results\n",
        "   - Display test summaries\n",
        "   - Show performance metrics\n",
        "   - Log any failures\n",
        "\n",
        "## Usage Notes\n",
        "- Run setup cells in sequence\n",
        "- Clean cache between major test runs\n",
        "- Use appropriate test script for specific needs\n",
        "- Monitor test execution times\n",
        "- Review failure details carefully\n",
        "- Maintain clean state between test runs"
      ],
      "metadata": {
        "id": "EmHzF483VpmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive if not already mounted\n",
        "import os\n",
        "if not os.path.exists(\"/content/drive\"):\n",
        "      from google.colab import drive # Import the drive function\n",
        "      drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VjLF3zXjSxZv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# variabls that need to be customized && make sure that base_dir exists\n",
        "base_dir = \"/content/drive/My Drive/python-projects/kaggle_experiments/\"\n",
        "repo_git = \"https://github.com/ArindamBanerji/supply-chain-llm.git\"\n",
        "local_repo = \"code_gen_MVP_0.15/\"\n",
        "local_repo_path = base_dir + local_repo"
      ],
      "metadata": {
        "id": "9wwBf3vB_PjT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VARS: Set default variables\n",
        "\n",
        "DEFAULT_BASE_DIR = base_dir + local_repo + \"supply-chain-llm/\"\n",
        "default_base_dir = DEFAULT_BASE_DIR\n",
        "\n",
        "add_harness_path = \"test_harness/\"\n",
        "add_mock_sap_pth = \"test_harness/mock_sap/\"\n",
        "add_tests_path = \"test_harness/tests/\"\n",
        "\n",
        "\n",
        "requirements_fnm = DEFAULT_BASE_DIR + add_harness_path + \"requirements.txt\"\n",
        "test_harness_dir = DEFAULT_BASE_DIR\n",
        "harness_dir = DEFAULT_BASE_DIR + add_harness_path\n",
        "mock_sap_path = DEFAULT_BASE_DIR + add_mock_sap_pth\n",
        "tests_dir = DEFAULT_BASE_DIR + add_tests_path\n",
        "\n",
        "\n",
        "material_ops_test = DEFAULT_BASE_DIR + add_tests_path + \"test_material_ops.py\"\n",
        "p2p_flow_test = DEFAULT_BASE_DIR + add_tests_path + \"test_p2p_flow.py\"\n",
        "\n"
      ],
      "metadata": {
        "id": "fjFyNdmGbfGP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "from os import path\n",
        "\n",
        "def create_dir_if_needed (dir_path):\n",
        "  if path.exists(dir_path) == False:\n",
        "    os.mkdir(dir_path)\n",
        "    print(f\"Directory '{dir_path}' created successfully.\")\n",
        "  else:\n",
        "    print(f\"Directory '{dir_path}' already exists.\")\n"
      ],
      "metadata": {
        "id": "Zx0Z13HZJcdY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def remove_download_dir_if_exist (dir_path) :\n",
        "    if os.path.exists(dir_path):\n",
        "        shutil.rmtree(dir_path)\n",
        "        print(f\"Directory '{dir_path}' and its contents have been removed.\")\n",
        "    else:\n",
        "        print(f\"Directory '{dir_path}' does not exist.\")\n",
        ""
      ],
      "metadata": {
        "id": "U34kSj3iHhLD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# go to the correct directory and download the source code.\n",
        "import os\n",
        "\n",
        "create_dir_if_needed(local_repo_path)\n",
        "remove_download_dir_if_exist( DEFAULT_BASE_DIR)\n",
        "os.chdir(local_repo_path)\n",
        "\n",
        "! (test -d $local_repo && git -C $local_repo pull --rebase) || git clone $repo_git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5MApxz9A4bb",
        "outputId": "3ff656e5-938c-444c-fd13-1c391df1c6b0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory '/content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/' already exists.\n",
            "Directory '/content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/' and its contents have been removed.\n",
            "Cloning into 'supply-chain-llm'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 26 (delta 2), reused 26 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (26/26), 32.82 KiB | 1.64 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# does a given file exist\n",
        "def check_file_exists(file_path):\n",
        "\n",
        "    err_code = os.path.exists(file_path)\n",
        "\n",
        "    if err_code:\n",
        "        print(\"File exists. \", file_path)\n",
        "    else:\n",
        "        print(\"File does not exist. \", file_path)\n",
        "\n",
        "    return err_code"
      ],
      "metadata": {
        "id": "y5PDMy7ehljw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if directory exists and return True or False\n",
        "\n",
        "def check_directory_exists(dir_path):\n",
        "    err_code = os.path.isdir(dir_path)\n",
        "\n",
        "    if err_code:\n",
        "        print(f\"Directory '{dir_path}' exists.\")\n",
        "    else:\n",
        "        print(f\"Directory '{dir_path}' does not exist.\")\n",
        "\n",
        "    return err_code"
      ],
      "metadata": {
        "id": "bPXx_jO0ibav"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run some basic file/dir existence checks\n",
        "\n",
        "check_directory_exists(test_harness_dir)\n",
        "check_file_exists(requirements_fnm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eADlBKQ5jKPJ",
        "outputId": "6fb2da4b-bac8-4005-9eb4-87a94fc01c28"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory '/content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/' exists.\n",
            "File exists.  /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change directories...\n",
        "if check_directory_exists(test_harness_dir):\n",
        "  os.chdir(test_harness_dir)\n",
        "  print(\"Current working directory:\", os.getcwd())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6OMP3EdmlRO",
        "outputId": "7e9386f6-e984-4b01-a7b5-04406eb47e18"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory '/content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/' exists.\n",
            "Current working directory: /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Requirements install\n",
        "if (check_file_exists(requirements_fnm)):\n",
        "  !pip install -r \"$requirements_fnm\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4Ctk9vbwJw1",
        "outputId": "e87038ab-924b-436e-8c54-e631b09f2552"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File exists.  /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt\n",
            "Requirement already satisfied: pytest==7.4.3 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 2)) (7.4.3)\n",
            "Requirement already satisfied: pytest-asyncio==0.21.1 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 3)) (0.21.1)\n",
            "Requirement already satisfied: pytest-cov==4.1.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 4)) (4.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 7)) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 8)) (2.8.2)\n",
            "Requirement already satisfied: aiohttp==3.9.1 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 11)) (3.9.1)\n",
            "Requirement already satisfied: async-timeout==4.0.3 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 12)) (4.0.3)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 15)) (13.9.4)\n",
            "Requirement already satisfied: structlog==23.2.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 16)) (23.2.0)\n",
            "Requirement already satisfied: dataclasses-json==0.6.3 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 19)) (0.6.3)\n",
            "Requirement already satisfied: pyyaml==6.0.1 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 20)) (6.0.1)\n",
            "Requirement already satisfied: black==23.11.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 23)) (23.11.0)\n",
            "Requirement already satisfied: isort==5.12.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 24)) (5.12.0)\n",
            "Requirement already satisfied: mypy==1.7.1 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 25)) (1.7.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest==7.4.3->-r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest==7.4.3->-r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from pytest==7.4.3->-r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 2)) (1.5.0)\n",
            "Requirement already satisfied: coverage>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from coverage[toml]>=5.2.1->pytest-cov==4.1.0->-r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 4)) (7.6.11)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp==3.9.1->-r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 11)) (25.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp==3.9.1->-r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 11)) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp==3.9.1->-r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 11)) (1.18.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp==3.9.1->-r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 11)) (1.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp==3.9.1->-r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 11)) (1.3.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json==0.6.3->-r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 19)) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json==0.6.3->-r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 19)) (0.9.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from black==23.11.0->-r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 23)) (8.1.8)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from black==23.11.0->-r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 23)) (1.0.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from black==23.11.0->-r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 23)) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black==23.11.0->-r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 23)) (4.3.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->-r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 8)) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->-r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 15)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->-r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 15)) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->-r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 15)) (0.1.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.0->aiohttp==3.9.1->-r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 11)) (3.10)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.0->aiohttp==3.9.1->-r /content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/requirements.txt (line 11)) (0.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Python path - Verification\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def add_dir_to_path(dir_path):\n",
        "  if dir_path not in sys.path:\n",
        "    sys.path.append(dir_path)\n",
        "\n",
        "# Verify the path contains your module\n",
        "print(\"Python path:\")\n",
        "print(\"\\n\".join(sys.path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rChq00dTrbQF",
        "outputId": "f631ec94-273e-47eb-f734-f5da4b0e6957"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python path:\n",
            "/content\n",
            "/env/python\n",
            "/usr/lib/python311.zip\n",
            "/usr/lib/python3.11\n",
            "/usr/lib/python3.11/lib-dynload\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages\n",
            "/usr/lib/python3/dist-packages\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/extensions\n",
            "/usr/local/lib/python3.11/dist-packages/setuptools/_vendor\n",
            "/root/.ipython\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# add to existung path - some repeated code happens later\n",
        "add_dir_to_path(test_harness_dir)\n",
        "add_dir_to_path(harness_dir)\n",
        "add_dir_to_path(mock_sap_path)\n",
        "add_dir_to_path (tests_dir)\n",
        "\n",
        "print(\"Python path:\")\n",
        "print(\"\\n\".join(sys.path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1tKR5dyufAT",
        "outputId": "17cd6ad3-f81b-403a-8396-fcfd75660c1f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python path:\n",
            "/content\n",
            "/env/python\n",
            "/usr/lib/python311.zip\n",
            "/usr/lib/python3.11\n",
            "/usr/lib/python3.11/lib-dynload\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages\n",
            "/usr/lib/python3/dist-packages\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/extensions\n",
            "/usr/local/lib/python3.11/dist-packages/setuptools/_vendor\n",
            "/root/.ipython\n",
            "/content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/\n",
            "/content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/\n",
            "/content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/mock_sap/\n",
            "/content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/tests/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4BkK5d9FNX2T"
      },
      "outputs": [],
      "source": [
        "# Core imports cell\n",
        "from test_harness.mock_sap.sap_types import SAPResponse, SAPError\n",
        "from test_harness.mock_sap.api_operations import MockMaterialManagement\n",
        "from test_harness.mock_sap.api_simulator import SAPSimulator\n",
        "from test_harness.mock_sap.p2p_apis import P2PSimulator\n",
        "from test_harness.tests.test_config import TestConfig"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Script 1 : Basic handcrafted material management test\n",
        "\n",
        "import asyncio\n",
        "from test_harness.mock_sap.api_operations import MockMaterialManagement\n",
        "\n",
        "async def test_material_operations():\n",
        "    material_mgmt = MockMaterialManagement()\n",
        "\n",
        "    # Test material availability\n",
        "    response = await material_mgmt.check_material_availability(\n",
        "        material_id='MAT001',\n",
        "        plant='PLANT_1'\n",
        "    )\n",
        "\n",
        "    print(\"Material Availability Response:\")\n",
        "    print(f\"Success: {response.success}\")\n",
        "    print(f\"Data: {response.data}\")\n",
        "\n",
        "    # Test create material\n",
        "    new_material = {\n",
        "        'material_id': 'TEST001',\n",
        "        'description': 'Test Material',\n",
        "        'type': 'RAW',\n",
        "        'base_unit': 'KG',\n",
        "        'plant_data': {\n",
        "            'PLANT_1': {\n",
        "                'storage_location': 'A01',\n",
        "                'unrestricted_stock': 500.0\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    create_response = await material_mgmt.create_material_master(new_material)\n",
        "    print(\"\\nMaterial Creation Response:\")\n",
        "    print(f\"Success: {create_response.success}\")\n",
        "    print(f\"Data: {create_response.data}\")\n",
        "\n",
        "# Execute test\n",
        "await test_material_operations()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmBwu7OIx4x6",
        "outputId": "03697e81-796e-408a-f851-4fbee126c8f5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Material Availability Response:\n",
            "Success: True\n",
            "Data: {'material_id': 'MAT001', 'plant': 'PLANT_1', 'description': 'Raw Material A', 'base_unit': 'KG', 'unrestricted_stock': 1000.0, 'storage_location': 'A01', 'valuation': {'standard_price': 10.0, 'price_unit': 1, 'currency': 'USD'}}\n",
            "\n",
            "Material Creation Response:\n",
            "Success: True\n",
            "Data: {'material_id': 'TEST001', 'message': 'Material master created successfully'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Setup environment - path etc, check.\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup paths\n",
        "current_dir = os.getcwd()\n",
        "if current_dir not in sys.path:\n",
        "    sys.path.append(current_dir)\n",
        "\n",
        "# Verify directory structure\n",
        "def ensure_dir_structure():\n",
        "    dirs = [\n",
        "        'test_harness',\n",
        "        'test_harness/mock_sap',\n",
        "        'test_harness/tests',\n",
        "    ]\n",
        "\n",
        "    for d in dirs:\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "        init_file = os.path.join(d, '__init__.py')\n",
        "        if not os.path.exists(init_file):\n",
        "            with open(init_file, 'w') as f:\n",
        "                pass\n",
        "\n",
        "    print(\"Directory structure verified\")\n",
        "\n",
        "ensure_dir_structure()\n",
        "\n",
        "# Print verification\n",
        "print(\"Python path includes:\")\n",
        "print(\"\\n\".join(sys.path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2JheUCug4oK",
        "outputId": "c4c42a14-e7d8-43d3-91be-01a6d13c7e2f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory structure verified\n",
            "Python path includes:\n",
            "/content\n",
            "/env/python\n",
            "/usr/lib/python311.zip\n",
            "/usr/lib/python3.11\n",
            "/usr/lib/python3.11/lib-dynload\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages\n",
            "/usr/lib/python3/dist-packages\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/extensions\n",
            "/usr/local/lib/python3.11/dist-packages/setuptools/_vendor\n",
            "/root/.ipython\n",
            "/content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/\n",
            "/content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/\n",
            "/content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/mock_sap/\n",
            "/content/drive/My Drive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm/test_harness/tests/\n",
            "/content/drive/MyDrive/python-projects/kaggle_experiments/code_gen_MVP_0.15/supply-chain-llm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cleanup.py - cleanuppp script.- the caches cause chaos unless cleaned up.\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def cleanup_cache():\n",
        "    \"\"\"Clean up pytest and Python cache files/directories\"\"\"\n",
        "    cleaned = {\n",
        "        'pytest_cache': 0,\n",
        "        'pycache_dirs': 0,\n",
        "        'pyc_files': 0\n",
        "    }\n",
        "\n",
        "    # Walk through all directories\n",
        "    for root, dirs, files in os.walk('.'):\n",
        "        # Remove .pytest_cache directories\n",
        "        if '.pytest_cache' in dirs:\n",
        "            pytest_cache_path = os.path.join(root, '.pytest_cache')\n",
        "            shutil.rmtree(pytest_cache_path)\n",
        "            cleaned['pytest_cache'] += 1\n",
        "            dirs.remove('.pytest_cache')\n",
        "\n",
        "        # Remove __pycache__ directories\n",
        "        if '__pycache__' in dirs:\n",
        "            pycache_path = os.path.join(root, '__pycache__')\n",
        "            shutil.rmtree(pycache_path)\n",
        "            cleaned['pycache_dirs'] += 1\n",
        "            dirs.remove('__pycache__')\n",
        "\n",
        "        # Remove .pyc files\n",
        "        for file in files:\n",
        "            if file.endswith('.pyc'):\n",
        "                os.remove(os.path.join(root, file))\n",
        "                cleaned['pyc_files'] += 1\n",
        "\n",
        "    print(f\"Cleaned up:\")\n",
        "    print(f\"- {cleaned['pytest_cache']} pytest cache directories\")\n",
        "    print(f\"- {cleaned['pycache_dirs']} Python cache directories\")\n",
        "    print(f\"- {cleaned['pyc_files']} .pyc files\")\n",
        "\n",
        "# Run cleanup\n",
        "cleanup_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46U4zyqJfadm",
        "outputId": "eaceaa37-b656-4f92-88df-d8851add488d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned up:\n",
            "- 0 pytest cache directories\n",
            "- 3 Python cache directories\n",
            "- 0 .pyc files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Script 2 : Test Runner V1 for Material Ops.\n",
        "\n",
        "async def run_direct_tests_in_sequence():\n",
        "    \"\"\"Run test methods sequentially and collect results\"\"\"\n",
        "    from test_harness.tests.test_material_ops import TestMaterialOperations\n",
        "    import inspect\n",
        "    import traceback\n",
        "\n",
        "    print(\"Starting Test Execution...\\n\")\n",
        "\n",
        "    # Test results collection\n",
        "    results = {\n",
        "        'total': 0,\n",
        "        'passed': 0,\n",
        "        'failed': 0,\n",
        "        'failures': []\n",
        "    }\n",
        "\n",
        "    # Create test instance\n",
        "    test_instance = TestMaterialOperations()\n",
        "\n",
        "    try:\n",
        "        # Run setUp\n",
        "        test_instance.setUp()\n",
        "\n",
        "        # Get all test methods (ensure we get them in defined order)\n",
        "        test_methods = [\n",
        "            method[0] for method in inspect.getmembers(test_instance, predicate=inspect.ismethod)\n",
        "            if method[0].startswith('test_')\n",
        "        ]\n",
        "        results['total'] = len(test_methods)\n",
        "\n",
        "        # Run each test method sequentially\n",
        "        for method_name in test_methods:\n",
        "            print(f\"\\nExecuting: {method_name}\")\n",
        "            print(\"-\" * (len(method_name) + 11))\n",
        "\n",
        "            try:\n",
        "                # Get the original method before decoration\n",
        "                original_method = getattr(TestMaterialOperations, method_name)\n",
        "                # Run the coroutine directly\n",
        "                await original_method(test_instance)\n",
        "                print(f\"Result: PASSED\")\n",
        "                results['passed'] += 1\n",
        "\n",
        "            except AssertionError as ae:\n",
        "                print(f\"Result: FAILED\")\n",
        "                print(f\"Error: {str(ae)}\")\n",
        "                results['failed'] += 1\n",
        "                results['failures'].append({\n",
        "                    'test': method_name,\n",
        "                    'type': 'AssertionError',\n",
        "                    'message': str(ae),\n",
        "                    'traceback': traceback.format_exc()\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Result: FAILED\")\n",
        "                print(f\"Error: {type(e).__name__} - {str(e)}\")\n",
        "                results['failed'] += 1\n",
        "                results['failures'].append({\n",
        "                    'test': method_name,\n",
        "                    'type': type(e).__name__,\n",
        "                    'message': str(e),\n",
        "                    'traceback': traceback.format_exc()\n",
        "                })\n",
        "\n",
        "            # Add a small delay between tests\n",
        "            await asyncio.sleep(0.1)\n",
        "\n",
        "        # Run tearDown\n",
        "        if hasattr(test_instance, 'tearDown'):\n",
        "            test_instance.tearDown()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTest suite execution failed: {str(e)}\")\n",
        "        print(traceback.format_exc())\n",
        "\n",
        "    finally:\n",
        "        # Print summary\n",
        "        print(\"\\n\" + \"=\"* 50)\n",
        "        print(\"Test Execution Summary\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Total Tests   : {results['total']}\")\n",
        "        print(f\"Tests Passed  : {results['passed']}\")\n",
        "        print(f\"Tests Failed  : {results['failed']}\")\n",
        "\n",
        "        # Print failures if any\n",
        "        if results['failures']:\n",
        "            print(\"\\nFailure Details:\")\n",
        "            print(\"-\" * 50)\n",
        "            for i, failure in enumerate(results['failures'], 1):\n",
        "                print(f\"\\n{i}. Test: {failure['test']}\")\n",
        "                print(f\"   Type: {failure['type']}\")\n",
        "                print(f\"   Message: {failure['message']}\")\n",
        "                print(f\"   Traceback:\")\n",
        "                print(\"   \" + \"\\n   \".join(failure['traceback'].split('\\n')))\n",
        "\n",
        "# Run tests\n",
        "import asyncio\n",
        "await run_direct_tests_in_sequence()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdnv2Jui0Xe9",
        "outputId": "c87d828e-8595-49fc-970f-80389c9af96f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Test Execution...\n",
            "\n",
            "\n",
            "Executing: test_create_and_verify_material\n",
            "------------------------------------------\n",
            "Result: PASSED\n",
            "\n",
            "Executing: test_default_values\n",
            "------------------------------\n",
            "Result: PASSED\n",
            "\n",
            "Executing: test_duplicate_material_creation\n",
            "-------------------------------------------\n",
            "Result: PASSED\n",
            "\n",
            "Executing: test_empty_input_handling\n",
            "------------------------------------\n",
            "Result: PASSED\n",
            "\n",
            "Executing: test_get_material_master_data\n",
            "----------------------------------------\n",
            "Result: PASSED\n",
            "\n",
            "Executing: test_initial_material_check\n",
            "--------------------------------------\n",
            "Result: PASSED\n",
            "\n",
            "Executing: test_invalid_material_check\n",
            "--------------------------------------\n",
            "Result: PASSED\n",
            "\n",
            "Executing: test_invalid_plant_check\n",
            "-----------------------------------\n",
            "Result: PASSED\n",
            "\n",
            "Executing: test_material_creation_without_id\n",
            "--------------------------------------------\n",
            "Result: PASSED\n",
            "\n",
            "Executing: test_missing_required_fields\n",
            "---------------------------------------\n",
            "Result: PASSED\n",
            "\n",
            "==================================================\n",
            "Test Execution Summary\n",
            "==================================================\n",
            "Total Tests   : 10\n",
            "Tests Passed  : 10\n",
            "Tests Failed  : 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Script 3 : Test Runner V2 for Material Ops\n",
        "\n",
        "\n",
        "async def run_direct_tests():\n",
        "    \"\"\"Run tests directly without pytest\"\"\"\n",
        "    from test_harness.tests.test_material_ops import TestMaterialOperations\n",
        "    import asyncio\n",
        "    import nest_asyncio\n",
        "    import traceback\n",
        "\n",
        "    # Apply nest_asyncio to allow nested event loops\n",
        "    nest_asyncio.apply()\n",
        "\n",
        "    print(\"Running direct tests...\")\n",
        "\n",
        "    results = {'passed': 0, 'failed': 0}\n",
        "    failed_tests = []\n",
        "\n",
        "    try:\n",
        "        test_class = TestMaterialOperations()\n",
        "        test_class.setUp()\n",
        "\n",
        "        # Get all test methods\n",
        "        test_methods = [m for m in dir(test_class)\n",
        "                       if m.startswith('test_') and callable(getattr(test_class, m))]\n",
        "\n",
        "        for method_name in test_methods:\n",
        "            print(f\"\\nRunning {method_name}...\")\n",
        "            try:\n",
        "                test_method = getattr(test_class, method_name)\n",
        "                # Ensure we're dealing with a coroutine and await it\n",
        "                if asyncio.iscoroutinefunction(test_method):\n",
        "                    await test_method()\n",
        "                else:\n",
        "                    test_method()\n",
        "\n",
        "                print(f\"{method_name}: PASSED\")\n",
        "                results['passed'] += 1\n",
        "\n",
        "            except AssertionError as ae:\n",
        "                print(f\"{method_name}: FAILED (Assertion Error)\")\n",
        "                print(f\"Error details: {str(ae)}\")\n",
        "                failed_tests.append({\n",
        "                    'test': method_name,\n",
        "                    'error_type': 'AssertionError',\n",
        "                    'error_msg': str(ae),\n",
        "                    'traceback': traceback.format_exc()\n",
        "                })\n",
        "                results['failed'] += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"{method_name}: FAILED ({type(e).__name__})\")\n",
        "                print(f\"Error details: {str(e)}\")\n",
        "                failed_tests.append({\n",
        "                    'test': method_name,\n",
        "                    'error_type': type(e).__name__,\n",
        "                    'error_msg': str(e),\n",
        "                    'traceback': traceback.format_exc()\n",
        "                })\n",
        "                results['failed'] += 1\n",
        "\n",
        "        if hasattr(test_class, 'tearDown'):\n",
        "            test_class.tearDown()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTest suite execution failed: {str(e)}\")\n",
        "        print(traceback.format_exc())\n",
        "\n",
        "    finally:\n",
        "        print(\"\\nTest Summary:\")\n",
        "        print(f\"Passed: {results['passed']}\")\n",
        "        print(f\"Failed: {results['failed']}\")\n",
        "        print(f\"Total: {results['passed'] + results['failed']}\")\n",
        "\n",
        "        if failed_tests:\n",
        "            print(\"\\nFailed Tests Details:\")\n",
        "            for test in failed_tests:\n",
        "                print(f\"\\n{test['test']}:\")\n",
        "                print(f\"Error Type: {test['error_type']}\")\n",
        "                print(f\"Error Message: {test['error_msg']}\")\n",
        "                print(\"Traceback:\")\n",
        "                print(test['traceback'])\n",
        "\n",
        "# Run tests\n",
        "await run_direct_tests()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQytLi2E6F8D",
        "outputId": "565b7a7f-28fe-4730-c30a-f0dee1d93a26"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running direct tests...\n",
            "\n",
            "Running test_create_and_verify_material...\n",
            "test_create_and_verify_material: PASSED\n",
            "\n",
            "Running test_default_values...\n",
            "test_default_values: PASSED\n",
            "\n",
            "Running test_duplicate_material_creation...\n",
            "test_duplicate_material_creation: PASSED\n",
            "\n",
            "Running test_empty_input_handling...\n",
            "test_empty_input_handling: PASSED\n",
            "\n",
            "Running test_get_material_master_data...\n",
            "test_get_material_master_data: PASSED\n",
            "\n",
            "Running test_initial_material_check...\n",
            "test_initial_material_check: PASSED\n",
            "\n",
            "Running test_invalid_material_check...\n",
            "test_invalid_material_check: PASSED\n",
            "\n",
            "Running test_invalid_plant_check...\n",
            "test_invalid_plant_check: PASSED\n",
            "\n",
            "Running test_material_creation_without_id...\n",
            "test_material_creation_without_id: PASSED\n",
            "\n",
            "Running test_missing_required_fields...\n",
            "test_missing_required_fields: PASSED\n",
            "\n",
            "Test Summary:\n",
            "Passed: 10\n",
            "Failed: 0\n",
            "Total: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Script 4 :  Local Colab P2P tests\n",
        "\n",
        "# Cell for P2P testing\n",
        "from test_harness.mock_sap.p2p_apis import P2PSimulator\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "async def test_p2p_flow():\n",
        "    p2p = P2PSimulator()\n",
        "\n",
        "    # Create PR\n",
        "    pr_data = {\n",
        "        'material_id': 'MAT001',\n",
        "        'quantity': 100,\n",
        "        'delivery_date': (datetime.now() + timedelta(days=30)).strftime('%Y-%m-%d'),\n",
        "        'plant': 'PLANT_1'\n",
        "    }\n",
        "\n",
        "    pr_response = await p2p.create_purchase_requisition(pr_data)\n",
        "    print(\"PR Creation Response:\")\n",
        "    print(f\"Success: {pr_response.success}\")\n",
        "    print(f\"Data: {pr_response.data}\")\n",
        "\n",
        "    if pr_response.success:\n",
        "        # Create PO\n",
        "        po_data = {\n",
        "            'pr_number': pr_response.data['pr_number'],\n",
        "            'vendor_id': 'VENDOR001'\n",
        "        }\n",
        "\n",
        "        po_response = await p2p.create_purchase_order(po_data)\n",
        "        print(\"\\nPO Creation Response:\")\n",
        "        print(f\"Success: {po_response.success}\")\n",
        "        print(f\"Data: {po_response.data}\")\n",
        "\n",
        "        # Check document status\n",
        "        status_response = await p2p.check_document_status(\n",
        "            pr_response.data['pr_number'],\n",
        "            'PR'\n",
        "        )\n",
        "        print(\"\\nPR Status Response:\")\n",
        "        print(f\"Success: {status_response.success}\")\n",
        "        print(f\"Data: {status_response.data}\")\n",
        "\n",
        "# Execute test\n",
        "await test_p2p_flow()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ai4uj_w960QA",
        "outputId": "b07bc0a8-b8de-4d7e-92a8-40de7c15fecb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PR Creation Response:\n",
            "Success: True\n",
            "Data: {'pr_number': 'PR0000000001', 'status': 'CREATED'}\n",
            "\n",
            "PO Creation Response:\n",
            "Success: True\n",
            "Data: {'po_number': 'PO0000000001', 'status': 'CREATED'}\n",
            "\n",
            "PR Status Response:\n",
            "Success: True\n",
            "Data: {'document_number': 'PR0000000001', 'document_type': 'PR', 'status': 'ORDERED', 'created_at': '2025-02-10T09:57:11.261667'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Script 5 :  Test Runner for State Management tests- Output ?\n",
        "\n",
        "async def run_state_management_tests():\n",
        "    \"\"\"Run state management test methods sequentially and collect results\"\"\"\n",
        "    from test_harness.mock_sap.p2p_apis import P2PSimulator\n",
        "    from datetime import datetime, timedelta\n",
        "    import inspect\n",
        "    import traceback\n",
        "    import time\n",
        "    import json\n",
        "\n",
        "    print(\"Starting State Management Test Execution...\\n\")\n",
        "\n",
        "    # Test results collection\n",
        "    results = {\n",
        "        'total_checks': 0,\n",
        "        'passed_checks': 0,\n",
        "        'failed_checks': 0,\n",
        "        'failures': [],\n",
        "        'state_snapshots': []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Initialize simulator\n",
        "        p2p = P2PSimulator()\n",
        "\n",
        "        # Test data\n",
        "        pr_data = {\n",
        "            'material_id': 'MAT001',\n",
        "            'quantity': 100,\n",
        "            'delivery_date': (datetime.now() + timedelta(days=30)).strftime('%Y-%m-%d'),\n",
        "            'plant': 'PLANT_1'\n",
        "        }\n",
        "\n",
        "        print(\"Test Phase 1: PR Creation and Initial State\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Create PR\n",
        "        pr_response = await p2p.create_purchase_requisition(pr_data)\n",
        "        results['total_checks'] += 1\n",
        "\n",
        "        if pr_response.success:\n",
        "            results['passed_checks'] += 1\n",
        "            print(\"PR Creation: SUCCESS\")\n",
        "        else:\n",
        "            results['failed_checks'] += 1\n",
        "            print(f\"PR Creation: FAILED - {pr_response.error.message}\")\n",
        "\n",
        "        # Check initial state\n",
        "        initial_state = p2p.get_state()\n",
        "        results['state_snapshots'].append(('Initial State', initial_state))\n",
        "\n",
        "        print(\"\\nInitial State Verification:\")\n",
        "        print(\"-\" * 25)\n",
        "\n",
        "        # Verify PR state\n",
        "        if pr_response.success:\n",
        "            pr_number = pr_response.data['pr_number']\n",
        "            try:\n",
        "                # Verify PR exists\n",
        "                assert pr_number in initial_state['purchase_requisitions']\n",
        "                print(\"✓ PR exists in state\")\n",
        "                results['passed_checks'] += 1\n",
        "\n",
        "                # Verify PR data\n",
        "                pr_record = initial_state['purchase_requisitions'][pr_number]\n",
        "                assert pr_record['material_id'] == pr_data['material_id']\n",
        "                assert pr_record['quantity'] == pr_data['quantity']\n",
        "                assert pr_record['plant'] == pr_data['plant']\n",
        "                assert pr_record['status'] == 'CREATED'\n",
        "                print(\"✓ PR data is correct\")\n",
        "                results['passed_checks'] += 1\n",
        "\n",
        "            except AssertionError as ae:\n",
        "                print(f\"✗ State verification failed: {str(ae)}\")\n",
        "                results['failed_checks'] += 1\n",
        "                results['failures'].append({\n",
        "                    'phase': 'Initial State',\n",
        "                    'error': str(ae)\n",
        "                })\n",
        "\n",
        "        print(\"\\nTest Phase 2: PO Creation and Final State\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        if pr_response.success:\n",
        "            # Create PO\n",
        "            po_data = {\n",
        "                'pr_number': pr_response.data['pr_number'],\n",
        "                'vendor_id': 'VENDOR001'\n",
        "            }\n",
        "\n",
        "            po_response = await p2p.create_purchase_order(po_data)\n",
        "            results['total_checks'] += 1\n",
        "\n",
        "            if po_response.success:\n",
        "                results['passed_checks'] += 1\n",
        "                print(\"PO Creation: SUCCESS\")\n",
        "            else:\n",
        "                results['failed_checks'] += 1\n",
        "                print(f\"PO Creation: FAILED - {po_response.error.message}\")\n",
        "\n",
        "            # Check final state\n",
        "            final_state = p2p.get_state()\n",
        "            results['state_snapshots'].append(('Final State', final_state))\n",
        "\n",
        "            print(\"\\nFinal State Verification:\")\n",
        "            print(\"-\" * 25)\n",
        "\n",
        "            try:\n",
        "                # Verify PR status change\n",
        "                pr_record = final_state['purchase_requisitions'][pr_number]\n",
        "                assert pr_record['status'] == 'ORDERED'\n",
        "                print(\"✓ PR status updated to ORDERED\")\n",
        "                results['passed_checks'] += 1\n",
        "\n",
        "                # Verify PO creation\n",
        "                if po_response.success:\n",
        "                    po_number = po_response.data['po_number']\n",
        "                    assert po_number in final_state['purchase_orders']\n",
        "\n",
        "                    po_record = final_state['purchase_orders'][po_number]\n",
        "                    assert po_record['pr_number'] == pr_number\n",
        "                    assert po_record['vendor_id'] == po_data['vendor_id']\n",
        "                    assert po_record['status'] == 'CREATED'\n",
        "                    print(\"✓ PO data is correct\")\n",
        "                    results['passed_checks'] += 1\n",
        "\n",
        "            except AssertionError as ae:\n",
        "                print(f\"✗ State verification failed: {str(ae)}\")\n",
        "                results['failed_checks'] += 1\n",
        "                results['failures'].append({\n",
        "                    'phase': 'Final State',\n",
        "                    'error': str(ae)\n",
        "                })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTest execution failed: {str(e)}\")\n",
        "        print(traceback.format_exc())\n",
        "        results['failures'].append({\n",
        "            'phase': 'Execution',\n",
        "            'error': str(e),\n",
        "            'traceback': traceback.format_exc()\n",
        "        })\n",
        "\n",
        "    finally:\n",
        "        # Print summary\n",
        "        print(\"\\n\" + \"=\"* 50)\n",
        "        print(\"State Management Test Summary\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Total Checks : {results['total_checks']}\")\n",
        "        print(f\"Passed       : {results['passed_checks']}\")\n",
        "        print(f\"Failed       : {results['failed_checks']}\")\n",
        "\n",
        "        # Print state snapshots\n",
        "        print(\"\\nState Snapshots:\")\n",
        "        print(\"=\" * 50)\n",
        "        for phase, state in results['state_snapshots']:\n",
        "            print(f\"\\n{phase}:\")\n",
        "            print(json.dumps(state, indent=2))\n",
        "\n",
        "        if results['failures']:\n",
        "            print(\"\\nFailure Details:\")\n",
        "            print(\"-\" * 50)\n",
        "            for i, failure in enumerate(results['failures'], 1):\n",
        "                print(f\"\\n{i}. Phase: {failure['phase']}\")\n",
        "                print(f\"   Error: {failure['error']}\")\n",
        "                if 'traceback' in failure:\n",
        "                    print(f\"   Traceback:\")\n",
        "                    print(\"   \" + \"\\n   \".join(failure['traceback'].split('\\n')))\n",
        "\n",
        "# Run tests\n",
        "import asyncio\n",
        "await run_state_management_tests()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pLeVxhCCs3D",
        "outputId": "a2a4e282-3eb0-4788-a76a-e160208814d0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting State Management Test Execution...\n",
            "\n",
            "Test Phase 1: PR Creation and Initial State\n",
            "--------------------------------------------------\n",
            "PR Creation: SUCCESS\n",
            "\n",
            "Initial State Verification:\n",
            "-------------------------\n",
            "✓ PR exists in state\n",
            "✓ PR data is correct\n",
            "\n",
            "Test Phase 2: PO Creation and Final State\n",
            "--------------------------------------------------\n",
            "PO Creation: SUCCESS\n",
            "\n",
            "Final State Verification:\n",
            "-------------------------\n",
            "✓ PR status updated to ORDERED\n",
            "✓ PO data is correct\n",
            "\n",
            "==================================================\n",
            "State Management Test Summary\n",
            "==================================================\n",
            "Total Checks : 2\n",
            "Passed       : 6\n",
            "Failed       : 0\n",
            "\n",
            "State Snapshots:\n",
            "==================================================\n",
            "\n",
            "Initial State:\n",
            "{\n",
            "  \"purchase_requisitions\": {\n",
            "    \"PR0000000001\": {\n",
            "      \"pr_number\": \"PR0000000001\",\n",
            "      \"material_id\": \"MAT001\",\n",
            "      \"quantity\": 100,\n",
            "      \"delivery_date\": \"2025-03-12\",\n",
            "      \"plant\": \"PLANT_1\",\n",
            "      \"status\": \"ORDERED\",\n",
            "      \"created_at\": \"2025-02-10T09:57:17.800967\",\n",
            "      \"material_data\": {\n",
            "        \"material_id\": \"MAT001\",\n",
            "        \"plant\": \"PLANT_1\",\n",
            "        \"description\": \"Raw Material A\",\n",
            "        \"base_unit\": \"KG\",\n",
            "        \"unrestricted_stock\": 1000.0,\n",
            "        \"storage_location\": \"A01\",\n",
            "        \"valuation\": {\n",
            "          \"standard_price\": 10.0,\n",
            "          \"price_unit\": 1,\n",
            "          \"currency\": \"USD\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"purchase_orders\": {\n",
            "    \"PO0000000001\": {\n",
            "      \"po_number\": \"PO0000000001\",\n",
            "      \"pr_number\": \"PR0000000001\",\n",
            "      \"vendor_id\": \"VENDOR001\",\n",
            "      \"material_id\": \"MAT001\",\n",
            "      \"quantity\": 100,\n",
            "      \"delivery_date\": \"2025-03-12\",\n",
            "      \"plant\": \"PLANT_1\",\n",
            "      \"status\": \"CREATED\",\n",
            "      \"created_at\": \"2025-02-10T09:57:17.805082\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "Final State:\n",
            "{\n",
            "  \"purchase_requisitions\": {\n",
            "    \"PR0000000001\": {\n",
            "      \"pr_number\": \"PR0000000001\",\n",
            "      \"material_id\": \"MAT001\",\n",
            "      \"quantity\": 100,\n",
            "      \"delivery_date\": \"2025-03-12\",\n",
            "      \"plant\": \"PLANT_1\",\n",
            "      \"status\": \"ORDERED\",\n",
            "      \"created_at\": \"2025-02-10T09:57:17.800967\",\n",
            "      \"material_data\": {\n",
            "        \"material_id\": \"MAT001\",\n",
            "        \"plant\": \"PLANT_1\",\n",
            "        \"description\": \"Raw Material A\",\n",
            "        \"base_unit\": \"KG\",\n",
            "        \"unrestricted_stock\": 1000.0,\n",
            "        \"storage_location\": \"A01\",\n",
            "        \"valuation\": {\n",
            "          \"standard_price\": 10.0,\n",
            "          \"price_unit\": 1,\n",
            "          \"currency\": \"USD\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"purchase_orders\": {\n",
            "    \"PO0000000001\": {\n",
            "      \"po_number\": \"PO0000000001\",\n",
            "      \"pr_number\": \"PR0000000001\",\n",
            "      \"vendor_id\": \"VENDOR001\",\n",
            "      \"material_id\": \"MAT001\",\n",
            "      \"quantity\": 100,\n",
            "      \"delivery_date\": \"2025-03-12\",\n",
            "      \"plant\": \"PLANT_1\",\n",
            "      \"status\": \"CREATED\",\n",
            "      \"created_at\": \"2025-02-10T09:57:17.805082\"\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Script 6:- State Mgmt\n",
        "\n",
        "async def run_state_management_tests():\n",
        "    \"\"\"Run state management test methods sequentially and collect results\"\"\"\n",
        "    from test_harness.mock_sap.p2p_apis import P2PSimulator\n",
        "    from datetime import datetime, timedelta\n",
        "    import traceback\n",
        "    import time\n",
        "\n",
        "    print(\"Starting State Management Test Execution...\\n\")\n",
        "\n",
        "    # Test results collection\n",
        "    results = {\n",
        "        'total': 0,\n",
        "        'passed': 0,\n",
        "        'failed': 0,\n",
        "        'failures': [],\n",
        "        'execution_time': {}\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Initialize simulator\n",
        "        p2p = P2PSimulator()\n",
        "\n",
        "        # Test data\n",
        "        pr_data = {\n",
        "            'material_id': 'MAT001',\n",
        "            'quantity': 100,\n",
        "            'delivery_date': (datetime.now() + timedelta(days=30)).strftime('%Y-%m-%d'),\n",
        "            'plant': 'PLANT_1'\n",
        "        }\n",
        "\n",
        "        # Test Case 1: Initial State After PR Creation\n",
        "        print(\"\\nTest Case 1: Initial State After PR Creation\")\n",
        "        print(\"-\" * 50)\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            pr_response = await p2p.create_purchase_requisition(pr_data)\n",
        "            initial_state = p2p.get_state()\n",
        "\n",
        "            assert pr_response.success, \"PR creation failed\"\n",
        "            assert pr_response.data['pr_number'] in initial_state['purchase_requisitions']\n",
        "\n",
        "            results['passed'] += 1\n",
        "            print(\"Result: PASSED\")\n",
        "        except Exception as e:\n",
        "            results['failed'] += 1\n",
        "            results['failures'].append({\n",
        "                'test': 'initial_state',\n",
        "                'error': str(e),\n",
        "                'traceback': traceback.format_exc()\n",
        "            })\n",
        "            print(f\"Result: FAILED - {str(e)}\")\n",
        "        finally:\n",
        "            results['total'] += 1\n",
        "            results['execution_time']['initial_state'] = time.time() - start_time\n",
        "\n",
        "        # Test Case 2: State Transition After PO Creation\n",
        "        if pr_response.success:\n",
        "            print(\"\\nTest Case 2: State Transition After PO Creation\")\n",
        "            print(\"-\" * 50)\n",
        "            start_time = time.time()\n",
        "\n",
        "            try:\n",
        "                po_data = {\n",
        "                    'pr_number': pr_response.data['pr_number'],\n",
        "                    'vendor_id': 'VENDOR001'\n",
        "                }\n",
        "\n",
        "                po_response = await p2p.create_purchase_order(po_data)\n",
        "                final_state = p2p.get_state()\n",
        "\n",
        "                assert po_response.success, \"PO creation failed\"\n",
        "                assert po_response.data['po_number'] in final_state['purchase_orders']\n",
        "                assert final_state['purchase_requisitions'][pr_response.data['pr_number']]['status'] == 'ORDERED'\n",
        "\n",
        "                results['passed'] += 1\n",
        "                print(\"Result: PASSED\")\n",
        "            except Exception as e:\n",
        "                results['failed'] += 1\n",
        "                results['failures'].append({\n",
        "                    'test': 'state_transition',\n",
        "                    'error': str(e),\n",
        "                    'traceback': traceback.format_exc()\n",
        "                })\n",
        "                print(f\"Result: FAILED - {str(e)}\")\n",
        "            finally:\n",
        "                results['total'] += 1\n",
        "                results['execution_time']['state_transition'] = time.time() - start_time\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTest suite execution failed: {str(e)}\")\n",
        "        print(traceback.format_exc())\n",
        "\n",
        "    finally:\n",
        "        # Print summary\n",
        "        print(\"\\n\" + \"=\"* 50)\n",
        "        print(\"State Management Test Execution Summary\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Total Tests   : {results['total']}\")\n",
        "        print(f\"Tests Passed  : {results['passed']}\")\n",
        "        print(f\"Tests Failed  : {results['failed']}\")\n",
        "\n",
        "        # Print timing information\n",
        "        print(\"\\nExecution Times:\")\n",
        "        print(\"-\" * 50)\n",
        "        for test_name, exec_time in results['execution_time'].items():\n",
        "            print(f\"{test_name}: {exec_time:.3f}s\")\n",
        "\n",
        "        if results['failures']:\n",
        "            print(\"\\nFailure Details:\")\n",
        "            print(\"-\" * 50)\n",
        "            for i, failure in enumerate(results['failures'], 1):\n",
        "                print(f\"\\n{i}. Test: {failure['test']}\")\n",
        "                print(f\"   Error: {failure['error']}\")\n",
        "                print(f\"   Traceback:\")\n",
        "                print(\"   \" + \"\\n   \".join(failure['traceback'].split('\\n')))\n",
        "\n",
        "# Run tests\n",
        "import asyncio\n",
        "await run_state_management_tests()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1mm5ZU8TRP0",
        "outputId": "3da16ec9-d049-4917-d357-8f4fbb214c11"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting State Management Test Execution...\n",
            "\n",
            "\n",
            "Test Case 1: Initial State After PR Creation\n",
            "--------------------------------------------------\n",
            "Result: PASSED\n",
            "\n",
            "Test Case 2: State Transition After PO Creation\n",
            "--------------------------------------------------\n",
            "Result: PASSED\n",
            "\n",
            "==================================================\n",
            "State Management Test Execution Summary\n",
            "==================================================\n",
            "Total Tests   : 2\n",
            "Tests Passed  : 2\n",
            "Tests Failed  : 0\n",
            "\n",
            "Execution Times:\n",
            "--------------------------------------------------\n",
            "initial_state: 0.000s\n",
            "state_transition: 0.000s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Script 7 - p2p tests\n",
        "\n",
        "async def run_p2p_tests():\n",
        "    \"\"\"Run P2P test methods sequentially and collect results\"\"\"\n",
        "    from test_harness.tests.test_p2p_flow import TestP2PFlow\n",
        "    import inspect\n",
        "    import traceback\n",
        "    import time\n",
        "    import asyncio\n",
        "\n",
        "    print(\"Starting P2P Test Execution...\\n\")\n",
        "\n",
        "    # Test results collection\n",
        "    results = {\n",
        "        'total': 0,\n",
        "        'passed': 0,\n",
        "        'failed': 0,\n",
        "        'failures': [],\n",
        "        'execution_time': {}\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Create test instance\n",
        "        test_instance = TestP2PFlow()\n",
        "\n",
        "        # Run setUp\n",
        "        test_instance.setUp()\n",
        "\n",
        "        # Get all test methods (ensure we get them in defined order)\n",
        "        test_methods = [\n",
        "            method[0] for method in inspect.getmembers(test_instance, predicate=inspect.ismethod)\n",
        "            if method[0].startswith('test_')\n",
        "        ]\n",
        "        test_methods.sort()  # Ensure consistent ordering\n",
        "        results['total'] = len(test_methods)\n",
        "\n",
        "        # Run each test method sequentially\n",
        "        for method_name in test_methods:\n",
        "            print(f\"\\nExecuting: {method_name}\")\n",
        "            print(\"-\" * (len(method_name) + 11))\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            try:\n",
        "                # Reset state before each test\n",
        "                test_instance.setUp()\n",
        "\n",
        "                # Get the original method\n",
        "                method = getattr(test_instance, method_name)\n",
        "\n",
        "                # Check if method is coroutine and execute appropriately\n",
        "                if inspect.iscoroutinefunction(method):\n",
        "                    await method()\n",
        "                else:\n",
        "                    method()\n",
        "\n",
        "                execution_time = time.time() - start_time\n",
        "                results['execution_time'][method_name] = execution_time\n",
        "\n",
        "                print(f\"Result: PASSED (Time: {execution_time:.3f}s)\")\n",
        "                results['passed'] += 1\n",
        "\n",
        "            except AssertionError as ae:\n",
        "                execution_time = time.time() - start_time\n",
        "                results['execution_time'][method_name] = execution_time\n",
        "\n",
        "                print(f\"Result: FAILED (Time: {execution_time:.3f}s)\")\n",
        "                print(f\"Error: {str(ae)}\")\n",
        "                results['failed'] += 1\n",
        "                results['failures'].append({\n",
        "                    'test': method_name,\n",
        "                    'type': 'AssertionError',\n",
        "                    'message': str(ae),\n",
        "                    'traceback': traceback.format_exc()\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                execution_time = time.time() - start_time\n",
        "                results['execution_time'][method_name] = execution_time\n",
        "\n",
        "                print(f\"Result: FAILED (Time: {execution_time:.3f}s)\")\n",
        "                print(f\"Error: {type(e).__name__} - {str(e)}\")\n",
        "                results['failed'] += 1\n",
        "                results['failures'].append({\n",
        "                    'test': method_name,\n",
        "                    'type': type(e).__name__,\n",
        "                    'message': str(e),\n",
        "                    'traceback': traceback.format_exc()\n",
        "                })\n",
        "\n",
        "            finally:\n",
        "                # Run tearDown after each test\n",
        "                if hasattr(test_instance, 'tearDown'):\n",
        "                    test_instance.tearDown()\n",
        "\n",
        "            # Add a small delay between tests\n",
        "            await asyncio.sleep(0.1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTest suite execution failed: {str(e)}\")\n",
        "        print(traceback.format_exc())\n",
        "\n",
        "    finally:\n",
        "        # Print summary\n",
        "        print(\"\\n\" + \"=\"* 50)\n",
        "        print(\"P2P Test Execution Summary\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Total Tests   : {results['total']}\")\n",
        "        print(f\"Tests Passed  : {results['passed']}\")\n",
        "        print(f\"Tests Failed  : {results['failed']}\")\n",
        "\n",
        "        # Print timing information\n",
        "        print(\"\\nExecution Times:\")\n",
        "        print(\"-\" * 50)\n",
        "        test_times = sorted(\n",
        "            results['execution_time'].items(),\n",
        "            key=lambda x: x[1],\n",
        "            reverse=True\n",
        "        )\n",
        "        for method_name, exec_time in test_times:\n",
        "            print(f\"{method_name}: {exec_time:.3f}s\")\n",
        "\n",
        "        if results['failures']:\n",
        "            print(\"\\nFailure Details:\")\n",
        "            print(\"-\" * 50)\n",
        "            for i, failure in enumerate(results['failures'], 1):\n",
        "                print(f\"\\n{i}. Test: {failure['test']}\")\n",
        "                print(f\"   Type: {failure['type']}\")\n",
        "                print(f\"   Message: {failure['message']}\")\n",
        "                print(f\"   Traceback:\")\n",
        "                print(\"   \" + \"\\n   \".join(failure['traceback'].split('\\n')))\n",
        "\n",
        "# Import required modules and run tests\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops (needed for Colab)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Run tests\n",
        "await run_p2p_tests()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNOkXyyUc0hZ",
        "outputId": "7352767c-eec1-40b2-96d1-89f9f336e47c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting P2P Test Execution...\n",
            "\n",
            "\n",
            "Executing: test_document_status_check\n",
            "-------------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_error_invalid_material\n",
            "--------------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_error_invalid_plant\n",
            "-----------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_error_invalid_pr_reference\n",
            "------------------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_error_invalid_vendor\n",
            "------------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_error_pr_already_ordered\n",
            "----------------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_error_zero_quantity\n",
            "-----------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_multi_create_pos\n",
            "--------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_multi_create_prs\n",
            "--------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_multi_document_state_consistency\n",
            "------------------------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_multi_document_validation\n",
            "-----------------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_multi_partial_ordering\n",
            "--------------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_po_creation\n",
            "---------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_pr_creation\n",
            "---------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "==================================================\n",
            "P2P Test Execution Summary\n",
            "==================================================\n",
            "Total Tests   : 14\n",
            "Tests Passed  : 14\n",
            "Tests Failed  : 0\n",
            "\n",
            "Execution Times:\n",
            "--------------------------------------------------\n",
            "test_multi_create_pos: 0.000s\n",
            "test_multi_document_validation: 0.000s\n",
            "test_error_pr_already_ordered: 0.000s\n",
            "test_multi_create_prs: 0.000s\n",
            "test_multi_document_state_consistency: 0.000s\n",
            "test_multi_partial_ordering: 0.000s\n",
            "test_po_creation: 0.000s\n",
            "test_error_zero_quantity: 0.000s\n",
            "test_pr_creation: 0.000s\n",
            "test_document_status_check: 0.000s\n",
            "test_error_invalid_vendor: 0.000s\n",
            "test_error_invalid_material: 0.000s\n",
            "test_error_invalid_pr_reference: 0.000s\n",
            "test_error_invalid_plant: 0.000s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Script 8 - error tests\n",
        "\n",
        "async def run_error_test_cases():\n",
        "    \"\"\"Run error test cases sequentially and collect results\"\"\"\n",
        "    from test_harness.tests.test_p2p_flow import TestP2PFlow\n",
        "    import inspect\n",
        "    import traceback\n",
        "    import time\n",
        "\n",
        "    print(\"Starting Error Test Cases Execution...\\n\")\n",
        "\n",
        "    # Test results collection\n",
        "    results = {\n",
        "        'total': 0,\n",
        "        'passed': 0,\n",
        "        'failed': 0,\n",
        "        'failures': [],\n",
        "        'execution_time': {}\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Create test instance\n",
        "        test_instance = TestP2PFlow()\n",
        "\n",
        "        # Get error test methods (ensure we get them in defined order)\n",
        "        test_methods = [\n",
        "            method[0] for method in inspect.getmembers(test_instance, predicate=inspect.ismethod)\n",
        "            if method[0].startswith('test_error_')  # Only run error test cases\n",
        "        ]\n",
        "        test_methods.sort()  # Ensure consistent ordering\n",
        "        results['total'] = len(test_methods)\n",
        "\n",
        "        # Run each test method sequentially\n",
        "        for method_name in test_methods:\n",
        "            print(f\"\\nExecuting: {method_name}\")\n",
        "            print(\"-\" * (len(method_name) + 11))\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            try:\n",
        "                # Reset state for each test\n",
        "                test_instance.setUp()\n",
        "\n",
        "                # Get the test method\n",
        "                method = getattr(test_instance, method_name)\n",
        "\n",
        "                # Execute test\n",
        "                await method()\n",
        "\n",
        "                execution_time = time.time() - start_time\n",
        "                results['execution_time'][method_name] = execution_time\n",
        "\n",
        "                print(f\"Result: PASSED (Time: {execution_time:.3f}s)\")\n",
        "                results['passed'] += 1\n",
        "\n",
        "            except AssertionError as ae:\n",
        "                execution_time = time.time() - start_time\n",
        "                results['execution_time'][method_name] = execution_time\n",
        "\n",
        "                print(f\"Result: FAILED (Time: {execution_time:.3f}s)\")\n",
        "                print(f\"Error: {str(ae)}\")\n",
        "                results['failed'] += 1\n",
        "                results['failures'].append({\n",
        "                    'test': method_name,\n",
        "                    'type': 'AssertionError',\n",
        "                    'message': str(ae),\n",
        "                    'traceback': traceback.format_exc()\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                execution_time = time.time() - start_time\n",
        "                results['execution_time'][method_name] = execution_time\n",
        "\n",
        "                print(f\"Result: FAILED (Time: {execution_time:.3f}s)\")\n",
        "                print(f\"Error: {type(e).__name__} - {str(e)}\")\n",
        "                results['failed'] += 1\n",
        "                results['failures'].append({\n",
        "                    'test': method_name,\n",
        "                    'type': type(e).__name__,\n",
        "                    'message': str(e),\n",
        "                    'traceback': traceback.format_exc()\n",
        "                })\n",
        "\n",
        "            finally:\n",
        "                # Clean up after each test\n",
        "                test_instance.tearDown()\n",
        "\n",
        "            # Add a small delay between tests\n",
        "            await asyncio.sleep(0.1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTest suite execution failed: {str(e)}\")\n",
        "        print(traceback.format_exc())\n",
        "\n",
        "    finally:\n",
        "        # Print summary\n",
        "        print(\"\\n\" + \"=\"* 50)\n",
        "        print(\"Error Test Cases Summary\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Total Tests   : {results['total']}\")\n",
        "        print(f\"Tests Passed  : {results['passed']}\")\n",
        "        print(f\"Tests Failed  : {results['failed']}\")\n",
        "\n",
        "        # Print timing information\n",
        "        print(\"\\nExecution Times:\")\n",
        "        print(\"-\" * 50)\n",
        "        test_times = sorted(\n",
        "            results['execution_time'].items(),\n",
        "            key=lambda x: x[1],\n",
        "            reverse=True\n",
        "        )\n",
        "        for method_name, exec_time in test_times:\n",
        "            print(f\"{method_name}: {exec_time:.3f}s\")\n",
        "\n",
        "        if results['failures']:\n",
        "            print(\"\\nFailure Details:\")\n",
        "            print(\"-\" * 50)\n",
        "            for i, failure in enumerate(results['failures'], 1):\n",
        "                print(f\"\\n{i}. Test: {failure['test']}\")\n",
        "                print(f\"   Type: {failure['type']}\")\n",
        "                print(f\"   Message: {failure['message']}\")\n",
        "                print(f\"   Traceback:\")\n",
        "                print(\"   \" + \"\\n   \".join(failure['traceback'].split('\\n')))\n",
        "\n",
        "# Import required modules and run tests\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops (needed for Colab)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Run tests\n",
        "await run_error_test_cases()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cyyk_OqEk18m",
        "outputId": "7dcfe2a9-40c0-4b4a-f9d5-773d0e7699c8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Error Test Cases Execution...\n",
            "\n",
            "\n",
            "Executing: test_error_invalid_material\n",
            "--------------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_error_invalid_plant\n",
            "-----------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_error_invalid_pr_reference\n",
            "------------------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_error_invalid_vendor\n",
            "------------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_error_pr_already_ordered\n",
            "----------------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_error_zero_quantity\n",
            "-----------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "==================================================\n",
            "Error Test Cases Summary\n",
            "==================================================\n",
            "Total Tests   : 6\n",
            "Tests Passed  : 6\n",
            "Tests Failed  : 0\n",
            "\n",
            "Execution Times:\n",
            "--------------------------------------------------\n",
            "test_error_invalid_vendor: 0.000s\n",
            "test_error_pr_already_ordered: 0.000s\n",
            "test_error_zero_quantity: 0.000s\n",
            "test_error_invalid_plant: 0.000s\n",
            "test_error_invalid_pr_reference: 0.000s\n",
            "test_error_invalid_material: 0.000s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Script 9 - Multi Document\n",
        "\n",
        "async def run_multi_document_tests():\n",
        "    \"\"\"Run multi-document test cases sequentially and collect results\"\"\"\n",
        "    from test_harness.tests.test_p2p_flow import TestP2PFlow\n",
        "    import inspect\n",
        "    import traceback\n",
        "    import time\n",
        "    import sys\n",
        "\n",
        "    print(\"Starting Multi-Document Test Execution...\\n\")\n",
        "\n",
        "    # Test results collection\n",
        "    results = {\n",
        "        'total': 0,\n",
        "        'passed': 0,\n",
        "        'failed': 0,\n",
        "        'failures': [],\n",
        "        'execution_time': {},\n",
        "        'skipped': []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Create test instance\n",
        "        test_instance = TestP2PFlow()\n",
        "\n",
        "        # Get all test methods\n",
        "        all_methods = inspect.getmembers(test_instance, predicate=inspect.ismethod)\n",
        "\n",
        "        # Filter for multi-document tests and sort\n",
        "        test_methods = [\n",
        "            method[0] for method in all_methods\n",
        "            if method[0].startswith('test_multi_')\n",
        "        ]\n",
        "        test_methods.sort()\n",
        "\n",
        "        if not test_methods:\n",
        "            print(\"No multi-document test cases found!\")\n",
        "            return\n",
        "\n",
        "        results['total'] = len(test_methods)\n",
        "        print(f\"Found {results['total']} multi-document test cases\\n\")\n",
        "\n",
        "        # Run each test method sequentially\n",
        "        for method_name in test_methods:\n",
        "            print(f\"\\nExecuting: {method_name}\")\n",
        "            print(\"-\" * (len(method_name) + 11))\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            try:\n",
        "                # Reset state for each test\n",
        "                test_instance.setUp()\n",
        "\n",
        "                # Get the test method\n",
        "                method = getattr(test_instance, method_name)\n",
        "\n",
        "                # Execute test with timeout protection\n",
        "                try:\n",
        "                    await method()\n",
        "                    execution_time = time.time() - start_time\n",
        "                    results['execution_time'][method_name] = execution_time\n",
        "\n",
        "                    print(f\"Result: PASSED (Time: {execution_time:.3f}s)\")\n",
        "                    results['passed'] += 1\n",
        "\n",
        "                except asyncio.TimeoutError:\n",
        "                    print(f\"Result: FAILED (Timeout)\")\n",
        "                    results['failed'] += 1\n",
        "                    results['failures'].append({\n",
        "                        'test': method_name,\n",
        "                        'type': 'TimeoutError',\n",
        "                        'message': 'Test execution timed out',\n",
        "                        'traceback': traceback.format_exc()\n",
        "                    })\n",
        "\n",
        "            except AssertionError as ae:\n",
        "                execution_time = time.time() - start_time\n",
        "                results['execution_time'][method_name] = execution_time\n",
        "\n",
        "                print(f\"Result: FAILED (Time: {execution_time:.3f}s)\")\n",
        "                print(f\"Error: {str(ae)}\")\n",
        "                results['failed'] += 1\n",
        "                results['failures'].append({\n",
        "                    'test': method_name,\n",
        "                    'type': 'AssertionError',\n",
        "                    'message': str(ae),\n",
        "                    'traceback': traceback.format_exc()\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                execution_time = time.time() - start_time\n",
        "                results['execution_time'][method_name] = execution_time\n",
        "\n",
        "                print(f\"Result: FAILED (Time: {execution_time:.3f}s)\")\n",
        "                print(f\"Error: {type(e).__name__} - {str(e)}\")\n",
        "                results['failed'] += 1\n",
        "                results['failures'].append({\n",
        "                    'test': method_name,\n",
        "                    'type': type(e).__name__,\n",
        "                    'message': str(e),\n",
        "                    'traceback': traceback.format_exc()\n",
        "                })\n",
        "\n",
        "            finally:\n",
        "                # Clean up after each test\n",
        "                try:\n",
        "                    test_instance.tearDown()\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Cleanup failed - {str(e)}\")\n",
        "\n",
        "            # Add a small delay between tests\n",
        "            await asyncio.sleep(0.1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTest suite execution failed: {str(e)}\")\n",
        "        print(traceback.format_exc())\n",
        "        return\n",
        "\n",
        "    finally:\n",
        "        # Print summary\n",
        "        print(\"\\n\" + \"=\"* 50)\n",
        "        print(\"Multi-Document Test Summary\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Total Tests    : {results['total']}\")\n",
        "        print(f\"Tests Passed   : {results['passed']}\")\n",
        "        print(f\"Tests Failed   : {results['failed']}\")\n",
        "        if results['skipped']:\n",
        "            print(f\"Tests Skipped  : {len(results['skipped'])}\")\n",
        "\n",
        "        if results['total'] > 0:\n",
        "            success_rate = (results['passed'] / results['total']) * 100\n",
        "            print(f\"Success Rate   : {success_rate:.1f}%\")\n",
        "\n",
        "        # Print timing information\n",
        "        if results['execution_time']:\n",
        "            print(\"\\nExecution Times (slowest to fastest):\")\n",
        "            print(\"-\" * 50)\n",
        "            test_times = sorted(\n",
        "                results['execution_time'].items(),\n",
        "                key=lambda x: x[1],\n",
        "                reverse=True\n",
        "            )\n",
        "\n",
        "            total_time = sum(t for _, t in test_times)\n",
        "            avg_time = total_time / len(test_times)\n",
        "\n",
        "            for method_name, exec_time in test_times:\n",
        "                print(f\"{method_name}: {exec_time:.3f}s\")\n",
        "\n",
        "            print(f\"\\nTotal Time: {total_time:.3f}s\")\n",
        "            print(f\"Average Time: {avg_time:.3f}s\")\n",
        "\n",
        "        if results['failures']:\n",
        "            print(\"\\nFailure Details:\")\n",
        "            print(\"-\" * 50)\n",
        "            for i, failure in enumerate(results['failures'], 1):\n",
        "                print(f\"\\n{i}. Test: {failure['test']}\")\n",
        "                print(f\"   Type: {failure['type']}\")\n",
        "                print(f\"   Message: {failure['message']}\")\n",
        "                print(f\"   Traceback:\")\n",
        "                print(\"   \" + \"\\n   \".join(failure['traceback'].split('\\n')))\n",
        "\n",
        "        if results['skipped']:\n",
        "            print(\"\\nSkipped Tests:\")\n",
        "            print(\"-\" * 50)\n",
        "            for test_name in results['skipped']:\n",
        "                print(f\"- {test_name}\")\n",
        "\n",
        "# Import required modules and run tests\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops (needed for Colab)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Run tests\n",
        "await run_multi_document_tests()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlLH0yE-J3Qf",
        "outputId": "b8a290e3-cfbd-4fce-9ac3-fa6ffda486fa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Multi-Document Test Execution...\n",
            "\n",
            "Found 5 multi-document test cases\n",
            "\n",
            "\n",
            "Executing: test_multi_create_pos\n",
            "--------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_multi_create_prs\n",
            "--------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_multi_document_state_consistency\n",
            "------------------------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_multi_document_validation\n",
            "-----------------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_multi_partial_ordering\n",
            "--------------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "==================================================\n",
            "Multi-Document Test Summary\n",
            "==================================================\n",
            "Total Tests    : 5\n",
            "Tests Passed   : 5\n",
            "Tests Failed   : 0\n",
            "Success Rate   : 100.0%\n",
            "\n",
            "Execution Times (slowest to fastest):\n",
            "--------------------------------------------------\n",
            "test_multi_document_state_consistency: 0.000s\n",
            "test_multi_partial_ordering: 0.000s\n",
            "test_multi_document_validation: 0.000s\n",
            "test_multi_create_pos: 0.000s\n",
            "test_multi_create_prs: 0.000s\n",
            "\n",
            "Total Time: 0.001s\n",
            "Average Time: 0.000s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Script 10 - Consolidated\n",
        "\n",
        "async def run_mvp_tests():\n",
        "    \"\"\"Run all MVP 0.1 test cases sequentially and collect results\"\"\"\n",
        "    from test_harness.tests.test_material_ops import TestMaterialOperations\n",
        "    from test_harness.tests.test_p2p_flow import TestP2PFlow\n",
        "    import inspect\n",
        "    import traceback\n",
        "    import time\n",
        "    import sys\n",
        "\n",
        "    print(\"Starting MVP 0.1 Test Execution...\\n\")\n",
        "\n",
        "    # Test results collection\n",
        "    results = {\n",
        "        'material_ops': {'passed': 0, 'failed': 0, 'total': 0},\n",
        "        'p2p_flow': {'passed': 0, 'failed': 0, 'total': 0},\n",
        "        'execution_time': {},\n",
        "        'failures': []\n",
        "    }\n",
        "\n",
        "    # Test mapping with explicit ordering\n",
        "    test_mapping = {\n",
        "        'Basic Material Operations': {\n",
        "            'class': TestMaterialOperations,\n",
        "            'methods': [\n",
        "                'test_initial_material_check',\n",
        "                'test_create_and_verify_material'\n",
        "            ]\n",
        "        },\n",
        "        'P2P Process Flow': {\n",
        "            'class': TestP2PFlow,\n",
        "            'methods': [\n",
        "                'test_pr_creation',\n",
        "                'test_po_creation'\n",
        "            ]\n",
        "        },\n",
        "        'Error Cases': {\n",
        "            'class': TestP2PFlow,\n",
        "            'methods': [\n",
        "                'test_error_invalid_material',\n",
        "                'test_error_pr_already_ordered'\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Run through each test category\n",
        "        for category, config in test_mapping.items():\n",
        "            print(f\"\\n{category}:\")\n",
        "            print(\"=\" * (len(category) + 1))\n",
        "\n",
        "            # Create test instance\n",
        "            test_instance = config['class']()\n",
        "\n",
        "            # Run tests in this category\n",
        "            for method_name in config['methods']:\n",
        "                print(f\"\\nExecuting: {method_name}\")\n",
        "                print(\"-\" * (len(method_name) + 11))\n",
        "\n",
        "                start_time = time.time()\n",
        "\n",
        "                try:\n",
        "                    # Reset state for each test\n",
        "                    test_instance.setUp()\n",
        "\n",
        "                    # Get the test method\n",
        "                    method = getattr(test_instance, method_name)\n",
        "\n",
        "                    # Execute test\n",
        "                    await method()\n",
        "\n",
        "                    execution_time = time.time() - start_time\n",
        "                    results['execution_time'][method_name] = execution_time\n",
        "\n",
        "                    print(f\"Result: PASSED (Time: {execution_time:.3f}s)\")\n",
        "                    if isinstance(test_instance, TestMaterialOperations):\n",
        "                        results['material_ops']['passed'] += 1\n",
        "                        results['material_ops']['total'] += 1\n",
        "                    else:\n",
        "                        results['p2p_flow']['passed'] += 1\n",
        "                        results['p2p_flow']['total'] += 1\n",
        "\n",
        "                except AssertionError as ae:\n",
        "                    execution_time = time.time() - start_time\n",
        "                    results['execution_time'][method_name] = execution_time\n",
        "\n",
        "                    print(f\"Result: FAILED (Time: {execution_time:.3f}s)\")\n",
        "                    print(f\"Error: {str(ae)}\")\n",
        "                    if isinstance(test_instance, TestMaterialOperations):\n",
        "                        results['material_ops']['failed'] += 1\n",
        "                        results['material_ops']['total'] += 1\n",
        "                    else:\n",
        "                        results['p2p_flow']['failed'] += 1\n",
        "                        results['p2p_flow']['total'] += 1\n",
        "\n",
        "                    results['failures'].append({\n",
        "                        'category': category,\n",
        "                        'test': method_name,\n",
        "                        'type': 'AssertionError',\n",
        "                        'message': str(ae),\n",
        "                        'traceback': traceback.format_exc()\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    execution_time = time.time() - start_time\n",
        "                    results['execution_time'][method_name] = execution_time\n",
        "\n",
        "                    print(f\"Result: FAILED (Time: {execution_time:.3f}s)\")\n",
        "                    print(f\"Error: {type(e).__name__} - {str(e)}\")\n",
        "                    if isinstance(test_instance, TestMaterialOperations):\n",
        "                        results['material_ops']['failed'] += 1\n",
        "                        results['material_ops']['total'] += 1\n",
        "                    else:\n",
        "                        results['p2p_flow']['failed'] += 1\n",
        "                        results['p2p_flow']['total'] += 1\n",
        "\n",
        "                    results['failures'].append({\n",
        "                        'category': category,\n",
        "                        'test': method_name,\n",
        "                        'type': type(e).__name__,\n",
        "                        'message': str(e),\n",
        "                        'traceback': traceback.format_exc()\n",
        "                    })\n",
        "\n",
        "                finally:\n",
        "                    # Clean up after each test\n",
        "                    try:\n",
        "                        test_instance.tearDown()\n",
        "                    except Exception as e:\n",
        "                        print(f\"Warning: Cleanup failed - {str(e)}\")\n",
        "\n",
        "                # Add a small delay between tests\n",
        "                await asyncio.sleep(0.1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTest suite execution failed: {str(e)}\")\n",
        "        print(traceback.format_exc())\n",
        "        return\n",
        "\n",
        "    finally:\n",
        "        # Print summary\n",
        "        print(\"\\n\" + \"=\"* 50)\n",
        "        print(\"MVP 0.1 Test Summary\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Material Operations Summary\n",
        "        print(\"\\nMaterial Operations Tests:\")\n",
        "        print(f\"Total Tests    : {results['material_ops']['total']}\")\n",
        "        print(f\"Tests Passed   : {results['material_ops']['passed']}\")\n",
        "        print(f\"Tests Failed   : {results['material_ops']['failed']}\")\n",
        "        if results['material_ops']['total'] > 0:\n",
        "            success_rate = (results['material_ops']['passed'] / results['material_ops']['total']) * 100\n",
        "            print(f\"Success Rate   : {success_rate:.1f}%\")\n",
        "\n",
        "        # P2P Flow Summary\n",
        "        print(\"\\nP2P Flow Tests:\")\n",
        "        print(f\"Total Tests    : {results['p2p_flow']['total']}\")\n",
        "        print(f\"Tests Passed   : {results['p2p_flow']['passed']}\")\n",
        "        print(f\"Tests Failed   : {results['p2p_flow']['failed']}\")\n",
        "        if results['p2p_flow']['total'] > 0:\n",
        "            success_rate = (results['p2p_flow']['passed'] / results['p2p_flow']['total']) * 100\n",
        "            print(f\"Success Rate   : {success_rate:.1f}%\")\n",
        "\n",
        "        # Overall Summary\n",
        "        total_tests = results['material_ops']['total'] + results['p2p_flow']['total']\n",
        "        total_passed = results['material_ops']['passed'] + results['p2p_flow']['passed']\n",
        "        if total_tests > 0:\n",
        "            overall_success_rate = (total_passed / total_tests) * 100\n",
        "            print(f\"\\nOverall Success Rate: {overall_success_rate:.1f}%\")\n",
        "\n",
        "        # Print timing information\n",
        "        if results['execution_time']:\n",
        "            print(\"\\nExecution Times (slowest to fastest):\")\n",
        "            print(\"-\" * 50)\n",
        "            test_times = sorted(\n",
        "                results['execution_time'].items(),\n",
        "                key=lambda x: x[1],\n",
        "                reverse=True\n",
        "            )\n",
        "\n",
        "            total_time = sum(t for _, t in test_times)\n",
        "            avg_time = total_time / len(test_times)\n",
        "\n",
        "            for method_name, exec_time in test_times:\n",
        "                print(f\"{method_name}: {exec_time:.3f}s\")\n",
        "\n",
        "            print(f\"\\nTotal Time: {total_time:.3f}s\")\n",
        "            print(f\"Average Time: {avg_time:.3f}s\")\n",
        "\n",
        "        if results['failures']:\n",
        "            print(\"\\nFailure Details:\")\n",
        "            print(\"-\" * 50)\n",
        "            for i, failure in enumerate(results['failures'], 1):\n",
        "                print(f\"\\n{i}. Category: {failure['category']}\")\n",
        "                print(f\"   Test: {failure['test']}\")\n",
        "                print(f\"   Type: {failure['type']}\")\n",
        "                print(f\"   Message: {failure['message']}\")\n",
        "                print(f\"   Traceback:\")\n",
        "                print(\"   \" + \"\\n   \".join(failure['traceback'].split('\\n')))\n",
        "\n",
        "# Import required modules and run tests\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops (needed for Colab)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Run tests\n",
        "await run_mvp_tests()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXHrtMqKNcGx",
        "outputId": "268eecd9-2bd1-48ef-c78d-0086823e8350"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting MVP 0.1 Test Execution...\n",
            "\n",
            "\n",
            "Basic Material Operations:\n",
            "==========================\n",
            "\n",
            "Executing: test_initial_material_check\n",
            "--------------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_create_and_verify_material\n",
            "------------------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "P2P Process Flow:\n",
            "=================\n",
            "\n",
            "Executing: test_pr_creation\n",
            "---------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_po_creation\n",
            "---------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Error Cases:\n",
            "============\n",
            "\n",
            "Executing: test_error_invalid_material\n",
            "--------------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "Executing: test_error_pr_already_ordered\n",
            "----------------------------------------\n",
            "Result: PASSED (Time: 0.000s)\n",
            "\n",
            "==================================================\n",
            "MVP 0.1 Test Summary\n",
            "==================================================\n",
            "\n",
            "Material Operations Tests:\n",
            "Total Tests    : 2\n",
            "Tests Passed   : 2\n",
            "Tests Failed   : 0\n",
            "Success Rate   : 100.0%\n",
            "\n",
            "P2P Flow Tests:\n",
            "Total Tests    : 4\n",
            "Tests Passed   : 4\n",
            "Tests Failed   : 0\n",
            "Success Rate   : 100.0%\n",
            "\n",
            "Overall Success Rate: 100.0%\n",
            "\n",
            "Execution Times (slowest to fastest):\n",
            "--------------------------------------------------\n",
            "test_error_pr_already_ordered: 0.000s\n",
            "test_po_creation: 0.000s\n",
            "test_pr_creation: 0.000s\n",
            "test_error_invalid_material: 0.000s\n",
            "test_create_and_verify_material: 0.000s\n",
            "test_initial_material_check: 0.000s\n",
            "\n",
            "Total Time: 0.001s\n",
            "Average Time: 0.000s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Script 2 (test runner calls) : Material Ops Runner V1\n",
        "from test_harness.tests.test_runner import run_material_tests\n",
        "from test_harness.tests.test_material_ops import TestMaterialOperations\n",
        "import asyncio\n",
        "import traceback\n",
        "import time\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "async def run_material_tests_v1() -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Run material operations tests with detailed output\n",
        "\n",
        "    Returns:\n",
        "        Optional[Dict[str, Any]]: Test results or None if execution failed\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        # Run tests with verbose output and timing\n",
        "        results = await run_material_tests(\n",
        "            TestMaterialOperations,\n",
        "            verbose=True,\n",
        "            include_timing=True\n",
        "        )\n",
        "\n",
        "        # Print results summary\n",
        "        print(\"\\n\" + \"=\"* 50)\n",
        "        print(\"Test Execution Summary\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Total Tests   : {results.total_tests}\")\n",
        "        print(f\"Tests Passed  : {results.passed_tests}\")\n",
        "        print(f\"Tests Failed  : {results.failed_tests}\")\n",
        "\n",
        "        # Print individual test results\n",
        "        print(\"\\nDetailed Test Results:\")\n",
        "        print(\"-\" * 50)\n",
        "        for test_result in results.results:\n",
        "            status = \"PASSED\" if test_result.success else \"FAILED\"\n",
        "            print(f\"\\nTest: {test_result.name}\")\n",
        "            print(f\"Status: {status}\")\n",
        "            print(f\"Time: {test_result.execution_time:.3f}s\")\n",
        "            if not test_result.success and test_result.error:\n",
        "                print(f\"Error: {test_result.error}\")\n",
        "\n",
        "        # Print overall execution time\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"\\nTotal Execution Time: {total_time:.3f}s\")\n",
        "\n",
        "        return {\n",
        "            'suite_results': results,\n",
        "            'total_time': total_time,\n",
        "            'timestamp': time.time()\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\nTest Execution Failed\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        print(\"\\nTraceback:\")\n",
        "        print(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "# Run tests\n",
        "try:\n",
        "    results = await run_material_tests_v1()\n",
        "    if results is None:\n",
        "        print(\"Test execution failed to complete\")\n",
        "except Exception as e:\n",
        "    print(f\"Fatal error in test execution: {str(e)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddd91SMXj5jW",
        "outputId": "b8cfa43d-f5ce-41a9-81c3-a43073515f50"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 09:58:39,056 - TestRunner-135660392493584 - INFO - Starting test suite: TestMaterialOperations\n",
            "INFO:TestRunner-135660392493584:Starting test suite: TestMaterialOperations\n",
            "2025-02-10 09:58:39,061 - TestRunner-135660392493584 - INFO - Found 10 test methods\n",
            "INFO:TestRunner-135660392493584:Found 10 test methods\n",
            "2025-02-10 09:58:39,065 - TestRunner-135660392493584 - INFO - Executing test: test_create_and_verify_material\n",
            "INFO:TestRunner-135660392493584:Executing test: test_create_and_verify_material\n",
            "2025-02-10 09:58:39,070 - TestRunner-135660392493584 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392493584:Running setUp\n",
            "2025-02-10 09:58:39,073 - TestRunner-135660392493584 - INFO - test_create_and_verify_material: PASSED (0.003s)\n",
            "INFO:TestRunner-135660392493584:test_create_and_verify_material: PASSED (0.003s)\n",
            "2025-02-10 09:58:39,075 - TestRunner-135660392493584 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392493584:Running tearDown\n",
            "2025-02-10 09:58:39,178 - TestRunner-135660392493584 - INFO - Executing test: test_default_values\n",
            "INFO:TestRunner-135660392493584:Executing test: test_default_values\n",
            "2025-02-10 09:58:39,184 - TestRunner-135660392493584 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392493584:Running setUp\n",
            "2025-02-10 09:58:39,188 - TestRunner-135660392493584 - INFO - test_default_values: PASSED (0.004s)\n",
            "INFO:TestRunner-135660392493584:test_default_values: PASSED (0.004s)\n",
            "2025-02-10 09:58:39,192 - TestRunner-135660392493584 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392493584:Running tearDown\n",
            "2025-02-10 09:58:39,298 - TestRunner-135660392493584 - INFO - Executing test: test_duplicate_material_creation\n",
            "INFO:TestRunner-135660392493584:Executing test: test_duplicate_material_creation\n",
            "2025-02-10 09:58:39,303 - TestRunner-135660392493584 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392493584:Running setUp\n",
            "2025-02-10 09:58:39,307 - TestRunner-135660392493584 - INFO - test_duplicate_material_creation: PASSED (0.004s)\n",
            "INFO:TestRunner-135660392493584:test_duplicate_material_creation: PASSED (0.004s)\n",
            "2025-02-10 09:58:39,310 - TestRunner-135660392493584 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392493584:Running tearDown\n",
            "2025-02-10 09:58:39,413 - TestRunner-135660392493584 - INFO - Executing test: test_empty_input_handling\n",
            "INFO:TestRunner-135660392493584:Executing test: test_empty_input_handling\n",
            "2025-02-10 09:58:39,418 - TestRunner-135660392493584 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392493584:Running setUp\n",
            "2025-02-10 09:58:39,420 - TestRunner-135660392493584 - INFO - test_empty_input_handling: PASSED (0.002s)\n",
            "INFO:TestRunner-135660392493584:test_empty_input_handling: PASSED (0.002s)\n",
            "2025-02-10 09:58:39,423 - TestRunner-135660392493584 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392493584:Running tearDown\n",
            "2025-02-10 09:58:39,527 - TestRunner-135660392493584 - INFO - Executing test: test_get_material_master_data\n",
            "INFO:TestRunner-135660392493584:Executing test: test_get_material_master_data\n",
            "2025-02-10 09:58:39,532 - TestRunner-135660392493584 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392493584:Running setUp\n",
            "2025-02-10 09:58:39,535 - TestRunner-135660392493584 - INFO - test_get_material_master_data: PASSED (0.002s)\n",
            "INFO:TestRunner-135660392493584:test_get_material_master_data: PASSED (0.002s)\n",
            "2025-02-10 09:58:39,538 - TestRunner-135660392493584 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392493584:Running tearDown\n",
            "2025-02-10 09:58:39,640 - TestRunner-135660392493584 - INFO - Executing test: test_initial_material_check\n",
            "INFO:TestRunner-135660392493584:Executing test: test_initial_material_check\n",
            "2025-02-10 09:58:39,644 - TestRunner-135660392493584 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392493584:Running setUp\n",
            "2025-02-10 09:58:39,646 - TestRunner-135660392493584 - INFO - test_initial_material_check: PASSED (0.002s)\n",
            "INFO:TestRunner-135660392493584:test_initial_material_check: PASSED (0.002s)\n",
            "2025-02-10 09:58:39,650 - TestRunner-135660392493584 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392493584:Running tearDown\n",
            "2025-02-10 09:58:39,754 - TestRunner-135660392493584 - INFO - Executing test: test_invalid_material_check\n",
            "INFO:TestRunner-135660392493584:Executing test: test_invalid_material_check\n",
            "2025-02-10 09:58:39,757 - TestRunner-135660392493584 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392493584:Running setUp\n",
            "2025-02-10 09:58:39,759 - TestRunner-135660392493584 - INFO - test_invalid_material_check: PASSED (0.002s)\n",
            "INFO:TestRunner-135660392493584:test_invalid_material_check: PASSED (0.002s)\n",
            "2025-02-10 09:58:39,762 - TestRunner-135660392493584 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392493584:Running tearDown\n",
            "2025-02-10 09:58:39,865 - TestRunner-135660392493584 - INFO - Executing test: test_invalid_plant_check\n",
            "INFO:TestRunner-135660392493584:Executing test: test_invalid_plant_check\n",
            "2025-02-10 09:58:39,868 - TestRunner-135660392493584 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392493584:Running setUp\n",
            "2025-02-10 09:58:39,871 - TestRunner-135660392493584 - INFO - test_invalid_plant_check: PASSED (0.003s)\n",
            "INFO:TestRunner-135660392493584:test_invalid_plant_check: PASSED (0.003s)\n",
            "2025-02-10 09:58:39,875 - TestRunner-135660392493584 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392493584:Running tearDown\n",
            "2025-02-10 09:58:39,980 - TestRunner-135660392493584 - INFO - Executing test: test_material_creation_without_id\n",
            "INFO:TestRunner-135660392493584:Executing test: test_material_creation_without_id\n",
            "2025-02-10 09:58:39,983 - TestRunner-135660392493584 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392493584:Running setUp\n",
            "2025-02-10 09:58:39,985 - TestRunner-135660392493584 - INFO - test_material_creation_without_id: PASSED (0.002s)\n",
            "INFO:TestRunner-135660392493584:test_material_creation_without_id: PASSED (0.002s)\n",
            "2025-02-10 09:58:39,989 - TestRunner-135660392493584 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392493584:Running tearDown\n",
            "2025-02-10 09:58:40,093 - TestRunner-135660392493584 - INFO - Executing test: test_missing_required_fields\n",
            "INFO:TestRunner-135660392493584:Executing test: test_missing_required_fields\n",
            "2025-02-10 09:58:40,096 - TestRunner-135660392493584 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392493584:Running setUp\n",
            "2025-02-10 09:58:40,101 - TestRunner-135660392493584 - INFO - test_missing_required_fields: PASSED (0.005s)\n",
            "INFO:TestRunner-135660392493584:test_missing_required_fields: PASSED (0.005s)\n",
            "2025-02-10 09:58:40,105 - TestRunner-135660392493584 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392493584:Running tearDown\n",
            "2025-02-10 09:58:40,209 - TestRunner-135660392493584 - INFO - \n",
            "Test Suite Results:\n",
            "INFO:TestRunner-135660392493584:\n",
            "Test Suite Results:\n",
            "2025-02-10 09:58:40,212 - TestRunner-135660392493584 - INFO - ==================================================\n",
            "INFO:TestRunner-135660392493584:==================================================\n",
            "2025-02-10 09:58:40,215 - TestRunner-135660392493584 - INFO - Total Tests    : 10\n",
            "INFO:TestRunner-135660392493584:Total Tests    : 10\n",
            "2025-02-10 09:58:40,218 - TestRunner-135660392493584 - INFO - Tests Passed   : 10\n",
            "INFO:TestRunner-135660392493584:Tests Passed   : 10\n",
            "2025-02-10 09:58:40,221 - TestRunner-135660392493584 - INFO - Tests Failed   : 0\n",
            "INFO:TestRunner-135660392493584:Tests Failed   : 0\n",
            "2025-02-10 09:58:40,226 - TestRunner-135660392493584 - INFO - Success Rate   : 100.0%\n",
            "INFO:TestRunner-135660392493584:Success Rate   : 100.0%\n",
            "2025-02-10 09:58:40,230 - TestRunner-135660392493584 - INFO - Execution Time : 1.153s\n",
            "INFO:TestRunner-135660392493584:Execution Time : 1.153s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Test Execution Summary\n",
            "==================================================\n",
            "Total Tests   : 10\n",
            "Tests Passed  : 10\n",
            "Tests Failed  : 0\n",
            "\n",
            "Detailed Test Results:\n",
            "--------------------------------------------------\n",
            "\n",
            "Test: test_create_and_verify_material\n",
            "Status: PASSED\n",
            "Time: 0.003s\n",
            "\n",
            "Test: test_default_values\n",
            "Status: PASSED\n",
            "Time: 0.004s\n",
            "\n",
            "Test: test_duplicate_material_creation\n",
            "Status: PASSED\n",
            "Time: 0.004s\n",
            "\n",
            "Test: test_empty_input_handling\n",
            "Status: PASSED\n",
            "Time: 0.002s\n",
            "\n",
            "Test: test_get_material_master_data\n",
            "Status: PASSED\n",
            "Time: 0.002s\n",
            "\n",
            "Test: test_initial_material_check\n",
            "Status: PASSED\n",
            "Time: 0.002s\n",
            "\n",
            "Test: test_invalid_material_check\n",
            "Status: PASSED\n",
            "Time: 0.002s\n",
            "\n",
            "Test: test_invalid_plant_check\n",
            "Status: PASSED\n",
            "Time: 0.003s\n",
            "\n",
            "Test: test_material_creation_without_id\n",
            "Status: PASSED\n",
            "Time: 0.002s\n",
            "\n",
            "Test: test_missing_required_fields\n",
            "Status: PASSED\n",
            "Time: 0.005s\n",
            "\n",
            "Total Execution Time: 1.178s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Script 2 (test runner  calls)  : Material Ops Runner V1\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add test harness to Python path if not already there\n",
        "if default_base_dir not in sys.path:\n",
        "    sys.path.append(default_base_dir)\n",
        "\n",
        "from test_harness.tests.test_runner import run_material_tests\n",
        "from test_harness.tests.test_material_ops import TestMaterialOperations\n",
        "import asyncio\n",
        "import traceback\n",
        "import time\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "async def run_material_tests_v1() -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Run material operations tests with detailed output\n",
        "\n",
        "    Returns:\n",
        "        Optional[Dict[str, Any]]: Test results or None if execution failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Run tests with verbose output and timing\n",
        "        results = await run_material_tests(\n",
        "            TestMaterialOperations,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        # Print results summary\n",
        "        print(\"\\n\" + \"=\"* 50)\n",
        "        print(\"Test Execution Summary\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Total Tests   : {results.total_tests}\")\n",
        "        print(f\"Tests Passed  : {results.passed_tests}\")\n",
        "        print(f\"Tests Failed  : {results.failed_tests}\")\n",
        "        print(f\"Total Time    : {results.execution_time:.3f}s\")\n",
        "\n",
        "        # Print failures if any\n",
        "        if results.failed_tests > 0:\n",
        "            print(\"\\nFailure Details:\")\n",
        "            print(\"-\" * 50)\n",
        "            for result in results.results:\n",
        "                if not result.success:\n",
        "                    print(f\"\\nTest: {result.name}\")\n",
        "                    print(f\"Error: {result.error}\")\n",
        "\n",
        "        return {\n",
        "            'suite_results': results,\n",
        "            'timestamp': time.time()\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Test execution failed: {str(e)}\")\n",
        "        print(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "# Run tests\n",
        "try:\n",
        "    results = await run_material_tests_v1()\n",
        "    if results is None:\n",
        "        print(\"Test execution failed to complete\")\n",
        "except Exception as e:\n",
        "    print(f\"Fatal error in test execution: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIKAQHmNmJ8M",
        "outputId": "76671cf7-de67-4357-de19-856ea10b1f16"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 09:58:52,476 - TestRunner-135660392784784 - INFO - Starting test suite: TestMaterialOperations\n",
            "INFO:TestRunner-135660392784784:Starting test suite: TestMaterialOperations\n",
            "2025-02-10 09:58:52,481 - TestRunner-135660392784784 - INFO - Found 10 test methods\n",
            "INFO:TestRunner-135660392784784:Found 10 test methods\n",
            "2025-02-10 09:58:52,484 - TestRunner-135660392784784 - INFO - Executing test: test_create_and_verify_material\n",
            "INFO:TestRunner-135660392784784:Executing test: test_create_and_verify_material\n",
            "2025-02-10 09:58:52,486 - TestRunner-135660392784784 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392784784:Running setUp\n",
            "2025-02-10 09:58:52,489 - TestRunner-135660392784784 - INFO - test_create_and_verify_material: PASSED (0.002s)\n",
            "INFO:TestRunner-135660392784784:test_create_and_verify_material: PASSED (0.002s)\n",
            "2025-02-10 09:58:52,491 - TestRunner-135660392784784 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392784784:Running tearDown\n",
            "2025-02-10 09:58:52,593 - TestRunner-135660392784784 - INFO - Executing test: test_default_values\n",
            "INFO:TestRunner-135660392784784:Executing test: test_default_values\n",
            "2025-02-10 09:58:52,598 - TestRunner-135660392784784 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392784784:Running setUp\n",
            "2025-02-10 09:58:52,603 - TestRunner-135660392784784 - INFO - test_default_values: PASSED (0.005s)\n",
            "INFO:TestRunner-135660392784784:test_default_values: PASSED (0.005s)\n",
            "2025-02-10 09:58:52,607 - TestRunner-135660392784784 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392784784:Running tearDown\n",
            "2025-02-10 09:58:52,711 - TestRunner-135660392784784 - INFO - Executing test: test_duplicate_material_creation\n",
            "INFO:TestRunner-135660392784784:Executing test: test_duplicate_material_creation\n",
            "2025-02-10 09:58:52,713 - TestRunner-135660392784784 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392784784:Running setUp\n",
            "2025-02-10 09:58:52,715 - TestRunner-135660392784784 - INFO - test_duplicate_material_creation: PASSED (0.002s)\n",
            "INFO:TestRunner-135660392784784:test_duplicate_material_creation: PASSED (0.002s)\n",
            "2025-02-10 09:58:52,717 - TestRunner-135660392784784 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392784784:Running tearDown\n",
            "2025-02-10 09:58:52,820 - TestRunner-135660392784784 - INFO - Executing test: test_empty_input_handling\n",
            "INFO:TestRunner-135660392784784:Executing test: test_empty_input_handling\n",
            "2025-02-10 09:58:52,824 - TestRunner-135660392784784 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392784784:Running setUp\n",
            "2025-02-10 09:58:52,826 - TestRunner-135660392784784 - INFO - test_empty_input_handling: PASSED (0.002s)\n",
            "INFO:TestRunner-135660392784784:test_empty_input_handling: PASSED (0.002s)\n",
            "2025-02-10 09:58:52,829 - TestRunner-135660392784784 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392784784:Running tearDown\n",
            "2025-02-10 09:58:52,932 - TestRunner-135660392784784 - INFO - Executing test: test_get_material_master_data\n",
            "INFO:TestRunner-135660392784784:Executing test: test_get_material_master_data\n",
            "2025-02-10 09:58:52,937 - TestRunner-135660392784784 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392784784:Running setUp\n",
            "2025-02-10 09:58:52,940 - TestRunner-135660392784784 - INFO - test_get_material_master_data: PASSED (0.004s)\n",
            "INFO:TestRunner-135660392784784:test_get_material_master_data: PASSED (0.004s)\n",
            "2025-02-10 09:58:52,943 - TestRunner-135660392784784 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392784784:Running tearDown\n",
            "2025-02-10 09:58:53,047 - TestRunner-135660392784784 - INFO - Executing test: test_initial_material_check\n",
            "INFO:TestRunner-135660392784784:Executing test: test_initial_material_check\n",
            "2025-02-10 09:58:53,050 - TestRunner-135660392784784 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392784784:Running setUp\n",
            "2025-02-10 09:58:53,053 - TestRunner-135660392784784 - INFO - test_initial_material_check: PASSED (0.003s)\n",
            "INFO:TestRunner-135660392784784:test_initial_material_check: PASSED (0.003s)\n",
            "2025-02-10 09:58:53,057 - TestRunner-135660392784784 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392784784:Running tearDown\n",
            "2025-02-10 09:58:53,166 - TestRunner-135660392784784 - INFO - Executing test: test_invalid_material_check\n",
            "INFO:TestRunner-135660392784784:Executing test: test_invalid_material_check\n",
            "2025-02-10 09:58:53,169 - TestRunner-135660392784784 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392784784:Running setUp\n",
            "2025-02-10 09:58:53,172 - TestRunner-135660392784784 - INFO - test_invalid_material_check: PASSED (0.003s)\n",
            "INFO:TestRunner-135660392784784:test_invalid_material_check: PASSED (0.003s)\n",
            "2025-02-10 09:58:53,174 - TestRunner-135660392784784 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392784784:Running tearDown\n",
            "2025-02-10 09:58:53,279 - TestRunner-135660392784784 - INFO - Executing test: test_invalid_plant_check\n",
            "INFO:TestRunner-135660392784784:Executing test: test_invalid_plant_check\n",
            "2025-02-10 09:58:53,283 - TestRunner-135660392784784 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392784784:Running setUp\n",
            "2025-02-10 09:58:53,291 - TestRunner-135660392784784 - INFO - test_invalid_plant_check: PASSED (0.008s)\n",
            "INFO:TestRunner-135660392784784:test_invalid_plant_check: PASSED (0.008s)\n",
            "2025-02-10 09:58:53,295 - TestRunner-135660392784784 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392784784:Running tearDown\n",
            "2025-02-10 09:58:53,399 - TestRunner-135660392784784 - INFO - Executing test: test_material_creation_without_id\n",
            "INFO:TestRunner-135660392784784:Executing test: test_material_creation_without_id\n",
            "2025-02-10 09:58:53,401 - TestRunner-135660392784784 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392784784:Running setUp\n",
            "2025-02-10 09:58:53,406 - TestRunner-135660392784784 - INFO - test_material_creation_without_id: PASSED (0.005s)\n",
            "INFO:TestRunner-135660392784784:test_material_creation_without_id: PASSED (0.005s)\n",
            "2025-02-10 09:58:53,409 - TestRunner-135660392784784 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392784784:Running tearDown\n",
            "2025-02-10 09:58:53,511 - TestRunner-135660392784784 - INFO - Executing test: test_missing_required_fields\n",
            "INFO:TestRunner-135660392784784:Executing test: test_missing_required_fields\n",
            "2025-02-10 09:58:53,514 - TestRunner-135660392784784 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392784784:Running setUp\n",
            "2025-02-10 09:58:53,519 - TestRunner-135660392784784 - INFO - test_missing_required_fields: PASSED (0.005s)\n",
            "INFO:TestRunner-135660392784784:test_missing_required_fields: PASSED (0.005s)\n",
            "2025-02-10 09:58:53,523 - TestRunner-135660392784784 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392784784:Running tearDown\n",
            "2025-02-10 09:58:53,627 - TestRunner-135660392784784 - INFO - \n",
            "Test Suite Results:\n",
            "INFO:TestRunner-135660392784784:\n",
            "Test Suite Results:\n",
            "2025-02-10 09:58:53,630 - TestRunner-135660392784784 - INFO - ==================================================\n",
            "INFO:TestRunner-135660392784784:==================================================\n",
            "2025-02-10 09:58:53,632 - TestRunner-135660392784784 - INFO - Total Tests    : 10\n",
            "INFO:TestRunner-135660392784784:Total Tests    : 10\n",
            "2025-02-10 09:58:53,634 - TestRunner-135660392784784 - INFO - Tests Passed   : 10\n",
            "INFO:TestRunner-135660392784784:Tests Passed   : 10\n",
            "2025-02-10 09:58:53,636 - TestRunner-135660392784784 - INFO - Tests Failed   : 0\n",
            "INFO:TestRunner-135660392784784:Tests Failed   : 0\n",
            "2025-02-10 09:58:53,638 - TestRunner-135660392784784 - INFO - Success Rate   : 100.0%\n",
            "INFO:TestRunner-135660392784784:Success Rate   : 100.0%\n",
            "2025-02-10 09:58:53,641 - TestRunner-135660392784784 - INFO - Execution Time : 1.151s\n",
            "INFO:TestRunner-135660392784784:Execution Time : 1.151s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Test Execution Summary\n",
            "==================================================\n",
            "Total Tests   : 10\n",
            "Tests Passed  : 10\n",
            "Tests Failed  : 0\n",
            "Total Time    : 1.151s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Script 3 (test runner  calls) : Material Ops Runner V2\n",
        "from test_harness.tests.test_runner import run_material_tests\n",
        "from test_harness.tests.test_material_ops import TestMaterialOperations\n",
        "import asyncio\n",
        "import traceback\n",
        "import time\n",
        "import nest_asyncio\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "async def run_material_tests_v2() -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Run material operations tests with simplified output\n",
        "\n",
        "    Returns:\n",
        "        Optional[Dict[str, Any]]: Test results or None if execution failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Apply nest_asyncio to allow nested event loops\n",
        "        nest_asyncio.apply()\n",
        "\n",
        "        print(\"Running material tests...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Run tests with minimal output\n",
        "        results = await run_material_tests(\n",
        "            TestMaterialOperations,\n",
        "            verbose=False,\n",
        "            include_timing=True\n",
        "        )\n",
        "\n",
        "        # Print simplified summary\n",
        "        print(\"\\nTest Summary:\")\n",
        "        print(\"-\" * 20)\n",
        "        print(f\"Passed: {results.passed_tests}\")\n",
        "        print(f\"Failed: {results.failed_tests}\")\n",
        "        print(f\"Total: {results.total_tests}\")\n",
        "\n",
        "        # Print failed tests only\n",
        "        if results.failed_tests > 0:\n",
        "            print(\"\\nFailed Tests:\")\n",
        "            print(\"-\" * 20)\n",
        "            for result in results.results:\n",
        "                if not result.success:\n",
        "                    print(f\"\\n{result.name}:\")\n",
        "                    print(f\"Error: {result.error}\")\n",
        "                    print(f\"Time: {result.execution_time:.3f}s\")\n",
        "\n",
        "        # Print execution time\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"\\nTotal Time: {total_time:.3f}s\")\n",
        "\n",
        "        return {\n",
        "            'suite_results': results,\n",
        "            'total_time': total_time,\n",
        "            'timestamp': time.time()\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\nTest Execution Failed\")\n",
        "        print(\"-\" * 20)\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        print(\"\\nTraceback:\")\n",
        "        print(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "# Run tests\n",
        "try:\n",
        "    results = await run_material_tests_v2()\n",
        "    if results is None:\n",
        "        print(\"Test execution failed to complete\")\n",
        "except Exception as e:\n",
        "    print(f\"Fatal error in test execution: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS13Fkx1nN3o",
        "outputId": "32ff6443-7c50-45d1-db6b-0909c90bc24d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 09:59:00,935 - TestRunner-135660392900944 - INFO - Starting test suite: TestMaterialOperations\n",
            "INFO:TestRunner-135660392900944:Starting test suite: TestMaterialOperations\n",
            "2025-02-10 09:59:00,939 - TestRunner-135660392900944 - INFO - Found 10 test methods\n",
            "INFO:TestRunner-135660392900944:Found 10 test methods\n",
            "2025-02-10 09:59:00,943 - TestRunner-135660392900944 - INFO - Executing test: test_create_and_verify_material\n",
            "INFO:TestRunner-135660392900944:Executing test: test_create_and_verify_material\n",
            "2025-02-10 09:59:00,946 - TestRunner-135660392900944 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392900944:Running setUp\n",
            "2025-02-10 09:59:00,949 - TestRunner-135660392900944 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392900944:Running tearDown\n",
            "2025-02-10 09:59:01,052 - TestRunner-135660392900944 - INFO - Executing test: test_default_values\n",
            "INFO:TestRunner-135660392900944:Executing test: test_default_values\n",
            "2025-02-10 09:59:01,055 - TestRunner-135660392900944 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392900944:Running setUp\n",
            "2025-02-10 09:59:01,057 - TestRunner-135660392900944 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392900944:Running tearDown\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running material tests...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 09:59:01,159 - TestRunner-135660392900944 - INFO - Executing test: test_duplicate_material_creation\n",
            "INFO:TestRunner-135660392900944:Executing test: test_duplicate_material_creation\n",
            "2025-02-10 09:59:01,164 - TestRunner-135660392900944 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392900944:Running setUp\n",
            "2025-02-10 09:59:01,169 - TestRunner-135660392900944 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392900944:Running tearDown\n",
            "2025-02-10 09:59:01,273 - TestRunner-135660392900944 - INFO - Executing test: test_empty_input_handling\n",
            "INFO:TestRunner-135660392900944:Executing test: test_empty_input_handling\n",
            "2025-02-10 09:59:01,276 - TestRunner-135660392900944 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392900944:Running setUp\n",
            "2025-02-10 09:59:01,278 - TestRunner-135660392900944 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392900944:Running tearDown\n",
            "2025-02-10 09:59:01,382 - TestRunner-135660392900944 - INFO - Executing test: test_get_material_master_data\n",
            "INFO:TestRunner-135660392900944:Executing test: test_get_material_master_data\n",
            "2025-02-10 09:59:01,384 - TestRunner-135660392900944 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392900944:Running setUp\n",
            "2025-02-10 09:59:01,391 - TestRunner-135660392900944 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392900944:Running tearDown\n",
            "2025-02-10 09:59:01,494 - TestRunner-135660392900944 - INFO - Executing test: test_initial_material_check\n",
            "INFO:TestRunner-135660392900944:Executing test: test_initial_material_check\n",
            "2025-02-10 09:59:01,498 - TestRunner-135660392900944 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392900944:Running setUp\n",
            "2025-02-10 09:59:01,500 - TestRunner-135660392900944 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392900944:Running tearDown\n",
            "2025-02-10 09:59:01,604 - TestRunner-135660392900944 - INFO - Executing test: test_invalid_material_check\n",
            "INFO:TestRunner-135660392900944:Executing test: test_invalid_material_check\n",
            "2025-02-10 09:59:01,608 - TestRunner-135660392900944 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392900944:Running setUp\n",
            "2025-02-10 09:59:01,610 - TestRunner-135660392900944 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392900944:Running tearDown\n",
            "2025-02-10 09:59:01,713 - TestRunner-135660392900944 - INFO - Executing test: test_invalid_plant_check\n",
            "INFO:TestRunner-135660392900944:Executing test: test_invalid_plant_check\n",
            "2025-02-10 09:59:01,716 - TestRunner-135660392900944 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392900944:Running setUp\n",
            "2025-02-10 09:59:01,719 - TestRunner-135660392900944 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392900944:Running tearDown\n",
            "2025-02-10 09:59:01,822 - TestRunner-135660392900944 - INFO - Executing test: test_material_creation_without_id\n",
            "INFO:TestRunner-135660392900944:Executing test: test_material_creation_without_id\n",
            "2025-02-10 09:59:01,825 - TestRunner-135660392900944 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392900944:Running setUp\n",
            "2025-02-10 09:59:01,828 - TestRunner-135660392900944 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392900944:Running tearDown\n",
            "2025-02-10 09:59:01,932 - TestRunner-135660392900944 - INFO - Executing test: test_missing_required_fields\n",
            "INFO:TestRunner-135660392900944:Executing test: test_missing_required_fields\n",
            "2025-02-10 09:59:01,935 - TestRunner-135660392900944 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392900944:Running setUp\n",
            "2025-02-10 09:59:01,940 - TestRunner-135660392900944 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392900944:Running tearDown\n",
            "2025-02-10 09:59:02,042 - TestRunner-135660392900944 - INFO - \n",
            "Test Suite Results:\n",
            "INFO:TestRunner-135660392900944:\n",
            "Test Suite Results:\n",
            "2025-02-10 09:59:02,047 - TestRunner-135660392900944 - INFO - ==================================================\n",
            "INFO:TestRunner-135660392900944:==================================================\n",
            "2025-02-10 09:59:02,049 - TestRunner-135660392900944 - INFO - Total Tests    : 10\n",
            "INFO:TestRunner-135660392900944:Total Tests    : 10\n",
            "2025-02-10 09:59:02,051 - TestRunner-135660392900944 - INFO - Tests Passed   : 10\n",
            "INFO:TestRunner-135660392900944:Tests Passed   : 10\n",
            "2025-02-10 09:59:02,053 - TestRunner-135660392900944 - INFO - Tests Failed   : 0\n",
            "INFO:TestRunner-135660392900944:Tests Failed   : 0\n",
            "2025-02-10 09:59:02,055 - TestRunner-135660392900944 - INFO - Success Rate   : 100.0%\n",
            "INFO:TestRunner-135660392900944:Success Rate   : 100.0%\n",
            "2025-02-10 09:59:02,058 - TestRunner-135660392900944 - INFO - Execution Time : 1.107s\n",
            "INFO:TestRunner-135660392900944:Execution Time : 1.107s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Summary:\n",
            "--------------------\n",
            "Passed: 10\n",
            "Failed: 0\n",
            "Total: 10\n",
            "\n",
            "Total Time: 1.125s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Script 5 (test runner calls) : State Management Tests\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add test harness to Python path if not already there\n",
        "if default_base_dir not in sys.path:\n",
        "    sys.path.append(default_base_dir)\n",
        "\n",
        "from test_harness.tests.test_runner import run_state_tests, TestSuiteResult\n",
        "from test_harness.mock_sap.p2p_apis import P2PSimulator\n",
        "from test_harness.tests.test_material_ops import TestMaterialOperations\n",
        "import asyncio\n",
        "import traceback\n",
        "import time\n",
        "import json\n",
        "from typing import Optional, Dict, Any\n",
        "from datetime import datetime\n",
        "\n",
        "# Import nest_asyncio for Colab compatibility\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def run_state_management_tests() -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Run state management tests with state tracking and snapshots\n",
        "\n",
        "    Returns:\n",
        "        Optional[Dict[str, Any]]: Test results and state snapshots or None if execution failed\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        print(\"\\nStarting State Management Test Execution...\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Initialize test data structures\n",
        "        state_snapshots = []\n",
        "        execution_metrics = {\n",
        "            'start_time': datetime.now().isoformat(),\n",
        "            'test_count': 0,\n",
        "            'success_count': 0\n",
        "        }\n",
        "\n",
        "        # Run tests with state snapshots enabled\n",
        "        results = await run_state_tests(\n",
        "            TestMaterialOperations,\n",
        "            verbose=True,\n",
        "            state_snapshots=True\n",
        "        )\n",
        "\n",
        "        # Process results\n",
        "        execution_stats = {\n",
        "            'total_checks': results.total_tests,\n",
        "            'passed_checks': results.passed_tests,\n",
        "            'failed_checks': results.failed_tests,\n",
        "            'execution_time': results.execution_time,\n",
        "            'success_rate': results.success_rate\n",
        "        }\n",
        "\n",
        "        # Print execution summary\n",
        "        print(\"\\nState Management Test Summary\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Total Checks    : {execution_stats['total_checks']}\")\n",
        "        print(f\"Passed          : {execution_stats['passed_checks']}\")\n",
        "        print(f\"Failed          : {execution_stats['failed_checks']}\")\n",
        "        print(f\"Success Rate    : {execution_stats['success_rate']:.1f}%\")\n",
        "        print(f\"Execution Time  : {execution_stats['execution_time']:.3f}s\")\n",
        "\n",
        "        # Process and print state snapshots\n",
        "        if results.state_snapshots:\n",
        "            print(\"\\nState Snapshots:\")\n",
        "            print(\"=\" * 50)\n",
        "            for idx, snapshot in enumerate(results.state_snapshots, 1):\n",
        "                print(f\"\\nSnapshot {idx} - {snapshot['test']}:\")\n",
        "                try:\n",
        "                    # Format state data\n",
        "                    formatted_state = json.dumps(\n",
        "                        snapshot['state'],\n",
        "                        indent=2,\n",
        "                        default=str  # Handle non-serializable objects\n",
        "                    )\n",
        "                    print(formatted_state)\n",
        "                    state_snapshots.append({\n",
        "                        'id': idx,\n",
        "                        'test': snapshot['test'],\n",
        "                        'timestamp': snapshot.get('timestamp', datetime.now().isoformat()),\n",
        "                        'state': snapshot['state']\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    print(f\"Error formatting state: {str(e)}\")\n",
        "                    print(\"Raw state:\", snapshot['state'])\n",
        "\n",
        "        # Print performance metrics if timing included\n",
        "        if results.execution_time > 0:\n",
        "            print(\"\\nPerformance Metrics:\")\n",
        "            print(\"=\" * 50)\n",
        "            print(f\"Average test time : {results.execution_time/results.total_tests:.3f}s\")\n",
        "            print(f\"Total suite time  : {results.execution_time:.3f}s\")\n",
        "\n",
        "        # Return comprehensive results\n",
        "        return {\n",
        "            'suite_results': results,\n",
        "            'execution_stats': execution_stats,\n",
        "            'state_snapshots': state_snapshots,\n",
        "            'total_time': time.time() - start_time,\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'performance': {\n",
        "                'avg_test_time': results.execution_time/results.total_tests,\n",
        "                'total_time': results.execution_time\n",
        "            }\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\nTest execution failed:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        print(\"\\nTraceback:\")\n",
        "        print(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "# Direct execution in Colab\n",
        "try:\n",
        "    # Execute tests\n",
        "    results = await run_state_management_tests()\n",
        "\n",
        "    if results is None:\n",
        "        print(\"\\nTest execution failed to complete.\")\n",
        "        print(\"Please check the error messages above.\")\n",
        "    else:\n",
        "        print(\"\\nTest execution completed successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nFatal error in test execution: {str(e)}\")\n",
        "    print(\"\\nTraceback:\")\n",
        "    print(traceback.format_exc())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnAoT-44sGnh",
        "outputId": "80c3e9f1-3dd2-46a1-a145-ef646b137148"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 09:59:08,696 - TestRunner-135660392310096 - INFO - Starting test suite: TestMaterialOperations\n",
            "INFO:TestRunner-135660392310096:Starting test suite: TestMaterialOperations\n",
            "2025-02-10 09:59:08,703 - TestRunner-135660392310096 - INFO - Found 10 test methods\n",
            "INFO:TestRunner-135660392310096:Found 10 test methods\n",
            "2025-02-10 09:59:08,710 - TestRunner-135660392310096 - INFO - Executing test: test_create_and_verify_material\n",
            "INFO:TestRunner-135660392310096:Executing test: test_create_and_verify_material\n",
            "2025-02-10 09:59:08,712 - TestRunner-135660392310096 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392310096:Running setUp\n",
            "2025-02-10 09:59:08,717 - TestRunner-135660392310096 - INFO - test_create_and_verify_material: PASSED (0.005s)\n",
            "INFO:TestRunner-135660392310096:test_create_and_verify_material: PASSED (0.005s)\n",
            "2025-02-10 09:59:08,722 - TestRunner-135660392310096 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392310096:Running tearDown\n",
            "2025-02-10 09:59:08,825 - TestRunner-135660392310096 - INFO - Executing test: test_default_values\n",
            "INFO:TestRunner-135660392310096:Executing test: test_default_values\n",
            "2025-02-10 09:59:08,828 - TestRunner-135660392310096 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392310096:Running setUp\n",
            "2025-02-10 09:59:08,830 - TestRunner-135660392310096 - INFO - test_default_values: PASSED (0.002s)\n",
            "INFO:TestRunner-135660392310096:test_default_values: PASSED (0.002s)\n",
            "2025-02-10 09:59:08,832 - TestRunner-135660392310096 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392310096:Running tearDown\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting State Management Test Execution...\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 09:59:08,935 - TestRunner-135660392310096 - INFO - Executing test: test_duplicate_material_creation\n",
            "INFO:TestRunner-135660392310096:Executing test: test_duplicate_material_creation\n",
            "2025-02-10 09:59:08,937 - TestRunner-135660392310096 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392310096:Running setUp\n",
            "2025-02-10 09:59:08,939 - TestRunner-135660392310096 - INFO - test_duplicate_material_creation: PASSED (0.002s)\n",
            "INFO:TestRunner-135660392310096:test_duplicate_material_creation: PASSED (0.002s)\n",
            "2025-02-10 09:59:08,941 - TestRunner-135660392310096 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392310096:Running tearDown\n",
            "2025-02-10 09:59:09,046 - TestRunner-135660392310096 - INFO - Executing test: test_empty_input_handling\n",
            "INFO:TestRunner-135660392310096:Executing test: test_empty_input_handling\n",
            "2025-02-10 09:59:09,048 - TestRunner-135660392310096 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392310096:Running setUp\n",
            "2025-02-10 09:59:09,050 - TestRunner-135660392310096 - INFO - test_empty_input_handling: PASSED (0.002s)\n",
            "INFO:TestRunner-135660392310096:test_empty_input_handling: PASSED (0.002s)\n",
            "2025-02-10 09:59:09,052 - TestRunner-135660392310096 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392310096:Running tearDown\n",
            "2025-02-10 09:59:09,157 - TestRunner-135660392310096 - INFO - Executing test: test_get_material_master_data\n",
            "INFO:TestRunner-135660392310096:Executing test: test_get_material_master_data\n",
            "2025-02-10 09:59:09,161 - TestRunner-135660392310096 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392310096:Running setUp\n",
            "2025-02-10 09:59:09,164 - TestRunner-135660392310096 - INFO - test_get_material_master_data: PASSED (0.003s)\n",
            "INFO:TestRunner-135660392310096:test_get_material_master_data: PASSED (0.003s)\n",
            "2025-02-10 09:59:09,167 - TestRunner-135660392310096 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392310096:Running tearDown\n",
            "2025-02-10 09:59:09,272 - TestRunner-135660392310096 - INFO - Executing test: test_initial_material_check\n",
            "INFO:TestRunner-135660392310096:Executing test: test_initial_material_check\n",
            "2025-02-10 09:59:09,274 - TestRunner-135660392310096 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392310096:Running setUp\n",
            "2025-02-10 09:59:09,277 - TestRunner-135660392310096 - INFO - test_initial_material_check: PASSED (0.003s)\n",
            "INFO:TestRunner-135660392310096:test_initial_material_check: PASSED (0.003s)\n",
            "2025-02-10 09:59:09,279 - TestRunner-135660392310096 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392310096:Running tearDown\n",
            "2025-02-10 09:59:09,384 - TestRunner-135660392310096 - INFO - Executing test: test_invalid_material_check\n",
            "INFO:TestRunner-135660392310096:Executing test: test_invalid_material_check\n",
            "2025-02-10 09:59:09,386 - TestRunner-135660392310096 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392310096:Running setUp\n",
            "2025-02-10 09:59:09,389 - TestRunner-135660392310096 - INFO - test_invalid_material_check: PASSED (0.003s)\n",
            "INFO:TestRunner-135660392310096:test_invalid_material_check: PASSED (0.003s)\n",
            "2025-02-10 09:59:09,391 - TestRunner-135660392310096 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392310096:Running tearDown\n",
            "2025-02-10 09:59:09,495 - TestRunner-135660392310096 - INFO - Executing test: test_invalid_plant_check\n",
            "INFO:TestRunner-135660392310096:Executing test: test_invalid_plant_check\n",
            "2025-02-10 09:59:09,497 - TestRunner-135660392310096 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392310096:Running setUp\n",
            "2025-02-10 09:59:09,499 - TestRunner-135660392310096 - INFO - test_invalid_plant_check: PASSED (0.002s)\n",
            "INFO:TestRunner-135660392310096:test_invalid_plant_check: PASSED (0.002s)\n",
            "2025-02-10 09:59:09,502 - TestRunner-135660392310096 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392310096:Running tearDown\n",
            "2025-02-10 09:59:09,604 - TestRunner-135660392310096 - INFO - Executing test: test_material_creation_without_id\n",
            "INFO:TestRunner-135660392310096:Executing test: test_material_creation_without_id\n",
            "2025-02-10 09:59:09,608 - TestRunner-135660392310096 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392310096:Running setUp\n",
            "2025-02-10 09:59:09,610 - TestRunner-135660392310096 - INFO - test_material_creation_without_id: PASSED (0.003s)\n",
            "INFO:TestRunner-135660392310096:test_material_creation_without_id: PASSED (0.003s)\n",
            "2025-02-10 09:59:09,614 - TestRunner-135660392310096 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392310096:Running tearDown\n",
            "2025-02-10 09:59:09,718 - TestRunner-135660392310096 - INFO - Executing test: test_missing_required_fields\n",
            "INFO:TestRunner-135660392310096:Executing test: test_missing_required_fields\n",
            "2025-02-10 09:59:09,720 - TestRunner-135660392310096 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392310096:Running setUp\n",
            "2025-02-10 09:59:09,723 - TestRunner-135660392310096 - INFO - test_missing_required_fields: PASSED (0.003s)\n",
            "INFO:TestRunner-135660392310096:test_missing_required_fields: PASSED (0.003s)\n",
            "2025-02-10 09:59:09,727 - TestRunner-135660392310096 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392310096:Running tearDown\n",
            "2025-02-10 09:59:09,831 - TestRunner-135660392310096 - INFO - \n",
            "Test Suite Results:\n",
            "INFO:TestRunner-135660392310096:\n",
            "Test Suite Results:\n",
            "2025-02-10 09:59:09,834 - TestRunner-135660392310096 - INFO - ==================================================\n",
            "INFO:TestRunner-135660392310096:==================================================\n",
            "2025-02-10 09:59:09,836 - TestRunner-135660392310096 - INFO - Total Tests    : 10\n",
            "INFO:TestRunner-135660392310096:Total Tests    : 10\n",
            "2025-02-10 09:59:09,837 - TestRunner-135660392310096 - INFO - Tests Passed   : 10\n",
            "INFO:TestRunner-135660392310096:Tests Passed   : 10\n",
            "2025-02-10 09:59:09,842 - TestRunner-135660392310096 - INFO - Tests Failed   : 0\n",
            "INFO:TestRunner-135660392310096:Tests Failed   : 0\n",
            "2025-02-10 09:59:09,844 - TestRunner-135660392310096 - INFO - Success Rate   : 100.0%\n",
            "INFO:TestRunner-135660392310096:Success Rate   : 100.0%\n",
            "2025-02-10 09:59:09,847 - TestRunner-135660392310096 - INFO - Execution Time : 1.135s\n",
            "INFO:TestRunner-135660392310096:Execution Time : 1.135s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "State Management Test Summary\n",
            "==================================================\n",
            "Total Checks    : 10\n",
            "Passed          : 10\n",
            "Failed          : 0\n",
            "Success Rate    : 100.0%\n",
            "Execution Time  : 1.135s\n",
            "\n",
            "Performance Metrics:\n",
            "==================================================\n",
            "Average test time : 0.114s\n",
            "Total suite time  : 1.135s\n",
            "\n",
            "Test execution completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Script 6 (test runner calls) : State Management Runner\n",
        "\n",
        "from test_harness.tests.test_runner import run_state_tests, TestType, create_runner\n",
        "import asyncio\n",
        "import traceback\n",
        "import time\n",
        "import json\n",
        "from typing import Optional, Dict, Any, List\n",
        "from datetime import datetime\n",
        "\n",
        "async def run_detailed_state_tests() -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Run detailed state management tests with complete state tracking\n",
        "\n",
        "    Returns:\n",
        "        Optional[Dict[str, Any]]: Detailed test results and state analysis or None if execution failed\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    state_history: List[Dict[str, Any]] = []\n",
        "\n",
        "    try:\n",
        "        print(\"Starting Detailed State Test Execution...\\n\")\n",
        "\n",
        "        # Create custom runner for detailed state tracking\n",
        "        runner = create_runner(\n",
        "            TestType.STATE,\n",
        "            verbose=True,\n",
        "            save_state=True,\n",
        "            state_snapshots=True,\n",
        "            include_timing=True\n",
        "        )\n",
        "\n",
        "        # Run test suite\n",
        "        results = await runner.run_test_suite(TestMaterialOperations)\n",
        "\n",
        "        # Track state changes\n",
        "        if results.state_snapshots:\n",
        "            for snapshot in results.state_snapshots:\n",
        "                state_history.append({\n",
        "                    'timestamp': datetime.now().isoformat(),\n",
        "                    'test': snapshot['test'],\n",
        "                    'state': snapshot['state']\n",
        "                })\n",
        "\n",
        "        # Print detailed summary\n",
        "        print(\"\\nDetailed State Test Summary\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Total Tests    : {results.total_tests}\")\n",
        "        print(f\"Tests Passed   : {results.passed_tests}\")\n",
        "        print(f\"Tests Failed   : {results.failed_tests}\")\n",
        "        print(f\"Success Rate   : {(results.passed_tests/results.total_tests)*100:.1f}%\")\n",
        "\n",
        "        # Print test results\n",
        "        print(\"\\nTest Results:\")\n",
        "        print(\"-\" * 50)\n",
        "        for result in results.results:\n",
        "            status = \"PASSED\" if result.success else \"FAILED\"\n",
        "            print(f\"\\nTest: {result.name}\")\n",
        "            print(f\"Status: {status}\")\n",
        "            print(f\"Time: {result.execution_time:.3f}s\")\n",
        "            if not result.success and result.error:\n",
        "                print(f\"Error: {result.error}\")\n",
        "\n",
        "        # Analyze state transitions\n",
        "        if state_history:\n",
        "            print(\"\\nState Transition Analysis:\")\n",
        "            print(\"-\" * 50)\n",
        "            for i, state in enumerate(state_history):\n",
        "                print(f\"\\nTransition {i+1}:\")\n",
        "                print(f\"Test: {state['test']}\")\n",
        "                print(f\"Timestamp: {state['timestamp']}\")\n",
        "                print(\"State:\", json.dumps(state['state'], indent=2))\n",
        "\n",
        "        # Return comprehensive results\n",
        "        return {\n",
        "            'suite_results': results,\n",
        "            'state_history': state_history,\n",
        "            'execution_time': time.time() - start_time,\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'test_details': [{\n",
        "                'name': result.name,\n",
        "                'success': result.success,\n",
        "                'execution_time': result.execution_time,\n",
        "                'error': result.error\n",
        "            } for result in results.results]\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\nDetailed state test execution failed:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        print(\"\\nTraceback:\")\n",
        "        print(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "# Run detailed state tests with proper error handling\n",
        "try:\n",
        "    results = await run_detailed_state_tests()\n",
        "    if results is None:\n",
        "        print(\"Detailed state test execution failed to complete\")\n",
        "except Exception as e:\n",
        "    print(f\"Fatal error in test execution: {str(e)}\")\n",
        "    print(traceback.format_exc())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmJbjbZAtRS_",
        "outputId": "69488552-8c62-474a-edb2-6acabeada35d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 09:59:21,528 - TestRunner-135660392492048 - INFO - Starting test suite: TestMaterialOperations\n",
            "INFO:TestRunner-135660392492048:Starting test suite: TestMaterialOperations\n",
            "2025-02-10 09:59:21,533 - TestRunner-135660392492048 - INFO - Found 10 test methods\n",
            "INFO:TestRunner-135660392492048:Found 10 test methods\n",
            "2025-02-10 09:59:21,537 - TestRunner-135660392492048 - INFO - Executing test: test_create_and_verify_material\n",
            "INFO:TestRunner-135660392492048:Executing test: test_create_and_verify_material\n",
            "2025-02-10 09:59:21,542 - TestRunner-135660392492048 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392492048:Running setUp\n",
            "2025-02-10 09:59:21,553 - TestRunner-135660392492048 - INFO - test_create_and_verify_material: PASSED (0.010s)\n",
            "INFO:TestRunner-135660392492048:test_create_and_verify_material: PASSED (0.010s)\n",
            "2025-02-10 09:59:21,559 - TestRunner-135660392492048 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392492048:Running tearDown\n",
            "2025-02-10 09:59:21,665 - TestRunner-135660392492048 - INFO - Executing test: test_default_values\n",
            "INFO:TestRunner-135660392492048:Executing test: test_default_values\n",
            "2025-02-10 09:59:21,669 - TestRunner-135660392492048 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392492048:Running setUp\n",
            "2025-02-10 09:59:21,672 - TestRunner-135660392492048 - INFO - test_default_values: PASSED (0.003s)\n",
            "INFO:TestRunner-135660392492048:test_default_values: PASSED (0.003s)\n",
            "2025-02-10 09:59:21,676 - TestRunner-135660392492048 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392492048:Running tearDown\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Detailed State Test Execution...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 09:59:21,781 - TestRunner-135660392492048 - INFO - Executing test: test_duplicate_material_creation\n",
            "INFO:TestRunner-135660392492048:Executing test: test_duplicate_material_creation\n",
            "2025-02-10 09:59:21,784 - TestRunner-135660392492048 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392492048:Running setUp\n",
            "2025-02-10 09:59:21,787 - TestRunner-135660392492048 - INFO - test_duplicate_material_creation: PASSED (0.003s)\n",
            "INFO:TestRunner-135660392492048:test_duplicate_material_creation: PASSED (0.003s)\n",
            "2025-02-10 09:59:21,791 - TestRunner-135660392492048 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392492048:Running tearDown\n",
            "2025-02-10 09:59:21,896 - TestRunner-135660392492048 - INFO - Executing test: test_empty_input_handling\n",
            "INFO:TestRunner-135660392492048:Executing test: test_empty_input_handling\n",
            "2025-02-10 09:59:21,899 - TestRunner-135660392492048 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392492048:Running setUp\n",
            "2025-02-10 09:59:21,902 - TestRunner-135660392492048 - INFO - test_empty_input_handling: PASSED (0.002s)\n",
            "INFO:TestRunner-135660392492048:test_empty_input_handling: PASSED (0.002s)\n",
            "2025-02-10 09:59:21,908 - TestRunner-135660392492048 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392492048:Running tearDown\n",
            "2025-02-10 09:59:22,010 - TestRunner-135660392492048 - INFO - Executing test: test_get_material_master_data\n",
            "INFO:TestRunner-135660392492048:Executing test: test_get_material_master_data\n",
            "2025-02-10 09:59:22,013 - TestRunner-135660392492048 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392492048:Running setUp\n",
            "2025-02-10 09:59:22,016 - TestRunner-135660392492048 - INFO - test_get_material_master_data: PASSED (0.002s)\n",
            "INFO:TestRunner-135660392492048:test_get_material_master_data: PASSED (0.002s)\n",
            "2025-02-10 09:59:22,019 - TestRunner-135660392492048 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392492048:Running tearDown\n",
            "2025-02-10 09:59:22,121 - TestRunner-135660392492048 - INFO - Executing test: test_initial_material_check\n",
            "INFO:TestRunner-135660392492048:Executing test: test_initial_material_check\n",
            "2025-02-10 09:59:22,124 - TestRunner-135660392492048 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392492048:Running setUp\n",
            "2025-02-10 09:59:22,126 - TestRunner-135660392492048 - INFO - test_initial_material_check: PASSED (0.002s)\n",
            "INFO:TestRunner-135660392492048:test_initial_material_check: PASSED (0.002s)\n",
            "2025-02-10 09:59:22,128 - TestRunner-135660392492048 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392492048:Running tearDown\n",
            "2025-02-10 09:59:22,235 - TestRunner-135660392492048 - INFO - Executing test: test_invalid_material_check\n",
            "INFO:TestRunner-135660392492048:Executing test: test_invalid_material_check\n",
            "2025-02-10 09:59:22,238 - TestRunner-135660392492048 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392492048:Running setUp\n",
            "2025-02-10 09:59:22,240 - TestRunner-135660392492048 - INFO - test_invalid_material_check: PASSED (0.002s)\n",
            "INFO:TestRunner-135660392492048:test_invalid_material_check: PASSED (0.002s)\n",
            "2025-02-10 09:59:22,242 - TestRunner-135660392492048 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392492048:Running tearDown\n",
            "2025-02-10 09:59:22,347 - TestRunner-135660392492048 - INFO - Executing test: test_invalid_plant_check\n",
            "INFO:TestRunner-135660392492048:Executing test: test_invalid_plant_check\n",
            "2025-02-10 09:59:22,349 - TestRunner-135660392492048 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392492048:Running setUp\n",
            "2025-02-10 09:59:22,351 - TestRunner-135660392492048 - INFO - test_invalid_plant_check: PASSED (0.002s)\n",
            "INFO:TestRunner-135660392492048:test_invalid_plant_check: PASSED (0.002s)\n",
            "2025-02-10 09:59:22,353 - TestRunner-135660392492048 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392492048:Running tearDown\n",
            "2025-02-10 09:59:22,455 - TestRunner-135660392492048 - INFO - Executing test: test_material_creation_without_id\n",
            "INFO:TestRunner-135660392492048:Executing test: test_material_creation_without_id\n",
            "2025-02-10 09:59:22,458 - TestRunner-135660392492048 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392492048:Running setUp\n",
            "2025-02-10 09:59:22,460 - TestRunner-135660392492048 - INFO - test_material_creation_without_id: PASSED (0.002s)\n",
            "INFO:TestRunner-135660392492048:test_material_creation_without_id: PASSED (0.002s)\n",
            "2025-02-10 09:59:22,462 - TestRunner-135660392492048 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392492048:Running tearDown\n",
            "2025-02-10 09:59:22,565 - TestRunner-135660392492048 - INFO - Executing test: test_missing_required_fields\n",
            "INFO:TestRunner-135660392492048:Executing test: test_missing_required_fields\n",
            "2025-02-10 09:59:22,567 - TestRunner-135660392492048 - INFO - Running setUp\n",
            "INFO:TestRunner-135660392492048:Running setUp\n",
            "2025-02-10 09:59:22,570 - TestRunner-135660392492048 - INFO - test_missing_required_fields: PASSED (0.002s)\n",
            "INFO:TestRunner-135660392492048:test_missing_required_fields: PASSED (0.002s)\n",
            "2025-02-10 09:59:22,572 - TestRunner-135660392492048 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660392492048:Running tearDown\n",
            "2025-02-10 09:59:22,677 - TestRunner-135660392492048 - INFO - \n",
            "Test Suite Results:\n",
            "INFO:TestRunner-135660392492048:\n",
            "Test Suite Results:\n",
            "2025-02-10 09:59:22,680 - TestRunner-135660392492048 - INFO - ==================================================\n",
            "INFO:TestRunner-135660392492048:==================================================\n",
            "2025-02-10 09:59:22,682 - TestRunner-135660392492048 - INFO - Total Tests    : 10\n",
            "INFO:TestRunner-135660392492048:Total Tests    : 10\n",
            "2025-02-10 09:59:22,685 - TestRunner-135660392492048 - INFO - Tests Passed   : 10\n",
            "INFO:TestRunner-135660392492048:Tests Passed   : 10\n",
            "2025-02-10 09:59:22,689 - TestRunner-135660392492048 - INFO - Tests Failed   : 0\n",
            "INFO:TestRunner-135660392492048:Tests Failed   : 0\n",
            "2025-02-10 09:59:22,693 - TestRunner-135660392492048 - INFO - Success Rate   : 100.0%\n",
            "INFO:TestRunner-135660392492048:Success Rate   : 100.0%\n",
            "2025-02-10 09:59:22,697 - TestRunner-135660392492048 - INFO - Execution Time : 1.148s\n",
            "INFO:TestRunner-135660392492048:Execution Time : 1.148s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Detailed State Test Summary\n",
            "==================================================\n",
            "Total Tests    : 10\n",
            "Tests Passed   : 10\n",
            "Tests Failed   : 0\n",
            "Success Rate   : 100.0%\n",
            "\n",
            "Test Results:\n",
            "--------------------------------------------------\n",
            "\n",
            "Test: test_create_and_verify_material\n",
            "Status: PASSED\n",
            "Time: 0.010s\n",
            "\n",
            "Test: test_default_values\n",
            "Status: PASSED\n",
            "Time: 0.003s\n",
            "\n",
            "Test: test_duplicate_material_creation\n",
            "Status: PASSED\n",
            "Time: 0.003s\n",
            "\n",
            "Test: test_empty_input_handling\n",
            "Status: PASSED\n",
            "Time: 0.002s\n",
            "\n",
            "Test: test_get_material_master_data\n",
            "Status: PASSED\n",
            "Time: 0.002s\n",
            "\n",
            "Test: test_initial_material_check\n",
            "Status: PASSED\n",
            "Time: 0.002s\n",
            "\n",
            "Test: test_invalid_material_check\n",
            "Status: PASSED\n",
            "Time: 0.002s\n",
            "\n",
            "Test: test_invalid_plant_check\n",
            "Status: PASSED\n",
            "Time: 0.002s\n",
            "\n",
            "Test: test_material_creation_without_id\n",
            "Status: PASSED\n",
            "Time: 0.002s\n",
            "\n",
            "Test: test_missing_required_fields\n",
            "Status: PASSED\n",
            "Time: 0.002s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Script 7 (test runner calls) : P2P Tests Runner\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add test harness to Python path if not already there\n",
        "if default_base_dir not in sys.path:\n",
        "    sys.path.append(default_base_dir)\n",
        "\n",
        "from test_harness.tests.test_runner import run_p2p_tests, TestSuiteResult\n",
        "from test_harness.tests.test_p2p_flow import TestP2PFlow\n",
        "import asyncio\n",
        "import traceback\n",
        "import time\n",
        "import json\n",
        "from typing import Optional, Dict, Any\n",
        "from datetime import datetime\n",
        "\n",
        "# Import nest_asyncio for Colab compatibility\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def run_p2p_flow_tests() -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Run P2P flow tests with transaction tracking\n",
        "\n",
        "    Returns:\n",
        "        Optional[Dict[str, Any]]: Test results and transaction info or None if execution failed\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        print(\"\\nStarting P2P Flow Test Execution...\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Run tests with state tracking enabled\n",
        "        results = await run_p2p_tests(\n",
        "            TestP2PFlow,\n",
        "            verbose=True,\n",
        "            save_state=True,\n",
        "            state_snapshots=True\n",
        "        )\n",
        "\n",
        "        # Process results\n",
        "        execution_stats = {\n",
        "            'total_tests': results.total_tests,\n",
        "            'passed_tests': results.passed_tests,\n",
        "            'failed_tests': results.failed_tests,\n",
        "            'execution_time': results.execution_time,\n",
        "            'success_rate': results.success_rate\n",
        "        }\n",
        "\n",
        "        # Print execution summary\n",
        "        print(\"\\nP2P Flow Test Summary\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Total Tests     : {execution_stats['total_tests']}\")\n",
        "        print(f\"Passed          : {execution_stats['passed_tests']}\")\n",
        "        print(f\"Failed          : {execution_stats['failed_tests']}\")\n",
        "        print(f\"Success Rate    : {execution_stats['success_rate']:.1f}%\")\n",
        "        print(f\"Execution Time  : {execution_stats['execution_time']:.3f}s\")\n",
        "\n",
        "        # Process failures if any\n",
        "        if results.failed_tests > 0:\n",
        "            print(\"\\nTest Failures:\")\n",
        "            print(\"-\" * 50)\n",
        "            for result in results.results:\n",
        "                if not result.success:\n",
        "                    print(f\"\\nTest: {result.name}\")\n",
        "                    print(f\"Error: {result.error}\")\n",
        "                    if result.state_snapshot:\n",
        "                        print(\"\\nState at failure:\")\n",
        "                        try:\n",
        "                            print(json.dumps(result.state_snapshot, indent=2, default=str))\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error formatting state: {str(e)}\")\n",
        "\n",
        "        # Print state snapshots if available\n",
        "        if results.state_snapshots:\n",
        "            print(\"\\nTransaction Flow:\")\n",
        "            print(\"-\" * 50)\n",
        "            for idx, snapshot in enumerate(results.state_snapshots, 1):\n",
        "                print(f\"\\nTransaction {idx} - {snapshot['test']}:\")\n",
        "                try:\n",
        "                    state_summary = {\n",
        "                        'prs': len(snapshot['state'].get('purchase_requisitions', {})),\n",
        "                        'pos': len(snapshot['state'].get('purchase_orders', {})),\n",
        "                        'timestamp': snapshot.get('timestamp', 'N/A')\n",
        "                    }\n",
        "                    print(json.dumps(state_summary, indent=2))\n",
        "                except Exception as e:\n",
        "                    print(f\"Error summarizing state: {str(e)}\")\n",
        "\n",
        "        # Return comprehensive results\n",
        "        return {\n",
        "            'suite_results': results,\n",
        "            'execution_stats': execution_stats,\n",
        "            'total_time': time.time() - start_time,\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'state_snapshots': results.state_snapshots,\n",
        "            'performance': {\n",
        "                'avg_test_time': results.execution_time/results.total_tests,\n",
        "                'total_time': results.execution_time\n",
        "            }\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\nTest execution failed:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        print(\"\\nTraceback:\")\n",
        "        print(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "# Direct execution in Colab\n",
        "try:\n",
        "    # Execute tests\n",
        "    results = await run_p2p_flow_tests()\n",
        "\n",
        "    if results is None:\n",
        "        print(\"\\nTest execution failed to complete.\")\n",
        "        print(\"Please check the error messages above.\")\n",
        "    else:\n",
        "        print(\"\\nTest execution completed successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nFatal error in test execution: {str(e)}\")\n",
        "    print(\"\\nTraceback:\")\n",
        "    print(traceback.format_exc())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSGnIr53xWbi",
        "outputId": "294bdc2c-855b-4585-98bf-3584c17fc305"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 09:59:34,131 - TestRunner-135660374683472 - INFO - Starting test suite: TestP2PFlow\n",
            "INFO:TestRunner-135660374683472:Starting test suite: TestP2PFlow\n",
            "2025-02-10 09:59:34,138 - TestRunner-135660374683472 - INFO - Found 14 test methods\n",
            "INFO:TestRunner-135660374683472:Found 14 test methods\n",
            "2025-02-10 09:59:34,141 - TestRunner-135660374683472 - INFO - Executing test: test_document_status_check\n",
            "INFO:TestRunner-135660374683472:Executing test: test_document_status_check\n",
            "2025-02-10 09:59:34,144 - TestRunner-135660374683472 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374683472:Running setUp\n",
            "2025-02-10 09:59:34,147 - TestRunner-135660374683472 - INFO - test_document_status_check: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374683472:test_document_status_check: PASSED (0.003s)\n",
            "2025-02-10 09:59:34,151 - TestRunner-135660374683472 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374683472:Running tearDown\n",
            "2025-02-10 09:59:34,255 - TestRunner-135660374683472 - INFO - Executing test: test_error_invalid_material\n",
            "INFO:TestRunner-135660374683472:Executing test: test_error_invalid_material\n",
            "2025-02-10 09:59:34,257 - TestRunner-135660374683472 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374683472:Running setUp\n",
            "2025-02-10 09:59:34,261 - TestRunner-135660374683472 - INFO - test_error_invalid_material: PASSED (0.004s)\n",
            "INFO:TestRunner-135660374683472:test_error_invalid_material: PASSED (0.004s)\n",
            "2025-02-10 09:59:34,265 - TestRunner-135660374683472 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374683472:Running tearDown\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting P2P Flow Test Execution...\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 09:59:34,368 - TestRunner-135660374683472 - INFO - Executing test: test_error_invalid_plant\n",
            "INFO:TestRunner-135660374683472:Executing test: test_error_invalid_plant\n",
            "2025-02-10 09:59:34,371 - TestRunner-135660374683472 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374683472:Running setUp\n",
            "2025-02-10 09:59:34,374 - TestRunner-135660374683472 - INFO - test_error_invalid_plant: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374683472:test_error_invalid_plant: PASSED (0.003s)\n",
            "2025-02-10 09:59:34,376 - TestRunner-135660374683472 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374683472:Running tearDown\n",
            "2025-02-10 09:59:34,480 - TestRunner-135660374683472 - INFO - Executing test: test_error_invalid_pr_reference\n",
            "INFO:TestRunner-135660374683472:Executing test: test_error_invalid_pr_reference\n",
            "2025-02-10 09:59:34,486 - TestRunner-135660374683472 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374683472:Running setUp\n",
            "2025-02-10 09:59:34,488 - TestRunner-135660374683472 - INFO - test_error_invalid_pr_reference: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374683472:test_error_invalid_pr_reference: PASSED (0.003s)\n",
            "2025-02-10 09:59:34,491 - TestRunner-135660374683472 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374683472:Running tearDown\n",
            "2025-02-10 09:59:34,597 - TestRunner-135660374683472 - INFO - Executing test: test_error_invalid_vendor\n",
            "INFO:TestRunner-135660374683472:Executing test: test_error_invalid_vendor\n",
            "2025-02-10 09:59:34,600 - TestRunner-135660374683472 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374683472:Running setUp\n",
            "2025-02-10 09:59:34,603 - TestRunner-135660374683472 - INFO - test_error_invalid_vendor: PASSED (0.002s)\n",
            "INFO:TestRunner-135660374683472:test_error_invalid_vendor: PASSED (0.002s)\n",
            "2025-02-10 09:59:34,605 - TestRunner-135660374683472 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374683472:Running tearDown\n",
            "2025-02-10 09:59:34,711 - TestRunner-135660374683472 - INFO - Executing test: test_error_pr_already_ordered\n",
            "INFO:TestRunner-135660374683472:Executing test: test_error_pr_already_ordered\n",
            "2025-02-10 09:59:34,714 - TestRunner-135660374683472 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374683472:Running setUp\n",
            "2025-02-10 09:59:34,716 - TestRunner-135660374683472 - INFO - test_error_pr_already_ordered: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374683472:test_error_pr_already_ordered: PASSED (0.003s)\n",
            "2025-02-10 09:59:34,720 - TestRunner-135660374683472 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374683472:Running tearDown\n",
            "2025-02-10 09:59:34,825 - TestRunner-135660374683472 - INFO - Executing test: test_error_zero_quantity\n",
            "INFO:TestRunner-135660374683472:Executing test: test_error_zero_quantity\n",
            "2025-02-10 09:59:34,828 - TestRunner-135660374683472 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374683472:Running setUp\n",
            "2025-02-10 09:59:34,830 - TestRunner-135660374683472 - INFO - test_error_zero_quantity: PASSED (0.002s)\n",
            "INFO:TestRunner-135660374683472:test_error_zero_quantity: PASSED (0.002s)\n",
            "2025-02-10 09:59:34,832 - TestRunner-135660374683472 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374683472:Running tearDown\n",
            "2025-02-10 09:59:34,938 - TestRunner-135660374683472 - INFO - Executing test: test_multi_create_pos\n",
            "INFO:TestRunner-135660374683472:Executing test: test_multi_create_pos\n",
            "2025-02-10 09:59:34,942 - TestRunner-135660374683472 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374683472:Running setUp\n",
            "2025-02-10 09:59:34,945 - TestRunner-135660374683472 - INFO - test_multi_create_pos: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374683472:test_multi_create_pos: PASSED (0.003s)\n",
            "2025-02-10 09:59:34,952 - TestRunner-135660374683472 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374683472:Running tearDown\n",
            "2025-02-10 09:59:35,055 - TestRunner-135660374683472 - INFO - Executing test: test_multi_create_prs\n",
            "INFO:TestRunner-135660374683472:Executing test: test_multi_create_prs\n",
            "2025-02-10 09:59:35,058 - TestRunner-135660374683472 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374683472:Running setUp\n",
            "2025-02-10 09:59:35,060 - TestRunner-135660374683472 - INFO - test_multi_create_prs: PASSED (0.002s)\n",
            "INFO:TestRunner-135660374683472:test_multi_create_prs: PASSED (0.002s)\n",
            "2025-02-10 09:59:35,062 - TestRunner-135660374683472 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374683472:Running tearDown\n",
            "2025-02-10 09:59:35,164 - TestRunner-135660374683472 - INFO - Executing test: test_multi_document_state_consistency\n",
            "INFO:TestRunner-135660374683472:Executing test: test_multi_document_state_consistency\n",
            "2025-02-10 09:59:35,168 - TestRunner-135660374683472 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374683472:Running setUp\n",
            "2025-02-10 09:59:35,171 - TestRunner-135660374683472 - INFO - test_multi_document_state_consistency: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374683472:test_multi_document_state_consistency: PASSED (0.003s)\n",
            "2025-02-10 09:59:35,173 - TestRunner-135660374683472 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374683472:Running tearDown\n",
            "2025-02-10 09:59:35,278 - TestRunner-135660374683472 - INFO - Executing test: test_multi_document_validation\n",
            "INFO:TestRunner-135660374683472:Executing test: test_multi_document_validation\n",
            "2025-02-10 09:59:35,281 - TestRunner-135660374683472 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374683472:Running setUp\n",
            "2025-02-10 09:59:35,283 - TestRunner-135660374683472 - INFO - test_multi_document_validation: PASSED (0.002s)\n",
            "INFO:TestRunner-135660374683472:test_multi_document_validation: PASSED (0.002s)\n",
            "2025-02-10 09:59:35,285 - TestRunner-135660374683472 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374683472:Running tearDown\n",
            "2025-02-10 09:59:35,388 - TestRunner-135660374683472 - INFO - Executing test: test_multi_partial_ordering\n",
            "INFO:TestRunner-135660374683472:Executing test: test_multi_partial_ordering\n",
            "2025-02-10 09:59:35,391 - TestRunner-135660374683472 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374683472:Running setUp\n",
            "2025-02-10 09:59:35,393 - TestRunner-135660374683472 - INFO - test_multi_partial_ordering: PASSED (0.002s)\n",
            "INFO:TestRunner-135660374683472:test_multi_partial_ordering: PASSED (0.002s)\n",
            "2025-02-10 09:59:35,395 - TestRunner-135660374683472 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374683472:Running tearDown\n",
            "2025-02-10 09:59:35,497 - TestRunner-135660374683472 - INFO - Executing test: test_po_creation\n",
            "INFO:TestRunner-135660374683472:Executing test: test_po_creation\n",
            "2025-02-10 09:59:35,500 - TestRunner-135660374683472 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374683472:Running setUp\n",
            "2025-02-10 09:59:35,503 - TestRunner-135660374683472 - INFO - test_po_creation: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374683472:test_po_creation: PASSED (0.003s)\n",
            "2025-02-10 09:59:35,507 - TestRunner-135660374683472 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374683472:Running tearDown\n",
            "2025-02-10 09:59:35,610 - TestRunner-135660374683472 - INFO - Executing test: test_pr_creation\n",
            "INFO:TestRunner-135660374683472:Executing test: test_pr_creation\n",
            "2025-02-10 09:59:35,614 - TestRunner-135660374683472 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374683472:Running setUp\n",
            "2025-02-10 09:59:35,618 - TestRunner-135660374683472 - INFO - test_pr_creation: PASSED (0.004s)\n",
            "INFO:TestRunner-135660374683472:test_pr_creation: PASSED (0.004s)\n",
            "2025-02-10 09:59:35,622 - TestRunner-135660374683472 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374683472:Running tearDown\n",
            "2025-02-10 09:59:35,727 - TestRunner-135660374683472 - INFO - \n",
            "Test Suite Results:\n",
            "INFO:TestRunner-135660374683472:\n",
            "Test Suite Results:\n",
            "2025-02-10 09:59:35,730 - TestRunner-135660374683472 - INFO - ==================================================\n",
            "INFO:TestRunner-135660374683472:==================================================\n",
            "2025-02-10 09:59:35,732 - TestRunner-135660374683472 - INFO - Total Tests    : 14\n",
            "INFO:TestRunner-135660374683472:Total Tests    : 14\n",
            "2025-02-10 09:59:35,734 - TestRunner-135660374683472 - INFO - Tests Passed   : 14\n",
            "INFO:TestRunner-135660374683472:Tests Passed   : 14\n",
            "2025-02-10 09:59:35,736 - TestRunner-135660374683472 - INFO - Tests Failed   : 0\n",
            "INFO:TestRunner-135660374683472:Tests Failed   : 0\n",
            "2025-02-10 09:59:35,738 - TestRunner-135660374683472 - INFO - Success Rate   : 100.0%\n",
            "INFO:TestRunner-135660374683472:Success Rate   : 100.0%\n",
            "2025-02-10 09:59:35,740 - TestRunner-135660374683472 - INFO - Execution Time : 1.595s\n",
            "INFO:TestRunner-135660374683472:Execution Time : 1.595s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "P2P Flow Test Summary\n",
            "==================================================\n",
            "Total Tests     : 14\n",
            "Passed          : 14\n",
            "Failed          : 0\n",
            "Success Rate    : 100.0%\n",
            "Execution Time  : 1.595s\n",
            "\n",
            "Test execution completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Script 8 (test runner calls): Error Test Cases\n",
        "from test_harness.tests.test_runner import run_error_tests, TestType, create_runner\n",
        "from test_harness.tests.test_p2p_flow import TestP2PFlow\n",
        "import asyncio\n",
        "import traceback\n",
        "import time\n",
        "import json\n",
        "from typing import Optional, Dict, Any, List\n",
        "from datetime import datetime\n",
        "\n",
        "async def run_error_test_cases() -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Run error test cases with detailed error analysis\n",
        "\n",
        "    Returns:\n",
        "        Optional[Dict[str, Any]]: Test results and error analysis or None if execution failed\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    error_log: List[Dict[str, Any]] = []\n",
        "\n",
        "    try:\n",
        "        print(\"Starting Error Test Cases Execution...\\n\")\n",
        "\n",
        "        # Create custom runner for error testing\n",
        "        runner = create_runner(\n",
        "            TestType.ERROR,\n",
        "            verbose=True,\n",
        "            error_focus=True,\n",
        "            include_timing=True\n",
        "        )\n",
        "\n",
        "        # Run test suite\n",
        "        results = await runner.run_test_suite(TestP2PFlow)\n",
        "\n",
        "        # Process error patterns\n",
        "        error_patterns: Dict[str, int] = {}\n",
        "        for result in results.results:\n",
        "            if not result.success and result.error:\n",
        "                error_type = result.error.split(':')[0]\n",
        "                error_patterns[error_type] = error_patterns.get(error_type, 0) + 1\n",
        "\n",
        "                error_log.append({\n",
        "                    'test': result.name,\n",
        "                    'timestamp': datetime.now().isoformat(),\n",
        "                    'error_type': error_type,\n",
        "                    'error_message': result.error,\n",
        "                    'execution_time': result.execution_time\n",
        "                })\n",
        "\n",
        "        # Print error test summary\n",
        "        print(\"\\nError Test Cases Summary\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Total Tests    : {results.total_tests}\")\n",
        "        print(f\"Expected Fails : {results.failed_tests}\")  # In error tests, failures are expected\n",
        "        print(f\"Unexpected Pass: {results.passed_tests}\")\n",
        "\n",
        "        # Print error patterns\n",
        "        if error_patterns:\n",
        "            print(\"\\nError Pattern Analysis:\")\n",
        "            print(\"-\" * 50)\n",
        "            for error_type, count in error_patterns.items():\n",
        "                print(f\"{error_type}: {count} occurrences\")\n",
        "\n",
        "        # Print detailed error log\n",
        "        print(\"\\nDetailed Error Log:\")\n",
        "        print(\"-\" * 50)\n",
        "        for error in error_log:\n",
        "            print(f\"\\nTest: {error['test']}\")\n",
        "            print(f\"Error Type: {error['error_type']}\")\n",
        "            print(f\"Message: {error['error_message']}\")\n",
        "            print(f\"Time: {error['execution_time']:.3f}s\")\n",
        "\n",
        "        # Return comprehensive results\n",
        "        return {\n",
        "            'suite_results': results,\n",
        "            'error_patterns': error_patterns,\n",
        "            'error_log': error_log,\n",
        "            'execution_time': time.time() - start_time,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\nError test execution failed:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        print(\"\\nTraceback:\")\n",
        "        print(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "# Run error tests with proper error handling\n",
        "try:\n",
        "    results = await run_error_test_cases()\n",
        "    if results is None:\n",
        "        print(\"Error test execution failed to complete\")\n",
        "except Exception as e:\n",
        "    print(f\"Fatal error in test execution: {str(e)}\")\n",
        "    print(traceback.format_exc())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsxGjEpLzEWO",
        "outputId": "02681209-4a0c-4114-b9ac-67bbe9a931d2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 09:59:44,227 - TestRunner-135660375417744 - INFO - Starting test suite: TestP2PFlow\n",
            "INFO:TestRunner-135660375417744:Starting test suite: TestP2PFlow\n",
            "2025-02-10 09:59:44,233 - TestRunner-135660375417744 - INFO - Found 14 test methods\n",
            "INFO:TestRunner-135660375417744:Found 14 test methods\n",
            "2025-02-10 09:59:44,238 - TestRunner-135660375417744 - INFO - Executing test: test_document_status_check\n",
            "INFO:TestRunner-135660375417744:Executing test: test_document_status_check\n",
            "2025-02-10 09:59:44,240 - TestRunner-135660375417744 - INFO - Running setUp\n",
            "INFO:TestRunner-135660375417744:Running setUp\n",
            "2025-02-10 09:59:44,250 - TestRunner-135660375417744 - INFO - test_document_status_check: PASSED (0.010s)\n",
            "INFO:TestRunner-135660375417744:test_document_status_check: PASSED (0.010s)\n",
            "2025-02-10 09:59:44,254 - TestRunner-135660375417744 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660375417744:Running tearDown\n",
            "2025-02-10 09:59:44,357 - TestRunner-135660375417744 - INFO - Executing test: test_error_invalid_material\n",
            "INFO:TestRunner-135660375417744:Executing test: test_error_invalid_material\n",
            "2025-02-10 09:59:44,361 - TestRunner-135660375417744 - INFO - Running setUp\n",
            "INFO:TestRunner-135660375417744:Running setUp\n",
            "2025-02-10 09:59:44,364 - TestRunner-135660375417744 - INFO - test_error_invalid_material: PASSED (0.003s)\n",
            "INFO:TestRunner-135660375417744:test_error_invalid_material: PASSED (0.003s)\n",
            "2025-02-10 09:59:44,366 - TestRunner-135660375417744 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660375417744:Running tearDown\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Error Test Cases Execution...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 09:59:44,469 - TestRunner-135660375417744 - INFO - Executing test: test_error_invalid_plant\n",
            "INFO:TestRunner-135660375417744:Executing test: test_error_invalid_plant\n",
            "2025-02-10 09:59:44,474 - TestRunner-135660375417744 - INFO - Running setUp\n",
            "INFO:TestRunner-135660375417744:Running setUp\n",
            "2025-02-10 09:59:44,477 - TestRunner-135660375417744 - INFO - test_error_invalid_plant: PASSED (0.003s)\n",
            "INFO:TestRunner-135660375417744:test_error_invalid_plant: PASSED (0.003s)\n",
            "2025-02-10 09:59:44,480 - TestRunner-135660375417744 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660375417744:Running tearDown\n",
            "2025-02-10 09:59:44,583 - TestRunner-135660375417744 - INFO - Executing test: test_error_invalid_pr_reference\n",
            "INFO:TestRunner-135660375417744:Executing test: test_error_invalid_pr_reference\n",
            "2025-02-10 09:59:44,587 - TestRunner-135660375417744 - INFO - Running setUp\n",
            "INFO:TestRunner-135660375417744:Running setUp\n",
            "2025-02-10 09:59:44,590 - TestRunner-135660375417744 - INFO - test_error_invalid_pr_reference: PASSED (0.002s)\n",
            "INFO:TestRunner-135660375417744:test_error_invalid_pr_reference: PASSED (0.002s)\n",
            "2025-02-10 09:59:44,592 - TestRunner-135660375417744 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660375417744:Running tearDown\n",
            "2025-02-10 09:59:44,695 - TestRunner-135660375417744 - INFO - Executing test: test_error_invalid_vendor\n",
            "INFO:TestRunner-135660375417744:Executing test: test_error_invalid_vendor\n",
            "2025-02-10 09:59:44,701 - TestRunner-135660375417744 - INFO - Running setUp\n",
            "INFO:TestRunner-135660375417744:Running setUp\n",
            "2025-02-10 09:59:44,703 - TestRunner-135660375417744 - INFO - test_error_invalid_vendor: PASSED (0.003s)\n",
            "INFO:TestRunner-135660375417744:test_error_invalid_vendor: PASSED (0.003s)\n",
            "2025-02-10 09:59:44,707 - TestRunner-135660375417744 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660375417744:Running tearDown\n",
            "2025-02-10 09:59:44,811 - TestRunner-135660375417744 - INFO - Executing test: test_error_pr_already_ordered\n",
            "INFO:TestRunner-135660375417744:Executing test: test_error_pr_already_ordered\n",
            "2025-02-10 09:59:44,815 - TestRunner-135660375417744 - INFO - Running setUp\n",
            "INFO:TestRunner-135660375417744:Running setUp\n",
            "2025-02-10 09:59:44,818 - TestRunner-135660375417744 - INFO - test_error_pr_already_ordered: PASSED (0.003s)\n",
            "INFO:TestRunner-135660375417744:test_error_pr_already_ordered: PASSED (0.003s)\n",
            "2025-02-10 09:59:44,820 - TestRunner-135660375417744 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660375417744:Running tearDown\n",
            "2025-02-10 09:59:44,923 - TestRunner-135660375417744 - INFO - Executing test: test_error_zero_quantity\n",
            "INFO:TestRunner-135660375417744:Executing test: test_error_zero_quantity\n",
            "2025-02-10 09:59:44,928 - TestRunner-135660375417744 - INFO - Running setUp\n",
            "INFO:TestRunner-135660375417744:Running setUp\n",
            "2025-02-10 09:59:44,931 - TestRunner-135660375417744 - INFO - test_error_zero_quantity: PASSED (0.002s)\n",
            "INFO:TestRunner-135660375417744:test_error_zero_quantity: PASSED (0.002s)\n",
            "2025-02-10 09:59:44,933 - TestRunner-135660375417744 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660375417744:Running tearDown\n",
            "2025-02-10 09:59:45,036 - TestRunner-135660375417744 - INFO - Executing test: test_multi_create_pos\n",
            "INFO:TestRunner-135660375417744:Executing test: test_multi_create_pos\n",
            "2025-02-10 09:59:45,042 - TestRunner-135660375417744 - INFO - Running setUp\n",
            "INFO:TestRunner-135660375417744:Running setUp\n",
            "2025-02-10 09:59:45,044 - TestRunner-135660375417744 - INFO - test_multi_create_pos: PASSED (0.003s)\n",
            "INFO:TestRunner-135660375417744:test_multi_create_pos: PASSED (0.003s)\n",
            "2025-02-10 09:59:45,047 - TestRunner-135660375417744 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660375417744:Running tearDown\n",
            "2025-02-10 09:59:45,150 - TestRunner-135660375417744 - INFO - Executing test: test_multi_create_prs\n",
            "INFO:TestRunner-135660375417744:Executing test: test_multi_create_prs\n",
            "2025-02-10 09:59:45,155 - TestRunner-135660375417744 - INFO - Running setUp\n",
            "INFO:TestRunner-135660375417744:Running setUp\n",
            "2025-02-10 09:59:45,159 - TestRunner-135660375417744 - INFO - test_multi_create_prs: PASSED (0.004s)\n",
            "INFO:TestRunner-135660375417744:test_multi_create_prs: PASSED (0.004s)\n",
            "2025-02-10 09:59:45,162 - TestRunner-135660375417744 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660375417744:Running tearDown\n",
            "2025-02-10 09:59:45,270 - TestRunner-135660375417744 - INFO - Executing test: test_multi_document_state_consistency\n",
            "INFO:TestRunner-135660375417744:Executing test: test_multi_document_state_consistency\n",
            "2025-02-10 09:59:45,273 - TestRunner-135660375417744 - INFO - Running setUp\n",
            "INFO:TestRunner-135660375417744:Running setUp\n",
            "2025-02-10 09:59:45,277 - TestRunner-135660375417744 - INFO - test_multi_document_state_consistency: PASSED (0.004s)\n",
            "INFO:TestRunner-135660375417744:test_multi_document_state_consistency: PASSED (0.004s)\n",
            "2025-02-10 09:59:45,283 - TestRunner-135660375417744 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660375417744:Running tearDown\n",
            "2025-02-10 09:59:45,387 - TestRunner-135660375417744 - INFO - Executing test: test_multi_document_validation\n",
            "INFO:TestRunner-135660375417744:Executing test: test_multi_document_validation\n",
            "2025-02-10 09:59:45,391 - TestRunner-135660375417744 - INFO - Running setUp\n",
            "INFO:TestRunner-135660375417744:Running setUp\n",
            "2025-02-10 09:59:45,394 - TestRunner-135660375417744 - INFO - test_multi_document_validation: PASSED (0.003s)\n",
            "INFO:TestRunner-135660375417744:test_multi_document_validation: PASSED (0.003s)\n",
            "2025-02-10 09:59:45,398 - TestRunner-135660375417744 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660375417744:Running tearDown\n",
            "2025-02-10 09:59:45,501 - TestRunner-135660375417744 - INFO - Executing test: test_multi_partial_ordering\n",
            "INFO:TestRunner-135660375417744:Executing test: test_multi_partial_ordering\n",
            "2025-02-10 09:59:45,505 - TestRunner-135660375417744 - INFO - Running setUp\n",
            "INFO:TestRunner-135660375417744:Running setUp\n",
            "2025-02-10 09:59:45,508 - TestRunner-135660375417744 - INFO - test_multi_partial_ordering: PASSED (0.003s)\n",
            "INFO:TestRunner-135660375417744:test_multi_partial_ordering: PASSED (0.003s)\n",
            "2025-02-10 09:59:45,519 - TestRunner-135660375417744 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660375417744:Running tearDown\n",
            "2025-02-10 09:59:45,622 - TestRunner-135660375417744 - INFO - Executing test: test_po_creation\n",
            "INFO:TestRunner-135660375417744:Executing test: test_po_creation\n",
            "2025-02-10 09:59:45,627 - TestRunner-135660375417744 - INFO - Running setUp\n",
            "INFO:TestRunner-135660375417744:Running setUp\n",
            "2025-02-10 09:59:45,630 - TestRunner-135660375417744 - INFO - test_po_creation: PASSED (0.002s)\n",
            "INFO:TestRunner-135660375417744:test_po_creation: PASSED (0.002s)\n",
            "2025-02-10 09:59:45,634 - TestRunner-135660375417744 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660375417744:Running tearDown\n",
            "2025-02-10 09:59:45,737 - TestRunner-135660375417744 - INFO - Executing test: test_pr_creation\n",
            "INFO:TestRunner-135660375417744:Executing test: test_pr_creation\n",
            "2025-02-10 09:59:45,740 - TestRunner-135660375417744 - INFO - Running setUp\n",
            "INFO:TestRunner-135660375417744:Running setUp\n",
            "2025-02-10 09:59:45,743 - TestRunner-135660375417744 - INFO - test_pr_creation: PASSED (0.003s)\n",
            "INFO:TestRunner-135660375417744:test_pr_creation: PASSED (0.003s)\n",
            "2025-02-10 09:59:45,745 - TestRunner-135660375417744 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660375417744:Running tearDown\n",
            "2025-02-10 09:59:45,857 - TestRunner-135660375417744 - INFO - \n",
            "Test Suite Results:\n",
            "INFO:TestRunner-135660375417744:\n",
            "Test Suite Results:\n",
            "2025-02-10 09:59:45,860 - TestRunner-135660375417744 - INFO - ==================================================\n",
            "INFO:TestRunner-135660375417744:==================================================\n",
            "2025-02-10 09:59:45,864 - TestRunner-135660375417744 - INFO - Total Tests    : 14\n",
            "INFO:TestRunner-135660375417744:Total Tests    : 14\n",
            "2025-02-10 09:59:45,868 - TestRunner-135660375417744 - INFO - Tests Passed   : 14\n",
            "INFO:TestRunner-135660375417744:Tests Passed   : 14\n",
            "2025-02-10 09:59:45,870 - TestRunner-135660375417744 - INFO - Tests Failed   : 0\n",
            "INFO:TestRunner-135660375417744:Tests Failed   : 0\n",
            "2025-02-10 09:59:45,872 - TestRunner-135660375417744 - INFO - Success Rate   : 100.0%\n",
            "INFO:TestRunner-135660375417744:Success Rate   : 100.0%\n",
            "2025-02-10 09:59:45,875 - TestRunner-135660375417744 - INFO - Execution Time : 1.631s\n",
            "INFO:TestRunner-135660375417744:Execution Time : 1.631s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error Test Cases Summary\n",
            "==================================================\n",
            "Total Tests    : 14\n",
            "Expected Fails : 0\n",
            "Unexpected Pass: 14\n",
            "\n",
            "Detailed Error Log:\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Script 9 (test runner calls) : Multi Document Tests\n",
        "\n",
        "from test_harness.tests.test_runner import run_multi_doc_tests, TestType, create_runner\n",
        "from test_harness.tests.test_p2p_flow import TestP2PFlow\n",
        "import asyncio\n",
        "import traceback\n",
        "import time\n",
        "import json\n",
        "from typing import Optional, Dict, Any, List, Set\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class DocumentRelation:\n",
        "    \"\"\"Track document relationships\"\"\"\n",
        "    source_doc: str\n",
        "    target_doc: str\n",
        "    relation_type: str\n",
        "    timestamp: str\n",
        "\n",
        "async def run_multi_document_tests() -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Run multi-document tests with relationship tracking\n",
        "\n",
        "    Returns:\n",
        "        Optional[Dict[str, Any]]: Test results and document analysis or None if execution failed\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    doc_relations: List[DocumentRelation] = []\n",
        "    unique_docs: Set[str] = set()\n",
        "\n",
        "    try:\n",
        "        print(\"Starting Multi-Document Test Execution...\\n\")\n",
        "\n",
        "        # Create runner for multi-document tests\n",
        "        runner = create_runner(\n",
        "            TestType.MULTI_DOC,\n",
        "            verbose=True,\n",
        "            multi_doc=True,\n",
        "            save_state=True,\n",
        "            include_timing=True\n",
        "        )\n",
        "\n",
        "        # Run test suite\n",
        "        results = await runner.run_test_suite(TestP2PFlow)\n",
        "\n",
        "        # Track document relationships from state snapshots\n",
        "        if results.state_snapshots:\n",
        "            for snapshot in results.state_snapshots:\n",
        "                state = snapshot['state']\n",
        "                if 'purchase_requisitions' in state:\n",
        "                    for pr_num, pr_data in state['purchase_requisitions'].items():\n",
        "                        unique_docs.add(pr_num)\n",
        "                        if 'po_number' in pr_data:\n",
        "                            doc_relations.append(\n",
        "                                DocumentRelation(\n",
        "                                    source_doc=pr_num,\n",
        "                                    target_doc=pr_data['po_number'],\n",
        "                                    relation_type='PR_TO_PO',\n",
        "                                    timestamp=datetime.now().isoformat()\n",
        "                                )\n",
        "                            )\n",
        "                            unique_docs.add(pr_data['po_number'])\n",
        "\n",
        "        # Calculate document metrics\n",
        "        doc_metrics = {\n",
        "            'total_documents': len(unique_docs),\n",
        "            'total_relations': len(doc_relations),\n",
        "            'avg_relations_per_doc': len(doc_relations) / len(unique_docs) if unique_docs else 0\n",
        "        }\n",
        "\n",
        "        # Print test summary\n",
        "        print(\"\\nMulti-Document Test Summary\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Total Tests    : {results.total_tests}\")\n",
        "        print(f\"Tests Passed   : {results.passed_tests}\")\n",
        "        print(f\"Tests Failed   : {results.failed_tests}\")\n",
        "\n",
        "        # Print document analysis\n",
        "        print(\"\\nDocument Analysis:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"Total Documents: {doc_metrics['total_documents']}\")\n",
        "        print(f\"Total Relations: {doc_metrics['total_relations']}\")\n",
        "        print(f\"Avg Relations/Doc: {doc_metrics['avg_relations_per_doc']:.2f}\")\n",
        "\n",
        "        # Print document relationships\n",
        "        if doc_relations:\n",
        "            print(\"\\nDocument Relationships:\")\n",
        "            print(\"-\" * 50)\n",
        "            for relation in doc_relations:\n",
        "                print(f\"{relation.source_doc} --[{relation.relation_type}]--> {relation.target_doc}\")\n",
        "\n",
        "        # Return comprehensive results\n",
        "        return {\n",
        "            'suite_results': results,\n",
        "            'doc_metrics': doc_metrics,\n",
        "            'doc_relations': [vars(r) for r in doc_relations],\n",
        "            'execution_time': time.time() - start_time,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\nMulti-document test execution failed:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        print(\"\\nTraceback:\")\n",
        "        print(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "# Run multi-document tests with proper error handling\n",
        "try:\n",
        "    results = await run_multi_document_tests()\n",
        "    if results is None:\n",
        "        print(\"Multi-document test execution failed to complete\")\n",
        "except Exception as e:\n",
        "    print(f\"Fatal error in test execution: {str(e)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuhZCAYUzKus",
        "outputId": "b6305d66-ee89-421c-cf15-4abe0d76c6b5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 09:59:53,476 - TestRunner-135660374897232 - INFO - Starting test suite: TestP2PFlow\n",
            "INFO:TestRunner-135660374897232:Starting test suite: TestP2PFlow\n",
            "2025-02-10 09:59:53,483 - TestRunner-135660374897232 - INFO - Found 14 test methods\n",
            "INFO:TestRunner-135660374897232:Found 14 test methods\n",
            "2025-02-10 09:59:53,487 - TestRunner-135660374897232 - INFO - Executing test: test_document_status_check\n",
            "INFO:TestRunner-135660374897232:Executing test: test_document_status_check\n",
            "2025-02-10 09:59:53,489 - TestRunner-135660374897232 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374897232:Running setUp\n",
            "2025-02-10 09:59:53,491 - TestRunner-135660374897232 - INFO - test_document_status_check: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374897232:test_document_status_check: PASSED (0.003s)\n",
            "2025-02-10 09:59:53,493 - TestRunner-135660374897232 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374897232:Running tearDown\n",
            "2025-02-10 09:59:53,605 - TestRunner-135660374897232 - INFO - Executing test: test_error_invalid_material\n",
            "INFO:TestRunner-135660374897232:Executing test: test_error_invalid_material\n",
            "2025-02-10 09:59:53,608 - TestRunner-135660374897232 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374897232:Running setUp\n",
            "2025-02-10 09:59:53,610 - TestRunner-135660374897232 - INFO - test_error_invalid_material: PASSED (0.002s)\n",
            "INFO:TestRunner-135660374897232:test_error_invalid_material: PASSED (0.002s)\n",
            "2025-02-10 09:59:53,612 - TestRunner-135660374897232 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374897232:Running tearDown\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Multi-Document Test Execution...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 09:59:53,715 - TestRunner-135660374897232 - INFO - Executing test: test_error_invalid_plant\n",
            "INFO:TestRunner-135660374897232:Executing test: test_error_invalid_plant\n",
            "2025-02-10 09:59:53,719 - TestRunner-135660374897232 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374897232:Running setUp\n",
            "2025-02-10 09:59:53,722 - TestRunner-135660374897232 - INFO - test_error_invalid_plant: PASSED (0.002s)\n",
            "INFO:TestRunner-135660374897232:test_error_invalid_plant: PASSED (0.002s)\n",
            "2025-02-10 09:59:53,724 - TestRunner-135660374897232 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374897232:Running tearDown\n",
            "2025-02-10 09:59:53,826 - TestRunner-135660374897232 - INFO - Executing test: test_error_invalid_pr_reference\n",
            "INFO:TestRunner-135660374897232:Executing test: test_error_invalid_pr_reference\n",
            "2025-02-10 09:59:53,829 - TestRunner-135660374897232 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374897232:Running setUp\n",
            "2025-02-10 09:59:53,831 - TestRunner-135660374897232 - INFO - test_error_invalid_pr_reference: PASSED (0.002s)\n",
            "INFO:TestRunner-135660374897232:test_error_invalid_pr_reference: PASSED (0.002s)\n",
            "2025-02-10 09:59:53,834 - TestRunner-135660374897232 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374897232:Running tearDown\n",
            "2025-02-10 09:59:53,938 - TestRunner-135660374897232 - INFO - Executing test: test_error_invalid_vendor\n",
            "INFO:TestRunner-135660374897232:Executing test: test_error_invalid_vendor\n",
            "2025-02-10 09:59:53,943 - TestRunner-135660374897232 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374897232:Running setUp\n",
            "2025-02-10 09:59:53,949 - TestRunner-135660374897232 - INFO - test_error_invalid_vendor: PASSED (0.006s)\n",
            "INFO:TestRunner-135660374897232:test_error_invalid_vendor: PASSED (0.006s)\n",
            "2025-02-10 09:59:53,952 - TestRunner-135660374897232 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374897232:Running tearDown\n",
            "2025-02-10 09:59:54,055 - TestRunner-135660374897232 - INFO - Executing test: test_error_pr_already_ordered\n",
            "INFO:TestRunner-135660374897232:Executing test: test_error_pr_already_ordered\n",
            "2025-02-10 09:59:54,058 - TestRunner-135660374897232 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374897232:Running setUp\n",
            "2025-02-10 09:59:54,061 - TestRunner-135660374897232 - INFO - test_error_pr_already_ordered: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374897232:test_error_pr_already_ordered: PASSED (0.003s)\n",
            "2025-02-10 09:59:54,064 - TestRunner-135660374897232 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374897232:Running tearDown\n",
            "2025-02-10 09:59:54,167 - TestRunner-135660374897232 - INFO - Executing test: test_error_zero_quantity\n",
            "INFO:TestRunner-135660374897232:Executing test: test_error_zero_quantity\n",
            "2025-02-10 09:59:54,171 - TestRunner-135660374897232 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374897232:Running setUp\n",
            "2025-02-10 09:59:54,174 - TestRunner-135660374897232 - INFO - test_error_zero_quantity: PASSED (0.004s)\n",
            "INFO:TestRunner-135660374897232:test_error_zero_quantity: PASSED (0.004s)\n",
            "2025-02-10 09:59:54,178 - TestRunner-135660374897232 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374897232:Running tearDown\n",
            "2025-02-10 09:59:54,282 - TestRunner-135660374897232 - INFO - Executing test: test_multi_create_pos\n",
            "INFO:TestRunner-135660374897232:Executing test: test_multi_create_pos\n",
            "2025-02-10 09:59:54,286 - TestRunner-135660374897232 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374897232:Running setUp\n",
            "2025-02-10 09:59:54,289 - TestRunner-135660374897232 - INFO - test_multi_create_pos: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374897232:test_multi_create_pos: PASSED (0.003s)\n",
            "2025-02-10 09:59:54,291 - TestRunner-135660374897232 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374897232:Running tearDown\n",
            "2025-02-10 09:59:54,394 - TestRunner-135660374897232 - INFO - Executing test: test_multi_create_prs\n",
            "INFO:TestRunner-135660374897232:Executing test: test_multi_create_prs\n",
            "2025-02-10 09:59:54,397 - TestRunner-135660374897232 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374897232:Running setUp\n",
            "2025-02-10 09:59:54,399 - TestRunner-135660374897232 - INFO - test_multi_create_prs: PASSED (0.002s)\n",
            "INFO:TestRunner-135660374897232:test_multi_create_prs: PASSED (0.002s)\n",
            "2025-02-10 09:59:54,401 - TestRunner-135660374897232 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374897232:Running tearDown\n",
            "2025-02-10 09:59:54,503 - TestRunner-135660374897232 - INFO - Executing test: test_multi_document_state_consistency\n",
            "INFO:TestRunner-135660374897232:Executing test: test_multi_document_state_consistency\n",
            "2025-02-10 09:59:54,508 - TestRunner-135660374897232 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374897232:Running setUp\n",
            "2025-02-10 09:59:54,513 - TestRunner-135660374897232 - INFO - test_multi_document_state_consistency: PASSED (0.005s)\n",
            "INFO:TestRunner-135660374897232:test_multi_document_state_consistency: PASSED (0.005s)\n",
            "2025-02-10 09:59:54,516 - TestRunner-135660374897232 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374897232:Running tearDown\n",
            "2025-02-10 09:59:54,620 - TestRunner-135660374897232 - INFO - Executing test: test_multi_document_validation\n",
            "INFO:TestRunner-135660374897232:Executing test: test_multi_document_validation\n",
            "2025-02-10 09:59:54,623 - TestRunner-135660374897232 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374897232:Running setUp\n",
            "2025-02-10 09:59:54,629 - TestRunner-135660374897232 - INFO - test_multi_document_validation: PASSED (0.006s)\n",
            "INFO:TestRunner-135660374897232:test_multi_document_validation: PASSED (0.006s)\n",
            "2025-02-10 09:59:54,632 - TestRunner-135660374897232 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374897232:Running tearDown\n",
            "2025-02-10 09:59:54,736 - TestRunner-135660374897232 - INFO - Executing test: test_multi_partial_ordering\n",
            "INFO:TestRunner-135660374897232:Executing test: test_multi_partial_ordering\n",
            "2025-02-10 09:59:54,738 - TestRunner-135660374897232 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374897232:Running setUp\n",
            "2025-02-10 09:59:54,741 - TestRunner-135660374897232 - INFO - test_multi_partial_ordering: PASSED (0.002s)\n",
            "INFO:TestRunner-135660374897232:test_multi_partial_ordering: PASSED (0.002s)\n",
            "2025-02-10 09:59:54,743 - TestRunner-135660374897232 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374897232:Running tearDown\n",
            "2025-02-10 09:59:54,847 - TestRunner-135660374897232 - INFO - Executing test: test_po_creation\n",
            "INFO:TestRunner-135660374897232:Executing test: test_po_creation\n",
            "2025-02-10 09:59:54,851 - TestRunner-135660374897232 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374897232:Running setUp\n",
            "2025-02-10 09:59:54,854 - TestRunner-135660374897232 - INFO - test_po_creation: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374897232:test_po_creation: PASSED (0.003s)\n",
            "2025-02-10 09:59:54,857 - TestRunner-135660374897232 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374897232:Running tearDown\n",
            "2025-02-10 09:59:54,960 - TestRunner-135660374897232 - INFO - Executing test: test_pr_creation\n",
            "INFO:TestRunner-135660374897232:Executing test: test_pr_creation\n",
            "2025-02-10 09:59:54,965 - TestRunner-135660374897232 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374897232:Running setUp\n",
            "2025-02-10 09:59:54,969 - TestRunner-135660374897232 - INFO - test_pr_creation: PASSED (0.004s)\n",
            "INFO:TestRunner-135660374897232:test_pr_creation: PASSED (0.004s)\n",
            "2025-02-10 09:59:54,972 - TestRunner-135660374897232 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374897232:Running tearDown\n",
            "2025-02-10 09:59:55,075 - TestRunner-135660374897232 - INFO - \n",
            "Test Suite Results:\n",
            "INFO:TestRunner-135660374897232:\n",
            "Test Suite Results:\n",
            "2025-02-10 09:59:55,078 - TestRunner-135660374897232 - INFO - ==================================================\n",
            "INFO:TestRunner-135660374897232:==================================================\n",
            "2025-02-10 09:59:55,081 - TestRunner-135660374897232 - INFO - Total Tests    : 14\n",
            "INFO:TestRunner-135660374897232:Total Tests    : 14\n",
            "2025-02-10 09:59:55,086 - TestRunner-135660374897232 - INFO - Tests Passed   : 14\n",
            "INFO:TestRunner-135660374897232:Tests Passed   : 14\n",
            "2025-02-10 09:59:55,089 - TestRunner-135660374897232 - INFO - Tests Failed   : 0\n",
            "INFO:TestRunner-135660374897232:Tests Failed   : 0\n",
            "2025-02-10 09:59:55,091 - TestRunner-135660374897232 - INFO - Success Rate   : 100.0%\n",
            "INFO:TestRunner-135660374897232:Success Rate   : 100.0%\n",
            "2025-02-10 09:59:55,094 - TestRunner-135660374897232 - INFO - Execution Time : 1.598s\n",
            "INFO:TestRunner-135660374897232:Execution Time : 1.598s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Multi-Document Test Summary\n",
            "==================================================\n",
            "Total Tests    : 14\n",
            "Tests Passed   : 14\n",
            "Tests Failed   : 0\n",
            "\n",
            "Document Analysis:\n",
            "--------------------------------------------------\n",
            "Total Documents: 0\n",
            "Total Relations: 0\n",
            "Avg Relations/Doc: 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Script 10 (test runner calls): Consolidated MVP Tests\n",
        "\n",
        "from test_harness.tests.test_runner import run_consolidated_tests, TestType, create_runner\n",
        "from test_harness.tests.test_material_ops import TestMaterialOperations\n",
        "from test_harness.tests.test_p2p_flow import TestP2PFlow\n",
        "import asyncio\n",
        "import traceback\n",
        "import time\n",
        "import json\n",
        "from typing import Optional, Dict, Any\n",
        "from datetime import datetime\n",
        "\n",
        "async def run_consolidated_mvp_tests() -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Run consolidated MVP tests with comprehensive reporting\n",
        "\n",
        "    Returns:\n",
        "        Optional[Dict[str, Any]]: Test results and analysis or None if execution failed\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        print(\"Starting Consolidated MVP Test Execution...\\n\")\n",
        "\n",
        "        # Define test classes with explicit order\n",
        "        test_classes = [\n",
        "            TestMaterialOperations,  # Material management tests first\n",
        "            TestP2PFlow             # P2P process tests second\n",
        "        ]\n",
        "\n",
        "        # Run consolidated tests with full configuration\n",
        "        results = await run_consolidated_tests(\n",
        "            test_classes,\n",
        "            verbose=True,\n",
        "            include_timing=True,\n",
        "            save_state=True,\n",
        "            state_snapshots=True,\n",
        "            reset_state=True\n",
        "        )\n",
        "\n",
        "        # Process results to create summary\n",
        "        summary = {\n",
        "            'total_tests': 0,\n",
        "            'total_passed': 0,\n",
        "            'total_failed': 0,\n",
        "            'total_time': time.time() - start_time,\n",
        "            'categories': {},\n",
        "            'state_snapshots': [],\n",
        "            'failures': []\n",
        "        }\n",
        "\n",
        "        # Calculate totals and collect state snapshots\n",
        "        for class_name, suite_result in results.items():\n",
        "            summary['total_tests'] += suite_result.total_tests\n",
        "            summary['total_passed'] += suite_result.passed_tests\n",
        "            summary['total_failed'] += suite_result.failed_tests\n",
        "\n",
        "            # Store category results\n",
        "            summary['categories'][class_name] = {\n",
        "                'total': suite_result.total_tests,\n",
        "                'passed': suite_result.passed_tests,\n",
        "                'failed': suite_result.failed_tests,\n",
        "                'success_rate': suite_result.success_rate,\n",
        "                'execution_time': suite_result.execution_time\n",
        "            }\n",
        "\n",
        "            # Collect failures\n",
        "            failed_tests = suite_result.get_failed_tests()\n",
        "            if failed_tests:\n",
        "                for test in failed_tests:\n",
        "                    summary['failures'].append({\n",
        "                        'category': class_name,\n",
        "                        'test': test.name,\n",
        "                        'error': test.error,\n",
        "                        'execution_time': test.execution_time,\n",
        "                        'details': test.details\n",
        "                    })\n",
        "\n",
        "            # Collect state snapshots\n",
        "            if suite_result.state_snapshots:\n",
        "                summary['state_snapshots'].extend([\n",
        "                    {\n",
        "                        'category': class_name,\n",
        "                        'snapshot': snapshot\n",
        "                    } for snapshot in suite_result.state_snapshots\n",
        "                ])\n",
        "\n",
        "        # Calculate overall success rate\n",
        "        overall_success_rate = (\n",
        "            (summary['total_passed'] / summary['total_tests'] * 100)\n",
        "            if summary['total_tests'] > 0 else 0.0\n",
        "        )\n",
        "\n",
        "        # Print comprehensive summary\n",
        "        print(\"\\nConsolidated MVP Test Summary\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Total Tests    : {summary['total_tests']}\")\n",
        "        print(f\"Total Passed   : {summary['total_passed']}\")\n",
        "        print(f\"Total Failed   : {summary['total_failed']}\")\n",
        "        print(f\"Overall Success: {overall_success_rate:.1f}%\")\n",
        "        print(f\"Total Time     : {summary['total_time']:.3f}s\")\n",
        "\n",
        "        # Print category summaries\n",
        "        print(\"\\nCategory Results:\")\n",
        "        print(\"-\" * 50)\n",
        "        for category, stats in summary['categories'].items():\n",
        "            print(f\"\\n{category}:\")\n",
        "            print(f\"Tests: {stats['total']}\")\n",
        "            print(f\"Passed: {stats['passed']}\")\n",
        "            print(f\"Failed: {stats['failed']}\")\n",
        "            print(f\"Success Rate: {stats['success_rate']:.1f}%\")\n",
        "            print(f\"Time: {stats['execution_time']:.3f}s\")\n",
        "\n",
        "        # Print failures if any\n",
        "        if summary['failures']:\n",
        "            print(\"\\nTest Failures:\")\n",
        "            print(\"-\" * 50)\n",
        "            for failure in summary['failures']:\n",
        "                print(f\"\\nCategory: {failure['category']}\")\n",
        "                print(f\"Test: {failure['test']}\")\n",
        "                print(f\"Error: {failure['error']}\")\n",
        "                print(f\"Time: {failure['execution_time']:.3f}s\")\n",
        "                if failure['details'] and failure['details'].get('error_traceback'):\n",
        "                    print(\"Traceback:\")\n",
        "                    print(failure['details']['error_traceback'])\n",
        "\n",
        "        # Return comprehensive results\n",
        "        return {\n",
        "            'summary': summary,\n",
        "            'detailed_results': results,\n",
        "            'execution_time': time.time() - start_time,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\nConsolidated test execution failed:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        print(\"\\nTraceback:\")\n",
        "        print(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "# Import required modules and run tests\n",
        "try:\n",
        "    import nest_asyncio\n",
        "    nest_asyncio.apply()  # Required for running async code in Colab\n",
        "\n",
        "    # Execute tests\n",
        "    results = await run_consolidated_mvp_tests()\n",
        "\n",
        "    if results is None:\n",
        "        print(\"\\nTest execution failed to complete.\")\n",
        "        print(\"Please check the error messages above.\")\n",
        "    else:\n",
        "        print(\"\\nTest execution completed successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Fatal error in test execution: {str(e)}\")\n",
        "    print(traceback.format_exc())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8Z16lGFj0TG",
        "outputId": "9bf7bdd0-46fc-4fb4-b94d-33d83c4f0d74"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 10:00:10,383 - TestRunner-135660374991760 - INFO - Starting test suite: TestMaterialOperations\n",
            "INFO:TestRunner-135660374991760:Starting test suite: TestMaterialOperations\n",
            "2025-02-10 10:00:10,391 - TestRunner-135660374991760 - INFO - Found 10 test methods\n",
            "INFO:TestRunner-135660374991760:Found 10 test methods\n",
            "2025-02-10 10:00:10,400 - TestRunner-135660374991760 - INFO - Executing test: test_create_and_verify_material\n",
            "INFO:TestRunner-135660374991760:Executing test: test_create_and_verify_material\n",
            "2025-02-10 10:00:10,404 - TestRunner-135660374991760 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374991760:Running setUp\n",
            "2025-02-10 10:00:10,408 - TestRunner-135660374991760 - INFO - test_create_and_verify_material: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374991760:test_create_and_verify_material: PASSED (0.003s)\n",
            "2025-02-10 10:00:10,411 - TestRunner-135660374991760 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374991760:Running tearDown\n",
            "2025-02-10 10:00:10,515 - TestRunner-135660374991760 - INFO - Executing test: test_default_values\n",
            "INFO:TestRunner-135660374991760:Executing test: test_default_values\n",
            "2025-02-10 10:00:10,518 - TestRunner-135660374991760 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374991760:Running setUp\n",
            "2025-02-10 10:00:10,521 - TestRunner-135660374991760 - INFO - test_default_values: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374991760:test_default_values: PASSED (0.003s)\n",
            "2025-02-10 10:00:10,524 - TestRunner-135660374991760 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374991760:Running tearDown\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Consolidated MVP Test Execution...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 10:00:10,627 - TestRunner-135660374991760 - INFO - Executing test: test_duplicate_material_creation\n",
            "INFO:TestRunner-135660374991760:Executing test: test_duplicate_material_creation\n",
            "2025-02-10 10:00:10,630 - TestRunner-135660374991760 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374991760:Running setUp\n",
            "2025-02-10 10:00:10,632 - TestRunner-135660374991760 - INFO - test_duplicate_material_creation: PASSED (0.002s)\n",
            "INFO:TestRunner-135660374991760:test_duplicate_material_creation: PASSED (0.002s)\n",
            "2025-02-10 10:00:10,634 - TestRunner-135660374991760 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374991760:Running tearDown\n",
            "2025-02-10 10:00:10,738 - TestRunner-135660374991760 - INFO - Executing test: test_empty_input_handling\n",
            "INFO:TestRunner-135660374991760:Executing test: test_empty_input_handling\n",
            "2025-02-10 10:00:10,742 - TestRunner-135660374991760 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374991760:Running setUp\n",
            "2025-02-10 10:00:10,744 - TestRunner-135660374991760 - INFO - test_empty_input_handling: PASSED (0.002s)\n",
            "INFO:TestRunner-135660374991760:test_empty_input_handling: PASSED (0.002s)\n",
            "2025-02-10 10:00:10,747 - TestRunner-135660374991760 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374991760:Running tearDown\n",
            "2025-02-10 10:00:10,850 - TestRunner-135660374991760 - INFO - Executing test: test_get_material_master_data\n",
            "INFO:TestRunner-135660374991760:Executing test: test_get_material_master_data\n",
            "2025-02-10 10:00:10,855 - TestRunner-135660374991760 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374991760:Running setUp\n",
            "2025-02-10 10:00:10,857 - TestRunner-135660374991760 - INFO - test_get_material_master_data: PASSED (0.002s)\n",
            "INFO:TestRunner-135660374991760:test_get_material_master_data: PASSED (0.002s)\n",
            "2025-02-10 10:00:10,860 - TestRunner-135660374991760 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374991760:Running tearDown\n",
            "2025-02-10 10:00:10,963 - TestRunner-135660374991760 - INFO - Executing test: test_initial_material_check\n",
            "INFO:TestRunner-135660374991760:Executing test: test_initial_material_check\n",
            "2025-02-10 10:00:10,967 - TestRunner-135660374991760 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374991760:Running setUp\n",
            "2025-02-10 10:00:10,970 - TestRunner-135660374991760 - INFO - test_initial_material_check: PASSED (0.002s)\n",
            "INFO:TestRunner-135660374991760:test_initial_material_check: PASSED (0.002s)\n",
            "2025-02-10 10:00:10,972 - TestRunner-135660374991760 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374991760:Running tearDown\n",
            "2025-02-10 10:00:11,076 - TestRunner-135660374991760 - INFO - Executing test: test_invalid_material_check\n",
            "INFO:TestRunner-135660374991760:Executing test: test_invalid_material_check\n",
            "2025-02-10 10:00:11,080 - TestRunner-135660374991760 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374991760:Running setUp\n",
            "2025-02-10 10:00:11,083 - TestRunner-135660374991760 - INFO - test_invalid_material_check: PASSED (0.002s)\n",
            "INFO:TestRunner-135660374991760:test_invalid_material_check: PASSED (0.002s)\n",
            "2025-02-10 10:00:11,085 - TestRunner-135660374991760 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374991760:Running tearDown\n",
            "2025-02-10 10:00:11,189 - TestRunner-135660374991760 - INFO - Executing test: test_invalid_plant_check\n",
            "INFO:TestRunner-135660374991760:Executing test: test_invalid_plant_check\n",
            "2025-02-10 10:00:11,193 - TestRunner-135660374991760 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374991760:Running setUp\n",
            "2025-02-10 10:00:11,195 - TestRunner-135660374991760 - INFO - test_invalid_plant_check: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374991760:test_invalid_plant_check: PASSED (0.003s)\n",
            "2025-02-10 10:00:11,199 - TestRunner-135660374991760 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374991760:Running tearDown\n",
            "2025-02-10 10:00:11,303 - TestRunner-135660374991760 - INFO - Executing test: test_material_creation_without_id\n",
            "INFO:TestRunner-135660374991760:Executing test: test_material_creation_without_id\n",
            "2025-02-10 10:00:11,306 - TestRunner-135660374991760 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374991760:Running setUp\n",
            "2025-02-10 10:00:11,309 - TestRunner-135660374991760 - INFO - test_material_creation_without_id: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374991760:test_material_creation_without_id: PASSED (0.003s)\n",
            "2025-02-10 10:00:11,313 - TestRunner-135660374991760 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374991760:Running tearDown\n",
            "2025-02-10 10:00:11,416 - TestRunner-135660374991760 - INFO - Executing test: test_missing_required_fields\n",
            "INFO:TestRunner-135660374991760:Executing test: test_missing_required_fields\n",
            "2025-02-10 10:00:11,419 - TestRunner-135660374991760 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374991760:Running setUp\n",
            "2025-02-10 10:00:11,421 - TestRunner-135660374991760 - INFO - test_missing_required_fields: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374991760:test_missing_required_fields: PASSED (0.003s)\n",
            "2025-02-10 10:00:11,426 - TestRunner-135660374991760 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374991760:Running tearDown\n",
            "2025-02-10 10:00:11,530 - TestRunner-135660374991760 - INFO - \n",
            "Test Suite Results:\n",
            "INFO:TestRunner-135660374991760:\n",
            "Test Suite Results:\n",
            "2025-02-10 10:00:11,533 - TestRunner-135660374991760 - INFO - ==================================================\n",
            "INFO:TestRunner-135660374991760:==================================================\n",
            "2025-02-10 10:00:11,536 - TestRunner-135660374991760 - INFO - Total Tests    : 10\n",
            "INFO:TestRunner-135660374991760:Total Tests    : 10\n",
            "2025-02-10 10:00:11,541 - TestRunner-135660374991760 - INFO - Tests Passed   : 10\n",
            "INFO:TestRunner-135660374991760:Tests Passed   : 10\n",
            "2025-02-10 10:00:11,546 - TestRunner-135660374991760 - INFO - Tests Failed   : 0\n",
            "INFO:TestRunner-135660374991760:Tests Failed   : 0\n",
            "2025-02-10 10:00:11,554 - TestRunner-135660374991760 - INFO - Success Rate   : 100.0%\n",
            "INFO:TestRunner-135660374991760:Success Rate   : 100.0%\n",
            "2025-02-10 10:00:11,559 - TestRunner-135660374991760 - INFO - Execution Time : 1.147s\n",
            "INFO:TestRunner-135660374991760:Execution Time : 1.147s\n",
            "2025-02-10 10:00:11,564 - TestRunner-135660374899600 - INFO - Starting test suite: TestP2PFlow\n",
            "INFO:TestRunner-135660374899600:Starting test suite: TestP2PFlow\n",
            "2025-02-10 10:00:11,569 - TestRunner-135660374899600 - INFO - Found 14 test methods\n",
            "INFO:TestRunner-135660374899600:Found 14 test methods\n",
            "2025-02-10 10:00:11,571 - TestRunner-135660374899600 - INFO - Executing test: test_document_status_check\n",
            "INFO:TestRunner-135660374899600:Executing test: test_document_status_check\n",
            "2025-02-10 10:00:11,574 - TestRunner-135660374899600 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374899600:Running setUp\n",
            "2025-02-10 10:00:11,576 - TestRunner-135660374899600 - INFO - test_document_status_check: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374899600:test_document_status_check: PASSED (0.003s)\n",
            "2025-02-10 10:00:11,578 - TestRunner-135660374899600 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374899600:Running tearDown\n",
            "2025-02-10 10:00:11,681 - TestRunner-135660374899600 - INFO - Executing test: test_error_invalid_material\n",
            "INFO:TestRunner-135660374899600:Executing test: test_error_invalid_material\n",
            "2025-02-10 10:00:11,684 - TestRunner-135660374899600 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374899600:Running setUp\n",
            "2025-02-10 10:00:11,687 - TestRunner-135660374899600 - INFO - test_error_invalid_material: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374899600:test_error_invalid_material: PASSED (0.003s)\n",
            "2025-02-10 10:00:11,692 - TestRunner-135660374899600 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374899600:Running tearDown\n",
            "2025-02-10 10:00:11,796 - TestRunner-135660374899600 - INFO - Executing test: test_error_invalid_plant\n",
            "INFO:TestRunner-135660374899600:Executing test: test_error_invalid_plant\n",
            "2025-02-10 10:00:11,799 - TestRunner-135660374899600 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374899600:Running setUp\n",
            "2025-02-10 10:00:11,802 - TestRunner-135660374899600 - INFO - test_error_invalid_plant: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374899600:test_error_invalid_plant: PASSED (0.003s)\n",
            "2025-02-10 10:00:11,808 - TestRunner-135660374899600 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374899600:Running tearDown\n",
            "2025-02-10 10:00:11,918 - TestRunner-135660374899600 - INFO - Executing test: test_error_invalid_pr_reference\n",
            "INFO:TestRunner-135660374899600:Executing test: test_error_invalid_pr_reference\n",
            "2025-02-10 10:00:11,921 - TestRunner-135660374899600 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374899600:Running setUp\n",
            "2025-02-10 10:00:11,928 - TestRunner-135660374899600 - INFO - test_error_invalid_pr_reference: PASSED (0.007s)\n",
            "INFO:TestRunner-135660374899600:test_error_invalid_pr_reference: PASSED (0.007s)\n",
            "2025-02-10 10:00:11,930 - TestRunner-135660374899600 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374899600:Running tearDown\n",
            "2025-02-10 10:00:12,033 - TestRunner-135660374899600 - INFO - Executing test: test_error_invalid_vendor\n",
            "INFO:TestRunner-135660374899600:Executing test: test_error_invalid_vendor\n",
            "2025-02-10 10:00:12,038 - TestRunner-135660374899600 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374899600:Running setUp\n",
            "2025-02-10 10:00:12,040 - TestRunner-135660374899600 - INFO - test_error_invalid_vendor: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374899600:test_error_invalid_vendor: PASSED (0.003s)\n",
            "2025-02-10 10:00:12,042 - TestRunner-135660374899600 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374899600:Running tearDown\n",
            "2025-02-10 10:00:12,145 - TestRunner-135660374899600 - INFO - Executing test: test_error_pr_already_ordered\n",
            "INFO:TestRunner-135660374899600:Executing test: test_error_pr_already_ordered\n",
            "2025-02-10 10:00:12,150 - TestRunner-135660374899600 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374899600:Running setUp\n",
            "2025-02-10 10:00:12,154 - TestRunner-135660374899600 - INFO - test_error_pr_already_ordered: PASSED (0.004s)\n",
            "INFO:TestRunner-135660374899600:test_error_pr_already_ordered: PASSED (0.004s)\n",
            "2025-02-10 10:00:12,157 - TestRunner-135660374899600 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374899600:Running tearDown\n",
            "2025-02-10 10:00:12,265 - TestRunner-135660374899600 - INFO - Executing test: test_error_zero_quantity\n",
            "INFO:TestRunner-135660374899600:Executing test: test_error_zero_quantity\n",
            "2025-02-10 10:00:12,269 - TestRunner-135660374899600 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374899600:Running setUp\n",
            "2025-02-10 10:00:12,274 - TestRunner-135660374899600 - INFO - test_error_zero_quantity: PASSED (0.005s)\n",
            "INFO:TestRunner-135660374899600:test_error_zero_quantity: PASSED (0.005s)\n",
            "2025-02-10 10:00:12,278 - TestRunner-135660374899600 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374899600:Running tearDown\n",
            "2025-02-10 10:00:12,391 - TestRunner-135660374899600 - INFO - Executing test: test_multi_create_pos\n",
            "INFO:TestRunner-135660374899600:Executing test: test_multi_create_pos\n",
            "2025-02-10 10:00:12,394 - TestRunner-135660374899600 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374899600:Running setUp\n",
            "2025-02-10 10:00:12,397 - TestRunner-135660374899600 - INFO - test_multi_create_pos: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374899600:test_multi_create_pos: PASSED (0.003s)\n",
            "2025-02-10 10:00:12,404 - TestRunner-135660374899600 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374899600:Running tearDown\n",
            "2025-02-10 10:00:12,507 - TestRunner-135660374899600 - INFO - Executing test: test_multi_create_prs\n",
            "INFO:TestRunner-135660374899600:Executing test: test_multi_create_prs\n",
            "2025-02-10 10:00:12,510 - TestRunner-135660374899600 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374899600:Running setUp\n",
            "2025-02-10 10:00:12,513 - TestRunner-135660374899600 - INFO - test_multi_create_prs: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374899600:test_multi_create_prs: PASSED (0.003s)\n",
            "2025-02-10 10:00:12,517 - TestRunner-135660374899600 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374899600:Running tearDown\n",
            "2025-02-10 10:00:12,621 - TestRunner-135660374899600 - INFO - Executing test: test_multi_document_state_consistency\n",
            "INFO:TestRunner-135660374899600:Executing test: test_multi_document_state_consistency\n",
            "2025-02-10 10:00:12,624 - TestRunner-135660374899600 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374899600:Running setUp\n",
            "2025-02-10 10:00:12,631 - TestRunner-135660374899600 - INFO - test_multi_document_state_consistency: PASSED (0.007s)\n",
            "INFO:TestRunner-135660374899600:test_multi_document_state_consistency: PASSED (0.007s)\n",
            "2025-02-10 10:00:12,634 - TestRunner-135660374899600 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374899600:Running tearDown\n",
            "2025-02-10 10:00:12,738 - TestRunner-135660374899600 - INFO - Executing test: test_multi_document_validation\n",
            "INFO:TestRunner-135660374899600:Executing test: test_multi_document_validation\n",
            "2025-02-10 10:00:12,741 - TestRunner-135660374899600 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374899600:Running setUp\n",
            "2025-02-10 10:00:12,744 - TestRunner-135660374899600 - INFO - test_multi_document_validation: PASSED (0.003s)\n",
            "INFO:TestRunner-135660374899600:test_multi_document_validation: PASSED (0.003s)\n",
            "2025-02-10 10:00:12,748 - TestRunner-135660374899600 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374899600:Running tearDown\n",
            "2025-02-10 10:00:12,852 - TestRunner-135660374899600 - INFO - Executing test: test_multi_partial_ordering\n",
            "INFO:TestRunner-135660374899600:Executing test: test_multi_partial_ordering\n",
            "2025-02-10 10:00:12,855 - TestRunner-135660374899600 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374899600:Running setUp\n",
            "2025-02-10 10:00:12,857 - TestRunner-135660374899600 - INFO - test_multi_partial_ordering: PASSED (0.002s)\n",
            "INFO:TestRunner-135660374899600:test_multi_partial_ordering: PASSED (0.002s)\n",
            "2025-02-10 10:00:12,860 - TestRunner-135660374899600 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374899600:Running tearDown\n",
            "2025-02-10 10:00:12,962 - TestRunner-135660374899600 - INFO - Executing test: test_po_creation\n",
            "INFO:TestRunner-135660374899600:Executing test: test_po_creation\n",
            "2025-02-10 10:00:12,966 - TestRunner-135660374899600 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374899600:Running setUp\n",
            "2025-02-10 10:00:12,969 - TestRunner-135660374899600 - INFO - test_po_creation: PASSED (0.004s)\n",
            "INFO:TestRunner-135660374899600:test_po_creation: PASSED (0.004s)\n",
            "2025-02-10 10:00:12,973 - TestRunner-135660374899600 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374899600:Running tearDown\n",
            "2025-02-10 10:00:13,079 - TestRunner-135660374899600 - INFO - Executing test: test_pr_creation\n",
            "INFO:TestRunner-135660374899600:Executing test: test_pr_creation\n",
            "2025-02-10 10:00:13,082 - TestRunner-135660374899600 - INFO - Running setUp\n",
            "INFO:TestRunner-135660374899600:Running setUp\n",
            "2025-02-10 10:00:13,084 - TestRunner-135660374899600 - INFO - test_pr_creation: PASSED (0.002s)\n",
            "INFO:TestRunner-135660374899600:test_pr_creation: PASSED (0.002s)\n",
            "2025-02-10 10:00:13,086 - TestRunner-135660374899600 - INFO - Running tearDown\n",
            "INFO:TestRunner-135660374899600:Running tearDown\n",
            "2025-02-10 10:00:13,189 - TestRunner-135660374899600 - INFO - \n",
            "Test Suite Results:\n",
            "INFO:TestRunner-135660374899600:\n",
            "Test Suite Results:\n",
            "2025-02-10 10:00:13,193 - TestRunner-135660374899600 - INFO - ==================================================\n",
            "INFO:TestRunner-135660374899600:==================================================\n",
            "2025-02-10 10:00:13,196 - TestRunner-135660374899600 - INFO - Total Tests    : 14\n",
            "INFO:TestRunner-135660374899600:Total Tests    : 14\n",
            "2025-02-10 10:00:13,201 - TestRunner-135660374899600 - INFO - Tests Passed   : 14\n",
            "INFO:TestRunner-135660374899600:Tests Passed   : 14\n",
            "2025-02-10 10:00:13,204 - TestRunner-135660374899600 - INFO - Tests Failed   : 0\n",
            "INFO:TestRunner-135660374899600:Tests Failed   : 0\n",
            "2025-02-10 10:00:13,208 - TestRunner-135660374899600 - INFO - Success Rate   : 100.0%\n",
            "INFO:TestRunner-135660374899600:Success Rate   : 100.0%\n",
            "2025-02-10 10:00:13,211 - TestRunner-135660374899600 - INFO - Execution Time : 1.625s\n",
            "INFO:TestRunner-135660374899600:Execution Time : 1.625s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Consolidated MVP Test Summary\n",
            "==================================================\n",
            "Total Tests    : 24\n",
            "Total Passed   : 24\n",
            "Total Failed   : 0\n",
            "Overall Success: 100.0%\n",
            "Total Time     : 2.832s\n",
            "\n",
            "Category Results:\n",
            "--------------------------------------------------\n",
            "\n",
            "TestMaterialOperations:\n",
            "Tests: 10\n",
            "Passed: 10\n",
            "Failed: 0\n",
            "Success Rate: 100.0%\n",
            "Time: 1.147s\n",
            "\n",
            "TestP2PFlow:\n",
            "Tests: 14\n",
            "Passed: 14\n",
            "Failed: 0\n",
            "Success Rate: 100.0%\n",
            "Time: 1.625s\n",
            "\n",
            "Test execution completed successfully.\n"
          ]
        }
      ]
    }
  ]
}